---
title: "A Network Epidemic Model for Online Community Commissioning Data"
author: Clement Lee (joint work with Andrew Garbett and Darren J Wilkinson) <br> Statistics research group seminar
date: 2017-03-24
output:
    ioslides_presentation:
        css:: styles.css
bibliography: /data/app_movement/ref_am.bib
---



[abstract]: # (Statistical models for network epidemics usually assume a Bernoulli random graph, in which any two nodes have the same probability of being connected. Such assumption provides computational simplicity but does not describe real-life networks well. We propose an epidemic model based on the preferential attachment model, which adds nodes sequentially by simple rules to generate a network. A simulation study based on the subsequent Markov Chain Monte Carlo algorithm reveals an identifiability issue with the model parameters, so an alternative parameterisation is suggested. Finally, the model is applied to a set of online commissioning data.)

[comment]: # (This will not be shown on the webpage, not even in the converted HTML source file - remember to skip a line below (or above?)!)

<div class="notes">

- Finished PhD last year
- Moved to Newcastle this Feburary to take up an RA position
- Although employed by Maths & Stats, based in Open Lab

</div>



----
<left><img src="openlab_light.svg" height = "50px" /></left>

https://digitalcivics.io

A computing research lab in

- Human-computer interaction
- Social and ubiquitous computing

Themes:

- Health and social care
- Education
- Politics

<div class="notes">

- Some postgraduate students here may know people in the open lab, because they also have a CDT there.
- Their work focus on helping or empowering communities in aspects such as health and education, with the use of web and mobile technologies.
- I will first talk about some projects I'm involved in
- Then move on to the statistical model developed when I was analysing the data of one of the projects

</div>



[comment]: # (Influenced by Sugata Mitra, TED Prize winner)

[comment]: # (Hole in the Wall experiment - put a computer in a slum in India, & let the children learn computing by themselves)

## [Learning Circle](https://learningcircle.io)
Online learning platform

Different from Massive Open Online Course (MOOC)

* passive learning

Students learn from interaction with each other and mentors/experts

* active learning
* the line between student and teacher becomes less clear

<div class="notes">

- One project in Education is called Learning circle, which is a online learning platform.
- It is different from the traditional way of learning, because it doesn't have a fixed set of course materials.
- Instead students from all over the world interact with each other in the class section every week, via e.g. Google hangout, and learn about the topic through the interaction with each other and with the mentors.
- After the class section they can continue the disccusion on the platform through comments.
- The mentors can moderate the discussion so that the platform doesn't become Facebook.
- The focus of the project is the active and self-organising nature of learning.
</div>



## [Connecting Classes](https://jonathanworth.org/connecting_classes/)
- Just like an ordinary class, but students take notes via twitter or write blogs
- [\#CClasses](https://twitter.com/search?q=%23CClasses) and the class specific hashtags for extracting and collecting data
- Can interact in the social network with each other, as well as _people outside the classroom_, e.g. experts in the subject
- Expand the learning space beyond the physical classroom

<div class="notes">
- Hashtags - so that we can trace the data afterwards by searching for these hashtags
- So why is it useful? For example, if the class is about photography, the students can draw in an expert in photography just by mentioning them in the tweets, and the expert can join the discussion right away. You don't need to wait until the class is over.
- In some sense the boundary of the classroom becomes less clear.
</div>



## Social network analysis
<center><img src="cclasses.jpg" height = "450px" /></center>
[comment]: # (talking points: Mostly exploratory analysis, writing apps to visualise)

<div class="notes">
- For both projects, my role is to visualise the data collected from these platforms, so that the leads of the projects can explore the data themselves.
- This one is for connecting classes (next slide)
</div>



## Social network analysis
<center><img src="learning_circle.jpg" height = "500px" /></center>
[comment]: # (Have to resize the image manually)
[comment]: # (or simply <center><...></center>

<div class="notes">
- This one for learning circle
- No modelling involved, but there is a common theme of social network analysis here
- This will also be a component of the epidemic model I will talk about.
</div>



## [Feed Finder](http://feed-finder.co.uk)
- An app for finding breastfeeding friendly places
- User review system

- How about other ideas that can be turned into a geolocation-based app?
- What if we let the community decide what app to develop?

<div class="notes">
- The next project is called FeedFinder, which is a mobile app for breastfeeding mothers
- They can use the app to find restaurants which are breastfeeding-friendly
- Can search for restaurants beforehand, or find nearby places when they are in city centres
- Leave reviews and rate restaurants for others to choose
- Community-driven, you somehow benefit from contributing if everybody does the same
- The idea of feed finder originated from somebody in the lab
- But communities may know better about their needs but don't have the technology to develop an app
</div>



## [App Movement](https://app-movement.com)
Support/sharing stage

* Start a movement, and share to friends on social network
* Potential users support and share

Design stage

* Use the templates provided by the developers
* Vote on the design aspects of the app

Launch stage

* Similar to Feed Finder, users search for places and give reviews

<div class="notes">
- This is where App Movement comes to rescue
- instead of the researchers thinking about what the community wants, we give control to the community itself
- Anybody in the community with an idea to create an app can start a movement and ask others to support the movement.
- After gaining enough support, the movement will enter the design stage, where the supporters vote on various aspects of the app.
- These decisions will be combined with the code templates written by the creators of the app movement, and then the mobile app will be generated.
- essentially a non-monetary kick-starter
</div>



## App Movement
Not all movements reach the target support and get launched

Some successful apps

* [Drone Zones](https://app-movement.com/apps/64)
* [Gender Neutral Toilet Finder](https://app-movement.com/apps/77)
* [Feed Finder, Lebanese version](https://app-movement.com/apps/83)

The sharing of a movement is like spreading an "epidemic" on the social network

This motivates us to develop a network epidemic model for the sharing data

<div class="notes">
- Not all movements reach the target support and get launched, but here are some successful ones.
- They are freely available in the app stores, you can give them a try to see if they are useful.
- The only downside: your data will be analysed and your life will be monitored by me.
- One thing interesting to look at is the sharing of a movement, because it is like spreading an "epidemic", except that you are doing it on the social network
- And this is the origin of our network epidemic model
</div>



## Epidemic models
Susceptible-Infected-Recovered (SIR) model
<center>
<img src="https://upload.wikimedia.org/wikipedia/commons/3/32/Sirsys-p9.png"><br>
    <font size="3">
          <a href="https://upload.wikimedia.org/wikipedia/commons/3/32/Sirsys-p9.png">
             image source
          </a>
    </font>
</center>

<div class="notes">
First thing to do - look for existing models to see if we can adapt
- The classic model is the SIR model, in which any individual is either susceptible, infected or recovered at any time point. The dynamics of the total number of individuals at each stage are governed by some differential equations.
- This SIR model is one example of the class of compartment models, which are very popular in the statistical literature.
</div>



## Epidemic models
Compartment models

* Each individual is at exactly one stage at any time
* Potentially goes from one stage to another 

Classical assumptions governing the dynamics

* Markovian
* Homogeneous mixing

See e.g. @ab00 [(link)](http://staff.math.su.se/tom.britton/newmain.pdf)

[comment]: # (not all style files support citation link)

<div class="notes">
- There are some variants of the SIR model, for example you can remove the recovered stage, or add an exposed stage before infected stage, but the idea is similar. You have several compartments, and at any time, the numbers of individuals in these compartments sum to a constant.
- One of the classical assumptions governing the dynamics is that the process of stage transitioning is Markovian.
- The other assumption of homogeneous mixing means that each individual interacts with all the other individuals uniformly, so, for example, all persons susceptible at this time point, have the same probability of being infected by another infected individual.
</div>



## Epidemic models
**Modelling heterogeneity**

Multiple levels of mixing

- @bms97: [two levels](http://projecteuclid.org/download/pdf_1/euclid.aoap/1034625252)
- @bko11: [three levels](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2010.00726.x/full)

Not quite applicable to our data

- No information on such levels in the social network

<div class="notes">
- To move away from the simplistic assumption of homogeneous mixing, there are models assuming multiple levels of mixing. You can infect somebody in the same house or in the same school, but the epidemic rates at these two levels are allowed to be different.
- But we don't have such kind of information about social network.
</div>



## Epidemic models
**Network modelling**

Previous work

- @bo02 [(link)](http://onlinelibrary.wiley.com/doi/10.1111/1467-9469.00296/abstract)
- @nr05 [(link)](http://link.springer.com/article/10.1007/s11222-005-4074-7)
- Focus on inference algorithms

Based on Bernoulli random graph (BRG)

+ $\Pr$(any two individuals are connected) = $p$
+ Such probability independent of all other connections
+ Not quite realistic for social networks

<div class="notes">
- There is another group of models which are more relevant because they did model the epidemic on a network.
- their focus is on the inference algorithms, so they just assumed the simplest network model possible, which is the Bernoulli random graph.
- points on slide
</div>



## Network models
@ba99: [Preferential attachment (PA) model](http://barabasi.com/f/67.pdf)

- Nodes join network sequentially, each with $\mu$ new edges
- Each existing node gets an edge with probability proportional to current degree

<center>
<img src="http://www3.nd.edu/~networks/Linked/fig7_1.jpg" height = "250px" /><br>     
    <font size="3">
          <a href="http://www3.nd.edu/~networks/Linked/fig7_1.jpg">
             image source
          </a>
    </font>
</center>

<div class="notes">
- Take a step back and look at network models alone with no epidemic.
- The most popular network model is the preferential attachment model.
- The original form of the model has two simple rules - bullet points on slides

- two new edges each
- earlier nodes are more likely to end up being the hubs
</div>



## Network models
Power-law degree distribution

```{r fig.width=3.9, fig.height=3.9, echo=FALSE, warning=FALSE}
source("functions_app_movement.R")
library(Matrix) # sparse matrices
library(Rcpp)
library(RcppArmadillo)
sourceCpp("algo.cpp") # reorder_G()
m <- 1000L
mu <- 3.0
gamma <- 0.0
set.seed(1234)
sim0.pat <- sim_pat(mu, m, gamma, T)
df0.degrees <- data_frame(degree = degree(sim0.pat)) %>%
            count(degree) %>%
            mutate(log.n = log(n), log.degree = log(degree), res = log.n + 3 * log.degree)
c <- df0.degrees %>% summarise(mean(res)) %>% unlist(use.names = F)
df0.degrees %<>% mutate(fitted = degree ^ (-3) * exp(c))
h <- df0.degrees %>%
  ggplot +
  geom_histogram(aes(degree, n), stat = "identity") +
  scale_x_continuous(breaks = c(1, 40, 80, 120)) + scale_y_continuous(breaks = c(1, 50, 100, 150)) +
  coord_cartesian(xlim = c(0, 125))
plot(h)
g0 <- df0.degrees %>%
  ggplot +
  geom_point(aes(degree, n)) +
  geom_line(aes(degree, fitted), col = 2) +
  scale_x_log10(breaks = c(1, 40, 80, 120)) + scale_y_log10(breaks = c(1, 50, 100, 150)) +
  coord_cartesian(ylim = c(0.1, 1000))
plot(g0)
```

<div class="notes">
- The PA model is very popular because it can capture one feature of real-life networks, which is the power-law degree distribution.
- This means you will see a straight line when you look at degree distribution on the log-scale
- On the original scale, you can see a few nodes highly connected while a lot of nodes have very small degrees
</div>



## Network models
Small path lengths

- Six degrees of separation

High clustering

- Three typical nodes $A,B$ and $C$
- Assume $A\leftrightarrow B$ and $B\leftrightarrow C$
- Clustering coefficient: how likely that $A\leftrightarrow C$
- BRG: whether each pair is connected is independent of other pairs a priori

<div class="notes">
- Another feature exhibited by real-life networks is small path length, which means it is easy to get to nodes on the other end of the network. This is essentially what the six degrees of separation is about.
- Also, real-life networks exhibit high clustering. This means two nodes, which are both connected to the same node, are more likely to be connected to each other.
- This is quite the opposite of what Bernoulli random graph implies.
</div>



## Network models
**Epidemic modelling**

Compartment models represented by ordinary differential equations

- $\displaystyle\frac{dS}{dt}=-\beta IS,
\quad\qquad\frac{dI}{dt}=\beta IS-\gamma I,
\quad\qquad\frac{dR}{dt}=\gamma I$

Incorporate network aspects as covariate

- E.g. same parameter values for nodes with same degree
- [Comprehensive review](http://journals.aps.org/rmp/abstract/10.1103/RevModPhys.87.925) by @pcvv15

[cue card]: # (Focus on modelling side; dominant in physics literature)

Quite remote from previously introduced statistical models

<div class="notes">
- Some people have also tried to start with a network model and incorporate an epidemic into the network.
- What they did is modify the differential equations governing the dynamics of the compartments, by incorporating network aspects as covariates.
- points in slide
- But they focus on the model structure and the simulation from the model, and care less about the inference
- In fact, inference would be quite difficult because of the number of parameters
</div>



## Ingredients

Population: $m$ nodes/individuals

The epidemic times $\boldsymbol{I}=(I_1,I_2,\ldots,I_m)$

- Susceptible-Infected (SI) model
- Markovian, constant infection rate $\beta$

The underlying graph/network $\boldsymbol{G}=\{G_{ij}\}_{m\times m}$

- PA model, parameter $\mu$
- $G_{ij}=1$ if $i\leftrightarrow j$ for $i\neq j$, $0$ otherwise

The transmission tree $\boldsymbol{P}=\{P_{ij}\}_{m\times m}$

- $P_{ij}=1$ if node $j$ is infected by $i$, $0$ otherwise

<div class="notes">
- After reviewing the models in the literature, we are ready to build our own model, and this is the list of ingredients of our model.
- Nodes and individuals interchangeable, so are graph and network
- Using the adjacency matrix representation, G and P are matrices with ones and zeros
</div>



## Model
**Step 1: Obtain new edges for the nodes**

Index the nodes by their order of entering the network

One edge to start with: nodes $1\leftrightarrow2$

Node $i~(>2)$ brings in $x_i$ new edges

- $x_i=\mu$ (constant) in original PA model too restrictive
- $x_i=y_i \wedge (i-1)$ where $y_i\sim$ Poission $(\mu)$

<div class="notes">
- The first step to build the model is to label the nodes by $1,2,\ldots,m$ according to their order of entering the network, and then obtain the sequence of new edges.
- Other than the initial edge between node 1 and node 2, each subsequent node brings in $x_i$ new edges when it joins the network.
- This number is a constant in the original PA model, but this makes the model too restrictive.
- So we assume this number of new edges follow a Possion distribution, but capped above by i-1.
</div>



## Model
**Step 1: Obtain new edges for the nodes**

Toy example: $m=8,\mu=3$

- $x_3=2,~x_4=3,~x_5=4,~x_6=2,~x_7=3,~x_8=4$

<div class="columns-2">
```{r, fig.width=4, fig.height=3.4, echo=FALSE, warning=FALSE}
set.seed(3456)
G <- sim_pat(3, 8) %>% as_adjacency_matrix
#set.seed(5308)
#jpeg(filename = "G.jpg", quality = 100, width = 320, height = 320)
#G %>% graph_from_adjacency_matrix("undirected") %>% plot(vertex.size = 50, layout = layout_in_circle, vertex.label.cex = 3)
#dev.off()
```
![](G.jpg)
```{r, echo=FALSE, warning=FALSE, collapse = TRUE}
print("G")
G %>% printSpMatrix
```
</div>

<div class="notes">
- Node 3 enters the network with two new edges, connects to nodes 1 and 2.
- Node 4 enters the network also with two new edges, and connects to nodes 1 and 2.
- Node 5 enters the network with three new edges, and connects to nodes 1, 2 and 3.
</div>



## Model
**Step 1: Obtain new edges for the nodes**


Toy example: $m=8,\mu=3$

- $x_3=2,~x_4=3,~x_5=4,~x_6=2,~x_7=3,~x_8=4$

<div class="columns-2">
Relationship between $x_i$ and $G_{ij}$

- For $i>2$, $x_i=\sum_{j=1}^{i-1}G_{ij}$
- $x_i$ essentially the column sum of $G_{ij}$, up to the major diagonal


```{r, echo=FALSE, warning=FALSE, collapse = TRUE}
print("G")
G %>% printSpMatrix
```
</div>

<div class="notes">
- We will discuss the rule of picking these nodes later on, but the new edges can be seen as the partial row or column sums of the individual elements Gij.
- This is useful because we won't need the x_i's anymore when we are writing down the likelihood. We just need the individual Gij's.
</div>




## Model
**Step 1: Obtain new edges for the nodes**

Likelihood:
$$
L_1(\boldsymbol{G};\mu)=\prod_{i=3}^{m}
\left[\frac{e^{-\mu}\mu^{x_i}}{x_i!}\right]^{\boldsymbol{1}\{0~\leq~x_i~<~i-1\}}
\left[\sum_{z=i-1}^\infty\frac{e^{-\mu}\mu^{z}}{z!}\right]^{\boldsymbol{1}\{x_i~=~i-1\}}\\
=\frac{e^{-\mu(m-2)}}{\prod_{i=3}^{m}(x_i!)}\prod_{i=3}^{m}
\mu^{\left[\sum_{j=1}^{i-1}G_{ij}\boldsymbol{1}\left\{0\leq\sum_{j=1}^{i-1}G_{ij}<i-1\right\}\right]}\\
\times\prod_{i=3}^{m}\left[(i-1)!\sum_{z=i-1}^{\infty}\frac{\mu^z}{z!}\right]^{\boldsymbol{1}\left\{\sum_{j=1}^{i-1}G_{ij}=i-1\right\}}
$$


<div class="notes">
- And this is how the likelihood contributed by the new edges look like.
- The mess is not that important, it only shows that you don't need the x_i's.
- We keep them here in the product of factorials only because it will get cancelled later on.
</div>



## Model
**Step 2: Preferentially attach the edges to build the network**

When node $i$ enters

- $x_i$ existing nodes chosen from $\{1,2,\ldots,i-1\}$
- Weighted sampling **without** replacement
- Weight of existing node $j = \frac{\sum_{k=1}^{i-1}G_{kj}}{\sum_{j=1}^{i-1}\sum_{k=1}^{i-1}G_{kj}}$

Exact likelihood

- Go through all $x_i!$ permutations of the selected nodes
- When $x=1,2,3,4,5,6,\ldots,~x!=1,2,6,24,120,720,\ldots$
- Computationally not feasible

<div class="notes">
- So we have generated the new edges, and we move on to the process of using the edges to connect the nodes, using the preferential attachment rule.
- Basically, when node $i$ enters the network, it chooses $x_i$ existing nodes from $\{1,2,\ldots,i-1\}$, with weights proportional to these nodes current degrees.
- If we are calculating the likelihood, we are essentially calculating the probability of picking the selected nodes under a weighted sampling scheme without replacement.
- It is very straightforward to simulate under weighted sampling without replacement, but to compute the exact likelihood, we need to go through all the permutations of the selected nodes, and then average the probabilities.
- This is not quite feasible because even with 6 new edges you will have to go through all 720 permutations in order to arrive at the exact likelihood. 
</div>



## Model
**Step 2: Preferentially attach the edges to build the network**

When node $i$ enters

- $x_i$ existing nodes chosen from $\{1,2,\ldots,i-1\}$
- Weighted sampling **without** replacement
- Weight of existing node $j = \frac{\sum_{k=1}^{i-1}G_{kj}}{\sum_{l=1}^{i-1}\sum_{k=1}^{i-1}G_{kl}}$

Approximate likelihood

- Weighted sampling **with** replacement

<div class="notes">
- Therefore, instead of calculating the exact likelihood, we use the approximate likelihood based on weighted sampling with replacement.
- This will make the expression of the likelihood much simpler. (next slide)
</div>



## Model
**Step 2: Preferentially attach the edges to build the network**

Contribution by node $i$'s new edges:
$$
L_{2i}=x_i!\times\prod_{j=1}^{i-1}\left(\frac{\sum_{k=1}^{i-1}G_{kj}}
                             {\sum_{l=1}^{i-1}\sum_{k=1}^{i-1}G_{kl}}\right)^{G_{ij}}
$$

Likelihood by the process of adding new edges:
$$
L_2(\boldsymbol{G}):=\prod_{i=3}^{m}L_{2i}=
  \prod_{i=3}^{m}(x_i!)\times
  \prod_{i=3}^{m}\prod_{j=1}^{i-1}\left(\frac{\sum_{k=1}^{i-1}G_{kj}}
                             {\sum_{l=1}^{i-1}\sum_{k=1}^{i-1}G_{kl}}\right)^{G_{ij}}
$$

<div class="notes">
- This L2i is the probability of picking the neighbours of node i when it joins the network
- And when we multiply over the all the i's from 3 to m, we obtain the likelihood contributed by this process of preferentially attaching the edges.
- This product of factorial cancels with what we've seen earlier (go back 3 slides), and this further simplifies the overall expression
</div>



## Model
**Step 3: Spread the epidemic on the given network**

Index the nodes by their epidemic (temporal) order

Infected node $i$ makes infectious contacts

- with its network neighbours
- at points of Poisson process with rate $\beta\sum_{j=1}^{m}G_{ij}$

Likelihood independent of transmission tree $\boldsymbol{P}$

<div class="centered">
$\pi(\boldsymbol{I}|\boldsymbol{G},\beta)=\beta^{m-1}\exp\left(-\beta\sum\sum_{(i,j):G_{ij}=1}\left[(I_j-I_i)\vee0\right]\right)\\
\qquad\quad~=\beta^{m-1}\exp\left(-\beta\sum_{i=1}^{m-1}\sum_{j=i+1}^{m}G_{ij}(I_j-I_i)\right)$
</div>

<div class="notes">
- After generating the network, we can spread the epidemic. The rule is quite simple too. Each node makes infectious contacts with those connected to them according to a Poisson process with a rate proportional to the number of susceptible neighbours.
- Whether the network is a preferential attachment one or a bernoulli random graph actually does not affect this rule of epidemic spreading. The network only affects the end product, that is how fast the epidemic is spreading.
- Then we can obtain the following expression, which we can see is independent of the transmission tree.
</div>



## Model
**Step 3: Spread the epidemic on the given network**

So where is $\boldsymbol{P}$ gone?

- "Uniform distribution on the set of all possible infection pathways" [@bo02]

<div class="centered">
$$
\pi(\boldsymbol{P}|\boldsymbol{G})\propto
\prod_{j=2}^m \frac{1}{~\sum_{i=1}^{j-1}G_{ij}~} \times 
\prod_{i=1}^{m-1}\prod_{j=i+1}^{m}\mathbf{1}\left\{P_{ij}\leq G_{ij}\right\}
$$
</div>

Posterior of $\boldsymbol{G}$ involves $\boldsymbol{P}$

- If $P_{ij}=1$, $G_{ij}(=G_{ji})=1$ with probability $1$ a posteriori
- If $P_{ij}=0$, posterior of $G_{ij}$ derived in inference

<div class="notes">
- So where is the information provided by the transmission tree?
- It actually follows a discrete uniform distribution on all possible infection pathways given by the network.
- Also, from this expression, you can see that, when it comes to inference, the posterior of the network will involve the transmission tree.
- If we know node i infects node j, then node i must be connected to node j.
- For the case where it is not node i infects node j, we will derive the posterior of Gij in the inference.
</div>



## Model
**Step 4: Connect the network and the epidemic**

Epidemic order not necessarily the same as network order

- Fix the labelling of nodes by epidemic order
- Introduce variable $\boldsymbol{\sigma}$ - permutation of $\{1,2,\ldots,m\}$

Convert from epidemic order to network order

- Replace $\boldsymbol{G}$ by $\boldsymbol{G}_{\boldsymbol{\sigma}}=f(\boldsymbol{G},\boldsymbol{\sigma})$ when computing $L_1(\cdot;\mu)$ and $L_2(\cdot)$

<div class="notes">
- One final step of building the model is to glue the epidemic order and the network order, which may not necessarily be the same.
- Our method is to fix the labelling according to the epidemic order, and introduce a vector variable which gives us the ordering of how these nodes enter the network.
- And then to calculate the likelihood, we stick to the graph G given by the epidemic order, but create a G sigma by converting G using sigma, and substitute this G sigma into L1 and L2. (next slide)
</div>



## Model
**Step 4: Connect the network and the epidemic**

Toy example continued: $m=8,\mu=3$

- $\boldsymbol{\sigma}=(5,3,8,2,6,4,7,1)$

<div class="columns-2">
```{r, echo=FALSE, collapse=TRUE}
print("G")
G %>% printSpMatrix
```
```{r, echo=FALSE, collapse=TRUE}
o <- c(5L,3L,8L,2L,6L,4L,7L,1L)-1L # 0-indexing in C++
G_sigma <- reorder_G(G, o, 8)
print("G_sigma = f(G, sigma)")
G_sigma %>% printSpMatrix
```
</div>

<div class="notes">
- If we go back to the toy example, we can see how this function f works. It simply reorders the rows and columns of G simultaneously, and outputs a matrix of the same dimensions as G.
- To calculate the likelihood, you simply use the adjacency matrix on the left for the epidemic, and the adjacency matrix on the right for the network.
</div>



## Bayesian inference

$\boldsymbol{G}$ is usually unknown

$~~~~\pi(\boldsymbol{G},\boldsymbol{\sigma},\beta,\mu|\boldsymbol{P},\boldsymbol{I})\\
\propto\pi(\boldsymbol{P},\boldsymbol{I},\boldsymbol{G},\boldsymbol{\sigma},\beta,\mu)\\
=\pi(\boldsymbol{P},\boldsymbol{I}|\boldsymbol{G},\boldsymbol{\sigma},\beta,\mu)~\pi(\boldsymbol{G},\boldsymbol{\sigma},\beta,\mu)\\
=\pi(\boldsymbol{P}|\boldsymbol{G})~\pi(\boldsymbol{I}|\boldsymbol{G},\beta)~\pi(\boldsymbol{G}|\boldsymbol{\sigma},\beta,\mu)~\pi(\boldsymbol{\sigma},\beta,\mu)\qquad(\boldsymbol{P}~\bot~\boldsymbol{I}~\text{given}~\boldsymbol{G})\\
=\pi(\boldsymbol{P}|\boldsymbol{G})~\pi(\boldsymbol{I}|\boldsymbol{G},\beta)~ L_1(\boldsymbol{G}_{\boldsymbol{\sigma}};\mu)~L_2(\boldsymbol{G}_{\boldsymbol{\sigma}})~\pi(\boldsymbol{\sigma})~\pi(\beta)~\pi(\mu)$

Markov Chain Monte Carlo (MCMC) algorithm straightforward

- Similar latent approach if $\boldsymbol{P}$ is also unknown

[cue card]: # (Similar both to G and to the previously cited papers)

<div class="notes">

- Now we have got all the components of the model, we can go on and carry out the infernce.
- But it is very common that G is unknown, and so it is quite natural to consider the edges of G as latent variables, and estimate their posterior with the other parameters.
- If we break down the posterior to these few components, and you can see that each component refers to a step we have seen just now, of course given the values of G.
- Also, once we specify the priors, which we assume to be independent, we can write out the whole line explicity, and it is quite straightforward to apply an MCMC algorithm here.
- This MCMC scheme can be used even when P is also unknown, as we can use a similar approach to G to calculate the posterior probability of each element of P. But we just stick to P being known here.
</div>



## Bayesian inference
**Uninformative priors**

$\beta\sim\text{Gamma}(a_\beta, \text{rate}=b_\beta)$  

$\mu\sim\text{Gamma}(a_\mu, \text{rate}=b_\mu)$

$\pi(\boldsymbol{\sigma})=(m!)^{-1}$

<div class="notes">
- Updating beta, mu and sigma is quite easy. You just start with some relatively uninformative and independent priors, and you will be able to arrive at their conditional posteriors up to a proportional constant. Then you can use a Metropolis step to propose to update each of these three parameters.
</div>



## Bayesian inference
**Posteriors**

$\beta|\ldots\sim
\text{Gamma}\left(a_\beta+m-1,\text{rate}=b_\beta+\sum_{i=1}^{m-1}\sum_{j=i+1}^{m}G_{ij}(I_j-I_i)\right)$

$\pi(\mu|\ldots)\propto L_1(\boldsymbol{G}_{\boldsymbol{\sigma}};\mu)~\pi(\mu)$

$\pi(\boldsymbol{\sigma}|\ldots)\propto L_1(\boldsymbol{G}_{\boldsymbol{\sigma}};\mu)~L_2(\boldsymbol{G}_{\boldsymbol{\sigma}})$


Exploring permutation space

- @bks06: [link](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_BezakovaKS06.pdf) 
- Random insertion
- More efficient than random swap

<div class="notes">
- It is even easier for beta because you can actually use a Gibbs step.
- Exploring the permutation space of sigma is slightly tricky, and we use the random insertion method suggested by this paper.
- The idea of random insertion is, imagine you have a deck of cards, which represent sigma. Now you randomly and uniformly take one card out, and put it back in another position, also randomly and uniformly.
- This method is empirically more efficient than the classical method of randomly swapping two adjacent cards, so we just go with this method, as long as the ordering is mixing properly.
</div>



## Bayesian inference
**Posteriors**

$\Pr(G_{ij}=1|P_{ij}=1,\ldots)=1$

$\boldsymbol{G}_0$ the same as $\boldsymbol{G}$ except $G_{ij}$ (and $G_{ji}$) is set to $0$

$\boldsymbol{G}_1$ the same as $\boldsymbol{G}$ except $G_{ij}$ (and $G_{ji}$) is set to $1$

$\Pr(G_{ij}=0|P_{ij}=0,\boldsymbol{G}_{-ij},\ldots)\propto
\displaystyle\frac{\quad\pi(\boldsymbol{G}_0|\boldsymbol{\sigma},\beta,\mu)\quad}
{\sum_{k=1,k\neq i}^{j-1}G_{kj}}$

$\Pr(G_{ij}=1|P_{ij}=0,\boldsymbol{G}_{-ij},\ldots)\propto
\displaystyle\frac{\pi(\boldsymbol{G}_1|\boldsymbol{\sigma},\beta,\mu)~e^{-\beta(I_j-I_i)}}
{\sum_{k=1,k\neq i}^{j-1}G_{kj}+1}$

[comment]: # (Perfect? Not quite)

<div class="notes">
- For each potential edge Gij, we divide into two cases.
- We have covered the trivial case where Pij equals 1, so we focus on the case where Pij equals 0.
- First we define two G's, both the same as G except Gij is set to 0 in this G0 and 1 in this G1. This means exactly one of G0 and G1 is identical to G.
- Then we calculate the weights under each scenario, and divide each line by their sum, we will have the explicit expression of this two probabilities

So everything is perfect, right? Unfortunately, it's not the case when it comes to simulation study.
</div>



## Simulation study
**Scenarios**

Simulate network only, estimate $(\mu,\boldsymbol{\sigma})$

- Good

Simulate network \& epidemic, estimate $(\mu,\beta,\boldsymbol{\sigma})$ given $\boldsymbol{G}$ & $\boldsymbol{I}$

- Good

Simulate network \& epidemic, estimate $(\mu,\beta,\boldsymbol{\sigma},\boldsymbol{G})$ given $\boldsymbol{P}$ & $\boldsymbol{I}$

- Problematic

<div class="notes">
- We created a few scenarios with different true values of the parameters, carried out the simulation, and fit the model to see if the inference recovers the true parameter values.
- It works absolutely fine when only the network is simulated and backfit.
- It even worked well when we also simulated the epidemic and fit the model by setting G to be known in the inference.
- But because in reality G is usually unknown, we simply used the same set of simulated data but fit the model by not knowing G, which means we have to carry out this Gibbs step to infer all the individual elemtns of G.
- The data we have for the inference is only P and I, and this is the case where the results become problematic.
</div>



## Simulation study
**Identifiability**

Posterior of $\mu$ does not depend not on its true value
```{r fig.width=5, fig.height=4, echo=FALSE, fig.align="center"}
## the following .csv is generated by knitting "net_epi_paper.Rnw" (or whatever the sweave file is named)
m_beta_mu_nedges_df <- read_csv("m_beta_mu_nedges_df.csv", col_types = paste0("idddc", paste0(rep("d", 14), collapse = ""))) %>%
             mutate(m = m %>% as.factor, muf = mu_true %>% as.factor, bf = beta_true %>% as.factor)
g1 <- m_beta_mu_nedges_df %>% ggplot(aes(mu_true, mu.mean, col = m)) +
    geom_line() + geom_point() +
    facet_wrap(~ bf) +
    labs(y = "estimate of mu", x = "true value of mu") +
    coord_cartesian(ylim = c(1, 2))
plot(g1)
```

<div class="notes">
- Here each figure represents the posterior mean of mu with a different true value of beta and different sample sizes, and you can see that the estimate of mu has nothing to do with its true value.
</div>



## Simulation study
**Identifiability**

$\beta$ not good either
```{r fig.width=5, fig.height=4, echo=FALSE, fig.align="center"}
g2 <- m_beta_mu_nedges_df %>% ggplot(aes(beta_true, beta.mean, col = m)) +
    geom_line() + geom_point() +
    facet_wrap(~ muf) +
    labs(y = "estimate of beta", x = "true value of beta") +
    scale_x_continuous(breaks = seq(0, 10, 2)) +
    scale_y_continuous(breaks = seq(0, 10, 2)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) +
    geom_abline(slope = 1, intercept = 0, lty = 3, col = 1)
plot(g2)
```

<div class="notes">
It's not good for beta either.
</div>



## Simulation study
**Identifiability**

What about $\ldots \alpha=\beta\times\mu$?
```{r fig.width=5, fig.height=4, echo=FALSE, fig.align="center"}
alpha_upper <- 14
g3 <- m_beta_mu_nedges_df %>% ggplot(aes(alpha_true, alpha.mean, col = m)) +
    geom_line() + geom_point() +
    facet_wrap(~ muf) +
    labs(y = "estimate of alpha", x = "true value of alpha") +
    scale_x_continuous(breaks = seq(0, alpha_upper, 2)) +
    scale_y_continuous(breaks = seq(0, alpha_upper, 2)) +
    coord_cartesian(xlim = c(0, alpha_upper), ylim = c(0, alpha_upper)) +
    geom_abline(slope = 1, intercept = 0, lty = 3, col = 1)
plot(g3)
```

<div class="notes">
- It turns out that only the product of the two parameters can be identified. We denote this parameter by alpha.
- identifying one parameter as good as we can get
</div>



## Simulation study
**Observations**

Inverse relationship between epidemic rate $\beta$ and parameter characterising network connectedness

- Average number new edges $\mu$ in PA model
- Edge inclusion probability $p$ in BRG model
    - Echoing observation by @bo02

Identifiability

- Identifying one parameter $(\alpha)$ as good as we can get
- Interpretation: network scaled epidemic rate

<div class="notes">
That the parameters' product is identifiable actually tells us there is a inverse relationship between the parameters, and this finding is actually consistent with the model using a Bernoulli random graph. 

- In PA it is beta and mu, in BRG it is beta and p
- So this echoes the observation by Britton and O'Neill that there are different explanations of the same outcome.
- Unless we put informative priors, we can really tell whether a high beta and small mu, or a low beta and large mu, is the more probable scenario giving rise to the data in hand
</div>



## Summary

Review on network epidemics

- Statistical models assume BRG, unrealistic
- Physical models focus on dynamics, difficult to do inference

Model and inference

- PA model grows the network, SI model spreads the epidemic
- Gibbs steps for $\beta$ and individual edges $G_{ij}$, Metropolis steps for $\mu$ and $\boldsymbol{\sigma}$

Simulation study

- Product of $\beta$ and $\mu$ identifiable but not individually



## Application - App Movement
Still waiting for results

Typical times taken per iteration:

- $m=100: 0.88\text{s}$
- $m=200: 14\text{s}$

Computational time $O(m^4)$

- \# potential edges $(G_{ij})$ to update $O(m^2)$
- \# computations involved in updating one edge $O(m^2)$

Data sets worth applying the model to

- $m = 350$ and up

<div class="notes">
But we can have some insights on the running times

- exactly 16 times
</div>



## Future work

Apply to App Movement data

- Sample sizes $m$ much larger than in simulation study
- Current MCMC algorithm not scalable with $m$ 

Compare with models which use BRG

- One possible way: calculate the marginal likelihoods

Marginal MCMC methods by simulating the network

- Direct according to the preferential attachment rule
- Takes much less time than updating the edges one by one


[comment]: # (change URL if change file name in github io repo)



## Bibliography
<style>
slides > slide { overflow: scroll; }
</style>
[comment]: # (pandoc rmarkdown will put references here automatically)