[
["index.html", "Lancaster University MATH550/SCC.461 R Programming Preface", " Lancaster University MATH550/SCC.461 R Programming Lecturer: Dr Clement Lee 2020/2021 Preface These are the R programming notes for the MATH550 Statistics in Practice and SCC.461 Programming for Data Scientists modules on the MSc in Statistics and the MSc in Data Science respectively. They were originally written by Dr Debbie Costain and Stuart Sharples. They have then been added to and amended by Dr Chris Nemeth and Dr Tom Palmer. "],
["introduction-to-r.html", "0 Introduction to R 0.1 Background 0.2 Saving your work 0.3 Getting started 0.4 Getting Help 0.5 R scripts 0.6 Functions", " 0 Introduction to R The purpose of this chapter is to provide you with a gentle introduction to R as an environment for working with and analysing data. In this lab, you will practice using some of the basic functions and operations in R, as well as how to create functions of your own. 0.1 Background R is a free and open-source programming language for statistical computing and graphics. `Open-source’ means that the code that was written to create the R environment is available for others to use and modify from its original design. Open-source software is typically thought of as a collaborative effort where people improve upon the source code and share the changes within the community so that other members can take advantage of their work, as well as help improve it further.1 Versions of R are available for Windows, Macs and Linux. To download and install R visit the website http://www.r-project.org. You will also need RStudio (http://www.rstudio.com). If you are using one of the computers in the lab, both of these will already be installed. One way in which someone may improve upon R is by trying to add functionality to it, like, making it possible to produce a particular graph or fit a particular kind of model. Figure 0.1: Correlograms help us visualise correlation matrices and are implemented in the corrgram package. Once someone has implemented a particular feature, they typically bundle it up into a package and request it to be stored on CRAN (the Comprehensive R Archive Network). This makes it incredibly easy for users to download and install other people’s packages which is something we will initially be doing a lot of. Installing packages and using other people’s code in order to get a piece of software to do what you want might seem a bit odd at first. But, actually, this modular nature is common in lots of software; Excel has its add-ins, SAS has extra procedures which are bundled into products, and SPSS has modules. An entire open-source operating system, Linux, has been built using this modular approach, again developed by a passionate community.2 0.2 Saving your work If you are using a university computer or are remotely connected to your virtual machine, start by finding your home folder (your H: drive). This H: drive will be made accessible to you no matter which university computer you use. Access it via Start button &gt; Computer. This is the best location to save all your files (reports, scripts, plots, data sets, and everything else). If using a university computer, do NOT save files to the local (C:) drive as they will probably not be there when you next login. Also, ISS makes backups of your H: drive (I think they do this every night), so if you ever permanently delete a file, they should be able to recover a previous version of it. 0.3 Getting started Start by opening R Studio. R Studio tries to makes best use of the screen by splitting it up in to three panels: An interactive R console – enter R commands manually at the prompt at the bottom (look for the &gt; symbol). Environment – as you progress through the materials, the objects you create will show up here which will include things like data sets, plots, and models. Stuff – a miscellaneous panel that has various uses, as you will see later. For now, click on the Plot tab. The panel should become blank (since we have not made any plots yet). 0.3.1 Using R as an overgrown calculator Everything you do in this section will be using the console panel. Start by entering one of the simplest possible commands you can do in R; addition of two numbers. Type the following and press Enter. 4 + 5 ## [1] 9 The result is automatically printed back to the user. R can do all the basic maths operations. The [1] refers to 9 being the first part of out answer. In this case, 9 is the first and only part to our answer. You will come across answers later which have many parts, and so R prints these numbers in square brackets to help. Basic maths operations in R. Operation Example Add 2 + 3 Subtract 2 - 3 Multiply 2*3 Divide 2/3 Modulus 2 %% 3 Square root sqrt(2) Power 2^3 Cosine cos(2*pi) Sine sin(pi/2) Tangent tan(pi/4) Natural log log(2) Expontential exp(2) Using numbers of your own, try the other basic mathematical operations listed below 100-45 65*89 45/-3 100 %% 9 sqrt(810) 2^10 Once you’re happy, try stringing some of these calculations all into one command, for example: 3 + 7*5/3 - 2 ## [1] 12.66667 (3 + 7)*5/(3 - 2) ## [1] 50 The multiplication and division calculations are performed first (* and /), followed by addition and subtraction (+ and -), going from left to right.3 This can be overridden by using putting the calculations we want to perform first in brackets. A more complex calculation is: \\[ \\frac{\\sqrt{3/4}}{\\frac{1}{3} - \\frac{2}{\\pi^2}}. \\] sqrt(3/4) / (1/3 - 2/pi^2) ## [1] 6.626513 This next step is when R stops behaving just like a calculator and starts becoming more useful. We can store the results of our calculations in an object using the assignment operator, &lt;- (a less-than symbol followed by a hyphen). We just need to think of useful names for our objects: apple &lt;- 20 - 16 banana &lt;- 4 + 5 apple - banana ## [1] -5 If you look in the environment panel of RStudio, you should now see two objects listed. These objects are now part of our working environment. We can access their contents by typing entering their names into the console: apple ## [1] 4 banana ## [1] 9 When creating objects, remember R is case-sensitive so apple is different from Apple: Apple ## Error in eval(expr, envir, enclos): object &#39;Apple&#39; not found And if you decide to assign a new value to an object, R will not hesitate in overwriting it (R does not issue a warning): apple &lt;- -100 apple ## [1] -100 We can increase the value an object contains by referencing itself: apple &lt;- apple + 1 apple ## [1] -99 In fact, all mathematical operations can be applied like this. 0.3.2 Vectors This is where R starts to become useful for data scientists, as it is optimised to work with vectors of numbers. A vector is like a list of numbers, or a single column in a spreadsheet. A lot of the mathematical operations we have already looked at also work on vectors. x &lt;- c(1, 2, 77, 2, -2, 14) y &lt;- 2 x + y ## [1] 3 4 79 4 0 16 x*y ## [1] 2 4 154 4 -4 28 x^y ## [1] 1 4 5929 4 4 196 The c() function allows us to combine (concatenate) numbers together into a list. And we stored this list inside x. In mathematics, a list of numbers like this is referred to as a vector. The expression x + y added 2 to every element in x, likewise x*y doubled each element. But what happens if y were a vector like x: y &lt;- c(1, 1, 2, 2, 3, 3) x + y ## [1] 2 3 79 4 1 17 x*y ## [1] 1 2 154 4 -6 42 You can see that R performs the operations on an elementwise basis. To check how many elements are in a vector type: length(x) ## [1] 6 length(y) ## [1] 6 In order to add or multiply two vectors they need to be the same length or one has to be a multiple of the other: y &lt;- c(55, 42, 13, 24) x + y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 56 44 90 26 53 56 y &lt;- c(55, 13) x + y ## [1] 56 15 132 15 53 27 The priority of operations is the same for vector arithmetic as it is for single values: 2*x + y ## [1] 57 17 209 17 51 41 2*(x + y) ## [1] 112 30 264 30 106 54 If we want to create a quick sequence of numbers then we can take advantage of the seq() function: x &lt;- seq(from=1, to=10, by=1) x ## [1] 1 2 3 4 5 6 7 8 9 10 y &lt;- seq(from=1, to=10, by=2) y ## [1] 1 3 5 7 9 This is our first use of a function with parameters. In order to use seq(), we need to specify three bits of information; what number do we start from, what do we go up to, and what size steps do we take. And we separate these using commas. A common task in writing R code is creating an integer sequence (like what we have done for x), as a result R offers us a shortcut using a colon in the following way: x &lt;- 0:15 x ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 y &lt;- 6:-6 y ## [1] 6 5 4 3 2 1 0 -1 -2 -3 -4 -5 -6 0.3.3 Basic plots Using vectors, the operations we have covered and the basic plotting function in R we can start to look at some interesting things. For example, we can look at the relationship between acceleration due to Earth’s gravity and the distance you are from Earth. To calculate the acceleration we use the following formula: \\[ a = G\\frac{m}{r^2} \\] where \\(a\\) is acceleration, \\(G\\) the gravitational constant, \\(m\\) the mass of the Earth, and \\(r\\) the distance between the centre of the Earth and the centre of you. (#fig:earth_human)The distance between the centre of Earth and the centre of human is represented by r. The mass of Earth is approximately \\(5.97\\times10^{24}\\) kg. To work out the acceleration you experience when you’re stood on the surface of Earth, we set \\(r\\) to be the distance between it’s centre and it’s surface (i.e. it’s radius). This is approximately \\(6.37\\times10^6\\) m. Finally, \\(G\\) takes the value \\(6.67\\times10^{-11}\\). So to calculate \\(a\\) at the surface of Earth enter: G &lt;- 6.6728*10^-11 r_earth &lt;- 6.371*10^6 m &lt;- 5.9736*10^24 r &lt;- r_earth a &lt;- G*m/r^2 a ## [1] 9.820397 This means that if you jump up in the air, once you begin to fall, you will be accelerating towards the ground at 9.82 metres per second.4 We fall very quickly. So, how does this effect change as we move further away from Earth? To answer this, we should make a plot. We will make a plot that shows the effect of gravity on the Earth’s surface (0m) all the way up the orbit height of GPS satellites (20,350km): dist &lt;- seq(from=0, to=20350*1000, by=1000) r &lt;- r_earth + dist a &lt;- G*m/r^2 dist_km &lt;- dist / 1000 plot(x=dist_km, y=a, type=&quot;l&quot;) The shape you see represents the decreasing effect of gravity the further we get from Earth, and is more generally known as the inverse-square law. This pattern appears a lot in nature; the intensity of the heat you feel from a burning fire and how close you are to it follows an inverse-square law, as does how bright it appears to you. We set three parameters when using the plot() function; we set the distance in kilometres from the Earth’s surface to be on the x-axis (x=dist_km), and the acceleration due to gravity on the y-axis (y=a). The type=\"l\" option (l stands for line) and specifies that we want to plot a line, if you remove this parameter, R will plot points instead. This is because the default value for type is \"p\" which is short for points. In a future lab, we look at how to change axis labels, tic marks, add a title, line colour and so on, as well as how to export plots. 0.3.4 Summary There is a priority of operations in arithmetic statements in R: raising to a power (^), multiplication and division (* and /), addition and subtraction (+ and -). Operations are performed left to right, and priorities are overridden by parentheses. 6*(5+2)^2/(2+6-9)/7*2 ## [1] -84 Results can be stored in objects using the assignment statement: &lt;-. Object names are case sensitive. Type the name into the console and R will print the contents. x &lt;- 0:10 y &lt;- x^2 y ## [1] 0 1 4 9 16 25 36 49 64 81 100 Vectors are ordered lists of numbers. To create vectors we use, c(), the combine function. Arithmetic operations work on vectors with the same priorities as single numbers. Use seq() to create a vector containing a specific number sequence, the parameters; by and length.out can be used to control the sequence. days_long &lt;- c(1, 2, 3, 4, 5, 6, 7) days_quick &lt;- 1:7 half_days &lt;- seq(1, 7, by=0.5) days_long + 0.5 ## [1] 1.5 2.5 3.5 4.5 5.5 6.5 7.5 2*half_days ## [1] 2 3 4 5 6 7 8 9 10 11 12 13 14 c(days_long, days_quick) ## [1] 1 2 3 4 5 6 7 1 2 3 4 5 6 7 The plot() function produces a plot of \\(x\\)-\\(y\\) points using two vectors. Whether points are used or a line is draw is determined by the type parameter (type=\"l\" for lines, or type=\"p\" for points). The points() function has the same parameters as plot() but will add it’s data to an existing plot, rather than create a new one. r &lt;- 1:1000 g &lt;- 1/r^2 plot(x=r, y=g, type=&quot;l&quot;, col=&quot;red&quot;) h &lt;- g*2 points(x=r, y=h, type=&quot;p&quot;, col=&quot;blue&quot;) 0.3.5 Exercises The exercises that follow can be carried out using the techniques and functions you have learned so far. Expect to make some mistakes. But if you start to get annoyed either take a break or ask for help. Evaluate the following expressions: \\(\\frac{49^2 - 542}{42 + 63^2}\\) \\(a^2\\) for \\(a = 1, 2, \\ldots, 10\\) Create plots for different powers of \\(x\\): Create a vector, \\(x\\), which goes from -100 up to 100. Create a new plot showing \\(y = x^2\\). Add a red line to the existing plot showing \\(y = x^4\\). 0.4 Getting Help For help concerning a known R function, you can use the help function. For instance, to get a listing of information about the seq() function, type the following into the console: help(plot) To get the top-level help page for a package, for example the base package, issue: help(package=base) Help pages are all formatted to have the same sections, the most useful of which are the first three; Decription, Usage and Arguments. These will help remind you how a function works, and what its arguments are. There is a shortcut to loading a help page, simply prefix the name of the function with a question mark: ?plot For broader help regarding statistical programming in R look in the library for books such as: Dalgaard, P. (2000). Introductory Statistics with R. Venables W.N. and Ripley B. D. (1999). Modern Applied Statistics with S-Plus. Compared to these notes, these books offer an alternative explanation regarding how to use R. 0.4.1 Ask questions Beyond searching the Internet, you can ask specific questions at www.stackoverflow.com, a searchable forum of questions and answers about all aspects of computer programming. StackOverflow has answered (and archived) thousands of questions related to R programming. So, it is likely that someone else has already asked your question and also got an answer. Use this to your advantage. You can see the latest questions tagged for R at https://stackoverflow.com/questions/tagged/r . Additionally, you can restrict a Google search to StackOverflow by appending site:stackoverflow.com to the end of your Google search. However, if you a have question that is more about statistical methodology than programming, there are also R users who are active on the the Cross Validated Q&amp;A website, a sister site to StackOverflow. 0.4.2 Keep up-to-date with the R community Read R-bloggers (www.r-bloggers.com), a blog aggregator that reposts R related articles from across the web. R bloggers is a good place to stumble across R tutorials, announcements, and example data analyses. Though other people’s code may not always be the best. 0.5 R scripts Some of you may start to be feeling at least slightly frustrated having to type commands line by line in to the console panel. Particularly, if you are like me and you often make mistakes, meaning that you have to re-enter the whole line. For example, with the moose-wolf data, if we entered one value incorrectly then the whole plot would be invalid. And once the mistake has been corrected, all of the subsequent commands to draw the plot would have to be re-entered. Fortunately, like all programming languages, R allows us to maintain a list of commands in a file we refer to as an R script. This script can then be edited (correcting our mistakes) as well as processing it either as a whole or line-by-line. The main advantage of this is that we can save and close a script, switch off our computers and go home, with the intention of carrying on tomorrow (maybe). 0.5.1 Creating and saving an R script Let us put the commands we used for calculating acceleration due to Earth’s gravity into an R script. To do this in RStudio, go to File &gt; New &gt; Script. This will have created a Script panel, resizing the window to be smaller. The Script panel has some useful features that will be discussed in more detail, but the nicest one is that as we type out commands in the script, they will be colour-coded, making it easier to read. One-by-one, type the commands used for the gravity calculations into the script panel. Your script should look like this (but with colour): G &lt;- 6.6728*10^-11 r_earth &lt;- 6.371*10^6 m &lt;- 5.9736*10^24 dist &lt;- seq(from=0, to=20350*1000, by=1000) r &lt;- r_earth + dist a &lt;- G*m/r^2 dist_km &lt;- dist / 1000 plot(x=dist_km, y=a, type=&quot;l&quot;) In the script panel, all of the usual text editing tools are available; cut, copy, paste, highlight and delete. If you want to split a command over more than one line, then just break it after a comma, or where it is obviously not complete. For example, as far as R is concerned, these two ways of creating the dist object are exactly the same: dist &lt;- seq(from=0, to=20350*1000, by=1000) dist &lt;- seq(from=0, to=20350*1000, by=1000) We can also break calculations over multiple lines, as long as it is obvious that we plan on entering more code, such as ending a line with a plus: 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 ## [1] 36 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 ## [1] 36 When we enter commands into an R script, these are not being processed by R. It is good to think of writing in a script as a way of preparing commands for R. Note that, even with a script open, you are still free to type directly into the console. To do so, simply click into the console panel and start typing. Click on the script to carry on writing there. A good habit when writing scripts is to save frequently. Go to File &gt; Save and save the script as Lab0-Gravity.R, somewhere where you will be able to find it again. You may want to create a folder called R-Programming and save the script in there. If you double click a file with the .R extension, RStudio will automatically try to open it, just like how Word is used to open doc' anddocx’ files. 0.5.2 Running an R script Given that you have typed out and saved the gravity commands, we are now ready to let RStudio run it! To do this highlight all the lines in your script and then click on the run button: By using the highlight-and-run technique we can highlight just small parts of our script that we want to re-run. Also, using the keyboard shortcut Ctrl + Enter (or Ctrl + R) means you do not have to click the run button. And if no code is highlighted, then it will simply send the current line to the console. Another way to run scripts is by sourcing them. To do this you need to know the full path of where the R script is stored, as well as the name of the script. For example, say, I store my gravity script in the following location: H:\\My Awesome R Scripts\\Lab0-Gravity.R. Then in order to source it I would enter into the console the following command: source(&quot;H:/My Awesome R Scripts/Lab0-Gravity.R&quot;) There are two things you must remember; (i) on Windows the backslashes in filepaths must be entered in R as either double backslashes or as forward slashes, and (ii) put the complete file name in quotes. If you have got the path or file name wrong, R will tell you that it cannot find the file. Secondly, if there is something wrong with your code when sourcing, it is likely that R will give you a useful error message, at least making it obvious where the error occurred. But finding what is actually causing the error is 10% of the battle, fixing it is the other 90%. Thirdly we may run R in what is known as batch mode. To do this on Unix/Linux or Mac OS we submit the following command in the Terminal (replacing the spaces in the filename with -), assuming your Terminal is in the same directory as your R script. R CMD BATCH Lab0-Gravity.R or with the command, R --vanilla &lt; Lab0-Gravity.R &gt; Lab0-Gravity.Rout The complication for Windows users is you have to point to the exact filepath of the R.exe executable.So the Windows Command Prompt command is for example as follows: &quot;C:\\Program Files\\R\\R-3.3.0\\bin\\x64\\R&quot; CMD BATCH Lab0-Gravity.R These ways of calling R will generate a text file of output called Lab0-Gravity.Rout. On some systems you can run R using the Rscript command: Rscript Lab0-Gravity.R 0.5.3 Dealing with errors Let us imagine that you have been coding for five days straight on a project (this will happen at some point during the MSc). Your R code is scattered across three or four scripts. Seldom will they run properly at first (this is the case even for experts). So, one activity you have to become familiar with (and hopefully good at) is tracking down and fixing these mistakes. In software development, mistakes like these are called bugs, and the time spent trying to remove them, is time spent debugging. (#fig:cross_stitch)Debugging feels both tedious and satisfying, much like cross stitching (image by Rachel Mckay on Flickr) 0.5.4 Adding comments to your script The best programming habit you could ever develop is putting verbose comments in your code. Any line in your script that begins with # will be ignored by R. Similarly if you use a # in the middle of a line, the remainder of that line will ignored. By using # we are able to add comments to our commands, and even comment-out our code. Type out the following script for calculating gravity, include the comments. Once you have typed out the script be sure to save it, as we will be modifying it soon. # Look at the relationship between acceleration due to # gravity and distance from the Earth&#39;s surface. # ref: http://en.wikipedia.org/wiki/Gravity_of_Earth # Constants ---- # Gravitational constant (m^3 kg^-1 s^-2) G &lt;- 6.6728*10^-11 # Approximate average radius of the Earth (m) r_earth &lt;- 6.371*10^6 # Approximate mass of the Earth (kg) m &lt;- 5.9736*10^24 # Generate distance vector from Earth&#39;s surface (0m) # up to the orbit height of GPS satelites (20,350 km) # at 1km intervals dist &lt;- seq(from=0, to=20350*1000, by=1000) # We measure from the centre of the Earth to the centre of # the other object: r &lt;- r_earth + dist # Earth&#39;s gravitational acceleration based on distance a &lt;- G*m/r^2 # Produce the plot ---- # Set distance to km instead of m for x-axis labels dist_km &lt;- dist / 1000 plot(x=dist_km, y=a, type=&quot;l&quot;) 0.5.5 Summary An R script is a prepared list of R commands that are processed sequentially by R from top to bottom. Using the script editor in RStudio, scripts can be written, edited, saved, and ran. To create a new script go to File &gt; New &gt; R Script. To open an existing scrip, File &gt; Open…. If you have more than one script open, RStudio will create tabs across the top of the script panel. R scripts have the file extension .R, make sure you include this when saving your script for the first time. Running an R script means sending the contents of the script to the console. Portions of the script can be sent by highlighting the relevant code and click the Run button or using the shortcut Ctrl + Enter. Scripts will often contain bugs. Bugs are usually caused by a missing common or bracket, or from an incorrect object name. In order to track down bugs, trying running your script one section at a time. Followed by one line at a time, when you have found the offending section. By starting a line with # we are able to add comments to our code, as R will ignore any text to the right of this symbol. Adding comments, to break-up your code into sections as well as explain the purpose of each section, is crucial for future you to understand the purpose of your script, what it is trying to achieve, and what features are still yet to be added. 0.5.6 Exercise Edit your gravity script so that the gravity plot extends up to the orbit height of geostationary weather satellites (35,880 km from Earth). Source the script to reproduce the plot. Figure 0.2: GOES 10, was an American weather satellite. Launched in 1997, part of its mission was to assist with hurricane predictions in North America; it was retired and manoeuvred to a graveyard orbit in 2009, (Image by NASA) 0.6 Functions Sometimes there are tasks that must be performed again and again. When we notice a repetitive task occurring, it is a good idea to write a function that performs this task, and then we repeatedly call that. For example, we have already covered several functions: c(...) Combines objects often into a vector, where ... represents the objects or values to be combined. seq(from, to, by) Generates a vector containing a seq of numbers specified by the arguments from, to and by. plot(x, y, type=\"p\") Creates a scatter plot of points using arguments x and y as co-ordinates for the points. source(file) Allows commands to be inputted by the R script specified by the argument file. length(x) Returns the number of elements in the vector x Here are some other functions that can be useful when working with vectors: sum(x) Returns the total from adding all the elements in x together prod(x) Returns the total from multiplying all the elements in x. Note that prod is short for product which is the mathematical name given to this process. sqrt(x) Returns a vector of the same length of , but each element is the square-root of the corresponding element in x. max(x), min(x) Returns the maximum and minimum element from a vector, respectively. Try them out: x &lt;- 1:10 sum(x) ## [1] 55 prod(x) ## [1] 3628800 min(x) ## [1] 1 max(x) ## [1] 10 For the sqrt(x) command, we were giving a warning saying NaNs had been produced. NaN is short for “Not a Number” and in this instance they have occurred because we tried to calculate the square-root of a negative number (which do not exist). Once an element becomes Not a Number, that status will persist throughout the rest of our calculations. For example: x &lt;- -2:2 y &lt;- c(10, 5, 0, -5, -10) sqrt(x) + sqrt(y) ## Warning in sqrt(x): NaNs produced ## Warning in sqrt(y): NaNs produced ## [1] NaN NaN 0 NaN NaN Because sqrt(x) and sqrt(y) produced NaNs that were in different places, this resulted in us producing four NaNs when adding the two square-rooted vectors together. 0.6.1 Creating New Functions R, just like nearly all programming languages, allows you to create your own functions. This is great timer saver when you are repeatedly doing the same task or calculation but on different data sets. And writing your own functions is definitely a habit you should develop. In order to explain how to build your own R function. We will revisit the equation for calculating gravity which stated that the acceleration, \\(a\\), due to gravity is dependent on the mass of the Earth, \\(m\\), and how far we are from it’s centre \\(r\\) in metres: \\[ a = G\\frac{m}{r^2}. \\] The following code creates a function called calc_gravity, add it to a new R script: calc_gravity &lt;- function(distance) { # constants G &lt;- 6.6728*10^-11 r_earth &lt;- 6.371*10^6 m &lt;- 5.9736*10^24 # calculation r &lt;- r_earth + distance a &lt;- G*m/r^2 return(a) } Now run the code. You will see nothing really happened. However, what actually happened is that a new function got added to our environment. Check the Environment panel, or use ls() in the console to list the contents of the environment. You should see that our new function is present. To check that it works try: calc_gravity(dist=0) ## [1] 9.820397 This matches our earlier calculations (9.82), so we can trust that we have implemented it correctly. Before we continue, there are a few things that need to be explained about creating a new function. The first statement we make is we define that the object calc_gravity() will actually be a function. A function which has one argument, distance. The name of the function and the arguments are just names that I chose, you may want to consider others that are shorter or easier for you to remember. There are a few guidelines when naming functions: Names should be lowercase. Use an underscore, _, to separate words within a name. Generally, function names should use verbs, as functions things. Strive for names that are concise and meaningful (this is not easy!). Avoid existing function names in R, such as length(). If you stick to these guidelines, then your coding-life just got a lot easier. This is mainly because it is easier for you to remember and guess what you have called your functions.5 The code used to calculate gravity then sits between two curly-brackets. The return(a) statement then defines the output of the function, and should also be the last command inside your function. In this case, we simply return a single value we calculated. But for more complex functions, it could be a vector, a plot or a model. When creating your own functions there are two important things to remember regarding how R stores and interprets them: The objects used inside a function are local to that function. This means that they exist only inside your function, so you do not need to worry if objects exist with the same name. And the values of them are not available outside of the function, hence why we pass everything we want to keep to return() at the end. Think of your function as a guarded sandbox, where no child leaves unless you specifically tell them to. Although we can see our function in the , the new function is not permanent. It will disappear when we exit RStudio. This is why we store our functions in scripts, so we can source them at the start of session to get back what we need. 0.6.2 Default values for function arguments Say, we want to calculate the gravity for the other planets. To do this we would modify our calc_gravity() function, so that it includes arguments for mass and radius of a planet. Right now, it assumes we are only interested in Earth. Make the following modifications in your script: calc_gravity &lt;- function(distance, mass, radius) { # constant G &lt;- 6.6728*10^-11 # calculation r &lt;- radius + distance a &lt;- G*mass/r^2 return(a) } In order to update calc_gravity() so that it includes these additional arguments you will need to run the above code. This means that simply passing the distance will no longer work: calc_gravity(distance=0) ## Error in calc_gravity(distance = 0): argument &quot;radius&quot; is missing, with no default We know have to include mass and radius: calc_gravity(distance=0, mass=5.9736*10^24, radius=6.371*10^6) ## [1] 9.820397 But, say, that 90% of the time when we are doing these calculations they are in the context of Earth. Then to be more efficient, rather than continuously have to give the mass and radius of Earth, we could set the arguments to have these as default values: calc_gravity &lt;- function(distance=0, mass=5.9736*10^24, radius=6.371*10^6) { # constant G &lt;- 6.6728*10^-11 # calculation r &lt;- radius + distance a &lt;- G*mass/r^2 return(a) } This means that if one simply calls calc_gravity() without specifying any arguments, the function will assume they are interested in in the gravity at Earth’s surface. calc_gravity() ## [1] 9.820397 But we are free to override these defaults: # On the surface of Saturn calc_gravity(mass=5.683*10^26, radius=5.8*10^7) ## [1] 11.27275 0.6.3 Summary Functions exist to reduce the amount of repetitive work we do. By creating our own functions, we write out a task or calculation once and then can use it many times. Useful functions for working with vectors include: length(), sum(), prod(), cumsum(), sqrt(), min(), and max(). When creating our own functions we also define the names of the functions arguments, their default values (if any), and what the function returns. In your script: say_hello &lt;- function(person=&quot;World&quot;) { msg &lt;- paste(&quot;Hello,&quot;, person) return(msg) } In the console: say_hello() ## [1] &quot;Hello, World&quot; say_hello(&quot;Bob&quot;) ## [1] &quot;Hello, Bob&quot; Guidelines you should try to stick to when naming functions: Names should be lowercase. Use an underscore, _, to separate words within a name. Generally, function names should use verbs, as functions do things. Strive for names that are concise and meaningful (this is not easy!). Avoid existing function names in R, such as length(). When naming objects, follow the same guidelines but instead use nouns instead of verbs. 0.6.4 Exercises Rewrite your gravity script, implementing the latest function for calc_gravity. And perform the following calculations. You will need to do a bit of research to find the necessary numbers. What is the gravity at the surface of each of the planets in our Solar System (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune)? Produce a plot of the gravity on Mars, from the surface (0m) up to the orbit height of Phobos (Mars’ first moon). Hint: Use the &quot;semi-major axis&quot; of Phobos&#39; orbit as the orbit height. This is because Phobos has an elliptical orbit. Figure 0.3: The Sun and planets of the Solar System. Sizes but not distances are to scale. (Image from Wikimedia Commons) A good recipe book will state all its baking temperatures in terms of Gas Mark, degrees Celsius, and degrees Fahrenheit. A lazy book will pick one, and simply provide a conversion table. The rubbish ones will offer no such table. In those instances, I often have to manually convert the numbers myself. Or rather I write a function that I can repeatedly use (which is also what you are about to do). Create and save a new script for this exercise. Look up the calculation for converting Fahrenheit to Celsius. Implement a function which takes a temperature in Fahrenheit and returns it in Celsius, call it degF_to_degC. Look up the conversion for Fahrenheit to Gas Mark, and implement this as degF_to_gas(). Hint: You may need to make use of the `ceiling()`, `floor()` and `round()` functions: x &lt;- 5.4567218 ceiling(x) ## [1] 6 floor(x) ## [1] 5 round(x) ## [1] 5 round(x, 3) ## [1] 5.457 Implement the conversion of Celsius to Gas Mark, degC_to_gas(), using the previous two functions. Confirm the following approximate conversions: Celsius Gas Mark 140 1 150 2 160 3 180 4 190 5 200 6 220 7 230 8 240 9 The web-browser, Firefox, is one example of many open-source software projects.↩ Tux the penguin is the mascot for the Linux operating system.↩ You may remember this from School as BODMAS: Brackets, Orders (powers, square roots), Division, Multiplication, Addition, Subtraction↩ What does this actually mean? It means that after falling for 1 second we will be travelling at 9.82 metres per second. After 2 seconds, our speed will now be 19.6 metres per second. Every second we fall, our speed increases by 9.82. But as you will soon see, this acceleration is dependent on how far we are from the surface. Though, in practice there is a limit to how fast you can fall, this limit is called your terminal velocity.↩ The same rules apply for objects. Except you should try to use nouns rather than verbs.↩ "],
["read-plot.html", "1 Data manipulation and plotting in R 1.1 Introduction 1.2 Working with data in R 1.3 Data Types 1.4 Logical comparisons and Boolean operations 1.5 Plots", " 1 Data manipulation and plotting in R 1.1 Introduction Revise the Getting Help section 0.4, and all the Summary subsections (0.3.4, 0.5.5, 0.6.3) from previous chapter. Make sure you downloaded the solution to last week’s coursework and compared it against what you submitted. Create a new R script to save the code you write for this chapter. The purpose of this chapter is to allow you to practice importing, manipulating and exporting data. As well as introduce you to the ggplot2 package, a plotting system for R, based on the grammar of graphics, which tries to make it easy to produce complex multi-layered graphics. 1.2 Working with data in R R developers have created functions and packages for almost every laborious task that stands between us and good quantitative scientific analysis. Data input, manipulation, and output is easy in R. It is just a matter of breaking down what needs to be done into small achievable tasks, and knowing the right functions. A data set that has been loaded into R, and is ready for analysis is normally stored by R in something called a data frame. A data frame can be thought of as a table of data where the columns are named vectors, with each vector containing a particular type of data (numeric, string, date, time). These columns are often called variables in statistics. With the rows of the data frame corresponding to individuals or a single observation across the variables. If, in Chapter 0, the information you collected and calculated regarding gravity at the surfaces of the planets in our Solar System was stored as a data frame it would look like: Table 1.1: Mass (kg), average radius (m) of each of the planets in our Solar System. Gravity (ms\\(^{-2}\\)) is calculated at the surface of each planet (distance = 0m). planet mass radius distance gravity Mercury \\(3.30\\times 10^{23}\\) \\(2.440\\times 10^6\\) \\(0\\) \\(3.70\\) Venus \\(4.87\\times 10^{24}\\) \\(6.052\\times 10^6\\) \\(0\\) \\(8.87\\) Earth \\(5.97\\times 10^{24}\\) \\(6.371\\times 10^6\\) \\(0\\) \\(9.80\\) Mars \\(6.42\\times 10^{23}\\) \\(3.390\\times 10^6\\) \\(0\\) \\(3.71\\) Jupiter \\(1.90\\times 10^{27}\\) \\(6.991\\times 10^7\\) \\(0\\) \\(24.79\\) Saturn \\(5.68\\times 10^{26}\\) \\(5.823\\times 10^7\\) \\(0\\) \\(10.44\\) Uranus \\(8.68\\times 10^{25}\\) \\(2.536\\times 10^7\\) \\(0\\) \\(8.69\\) Neptune \\(1.02\\times 10^{26}\\) \\(2.462\\times 10^7\\) \\(0\\) \\(11.15\\) To begin building this directly in R, type out the following into a script and run it. Note that if you have masses and radius already stored in vectors from last week’s script then copy and paste these as appropriate. Reusing your existing vectors from previous chapter to create the planets data frame may look like this: planet &lt;- data.frame( name = c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;), mass = c(3.30*10^23, 4.87*10^24, 5.97*10^24, 6.42*10^23, 1.90*10^27, 5.68*10^26, 8.68*10^25, 1.02*10^26), radius = c(2.440*10^6, 6.052*10^6, 6.371*10^6, 3.390*10^6, 6.991*10^7, 5.823*10^7, 2.536*10^7, 2.462*10^7) ) Check that planet really is a data.frame by typing: class(planet) ## [1] &quot;data.frame&quot; In R the class of objects is very important. The same function, for example summary(), will perform differently depending upon the class of the object. In computer programming this is called “function overloading”. Type planet into the console, and it will display the contents of our newly created data frame. Another way to see the contents, this time in a spreadsheet type view, is to type: View(planet) To access the individual variables we use the dollar syntax: planet$mass ## [1] 3.30e+23 4.87e+24 5.97e+24 6.42e+23 1.90e+27 5.68e+26 8.68e+25 1.02e+26 But we can also use square brackets to select the individual elements of the data frame: # first row, first column planet[1, 1] ## [1] &quot;Mercury&quot; # entire first row planet[1, ] ## name mass radius ## 1 Mercury 3.3e+23 2440000 # entire first column planet[, 1] ## [1] &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; &quot;Jupiter&quot; &quot;Saturn&quot; &quot;Uranus&quot; ## [8] &quot;Neptune&quot; # first 3 rows planet[1:3, ] ## name mass radius ## 1 Mercury 3.30e+23 2440000 ## 2 Venus 4.87e+24 6052000 ## 3 Earth 5.97e+24 6371000 Absolute referencing like this is a bad idea. If the order of your columns change, you may be referring to different columns than you thought you where, likewise for rows. By using the dollar syntax, we are using the name of the column, rather than its location in the data frame. Plus it is easier to understand what is happening when you reread the code, as you will see. If we wanted to calculate the volume of each planet, assuming planets are perfect spheres, we could use the equation for volume: \\[ V = \\frac{4}{3} \\pi r^3, \\] where \\(r\\) is the planet radius: 4/3 * pi * planet$radius^3 ## [1] 6.084965e+19 9.285074e+20 1.083207e+21 1.631878e+20 1.431220e+24 ## [6] 8.270447e+23 6.831819e+22 6.251047e+22 Since we created this data frame, we already know the names of the variable that are contained within it. But say we load a foreign data set, how do we know what the variables are called? Outside R, we hope that this data set also has good documentation, such as a PDF or website telling you all the details you could ever want know. But within R, we can get a list of the attributes of an object by using the names() function: names(planet) ## [1] &quot;name&quot; &quot;mass&quot; &quot;radius&quot; As well as using names(), it is also good practice to check the size of the data frame. We previously used length() to determine how many elements were in a vector. Since, a data frame is two-dimensional, we instead use nrow() and ncol() to determine the number of rows and columns respectively: nrow(planet) ## [1] 8 ncol(planet) ## [1] 3 dim(planet) ## [1] 8 3 Using the dollar syntax we can create new variables: planet$contains_humans &lt;- c(0, 0, 1, 0, 0, 0, 0, 0) planet$radius_km &lt;- planet$radius / 1000 Though there are more elegant ways of creating new variables, as you will see in the next section. 1.2.1 Basic manipulation The easiest way to add distance, and gravity as variables to our data frame is to make use of the dplyr package. Before we can load this package from the R library, you will need to install it: install.packages(&quot;dplyr&quot;) # if asked, choose a UK location You only need to install a package once. But you have to load all the packages you want to use at the start of every session. For this reason, make sure all library(...) commands sit at the top of your script. Add library(dplyr) to yours now and run it. Loading the dplyr package gives us access to four functions: filter() Focus on a subset of the rows of a data frame. arrange() Reorders the rows in the data frame. select() Allows you to zoom in on a useful number of columns. mutate() Easily add new columns that are functions of existing ones. The first argument to each of these functions is always the data frame we wish to work on, the subsequent arguments are the variables we wish to work with. For example, to focus on big planets i.e. those that have radius greater than \\(10^7\\)m we use the filter() function: big_planets &lt;- filter(planet, radius &gt; 10^7) big_planets ## name mass radius contains_humans radius_km ## 1 Jupiter 1.90e+27 69910000 0 69910 ## 2 Saturn 5.68e+26 58230000 0 58230 ## 3 Uranus 8.68e+25 25360000 0 25360 ## 4 Neptune 1.02e+26 24620000 0 24620 To arrange the planets by increasing size, we using the arrange() function: arrange(planet, radius) # increasing by default ## name mass radius contains_humans radius_km ## 1 Mercury 3.30e+23 2440000 0 2440 ## 2 Mars 6.42e+23 3390000 0 3390 ## 3 Venus 4.87e+24 6052000 0 6052 ## 4 Earth 5.97e+24 6371000 1 6371 ## 5 Neptune 1.02e+26 24620000 0 24620 ## 6 Uranus 8.68e+25 25360000 0 25360 ## 7 Saturn 5.68e+26 58230000 0 58230 ## 8 Jupiter 1.90e+27 69910000 0 69910 arrange(planet, desc(radius)) # decreasing ## name mass radius contains_humans radius_km ## 1 Jupiter 1.90e+27 69910000 0 69910 ## 2 Saturn 5.68e+26 58230000 0 58230 ## 3 Uranus 8.68e+25 25360000 0 25360 ## 4 Neptune 1.02e+26 24620000 0 24620 ## 5 Earth 5.97e+24 6371000 1 6371 ## 6 Venus 4.87e+24 6052000 0 6052 ## 7 Mars 6.42e+23 3390000 0 3390 ## 8 Mercury 3.30e+23 2440000 0 2440 If we want to store the data in this newly rearranged form then: planet &lt;- arrange(planet, desc(radius)) If we wanted to remove the column of planet names, effectively anonymising them, we would use the select() function to specify the columns we do want to keep. annon_planets &lt;- select(planet, mass, radius) In order to carry on with our gravity calculations, we need to add a distance column, use mutate() to do this. Using mutate() we can actually add the distance and gravity variables in one call: calc_gravity &lt;- function(distance=0, mass=5.9736 * 10^24, radius=6.371 * 10^6) { # constant G &lt;- 6.6728 * 10^-11 # calculation r &lt;- radius + distance a &lt;- G * mass/r^2 return(a) } planet &lt;- mutate(planet, distance = 0, gravity = calc_gravity(distance, mass, radius) ) When passing variable names to cal_gravity(), mutate() will check if these names correspond to variables inside the specified data frame. This means we do not have to repeatedly use the dollar syntax mentioned earlier to access the needed variables. Which makes our code look clean and easy to read (providing you know how mutate works). If there were no variables inside the data frame matching these names, mutate() would then search the local environment. Finally, stating an error if it could not find any object matching the name specified. Check what the data contained with planet now looks like. Next, arrange the rows by increasing gravity. Where does Earth rank among the Solar System? Are there any other planets were we would either (a) not be crushed to death, nor (b) float off into space if we fell over? 1.2.2 Matrices To take a bit of a side-step, I just want to explain a bit about matrices in R because they are similar to data frames but with a few restrictions. Though we will not be making use of matrices in this module, you will in the more-theoretical statistics modules. The word matrix is a mathematical term for a two-dimensional array of numbers, with a vector being a one-dimensional array. To create a matrix we can simply bind vectors together as columns: x &lt;- 1:5 y &lt;- 6:10 z &lt;- 11:15 mat &lt;- cbind(x, y, z) mat ## x y z ## [1,] 1 6 11 ## [2,] 2 7 12 ## [3,] 3 8 13 ## [4,] 4 9 14 ## [5,] 5 10 15 Or we can create one directly, using the matrix() function: mat &lt;- matrix(1:15, nrow=5, byrow=T) mat ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 ## [5,] 13 14 15 You can only access the data within a matrix using the square bracket notation. And, all the data in the matrix has to be of the same type. As a result of these restrictions you would not use a matrix to store data as information, but instead you would use matrices to perform algebraic calculations, such as matrix multiplication, or calculating the determinant of a matrix. 1.2.3 Exporting data Much like when we create custom functions, data frames only exist in our local environment. Which, again, means that when we quit R they will disappear. A good way to export a data frame is to write it as a CSV file to your H: drive. CSV stands for Comma Separated Values, and any self-respecting data analysis software will be able to read a CSV file. To write to a data frame to CSV from R, we make use of the write.csv() function specifying the full file path including the file name and extension: To see why we use the argument row.names=FALSE, set it to TRUE, rerun the command, and see if you can spot the difference in the file. Where are these row names coming from? Type row.names(planet) into the console. Row names should just be the equivalent of row indexes i.e. nothing of actual value. Therefore they do not need storing. filename &lt;- &quot;H:/My Awesome R Scripts/planet.csv&quot; # replace by actual path write.csv(planet, file=filename, row.names=FALSE) This will write a CSV file to the path you have specified. Locate the file, right click and choose to open with a text editor, such as Notepad++. The way the data is laid out in the file should follow the same structure as it appeared in the data frame, but the efficient formatting makes it difficult to read with your human eyes. One thing is clear though, and that is the first row contains the variable names, with the data existing in the remaining rows. If you double-click the CSV file, Excel will probably try to open it, you should try this. Excel is smart enough that will have automatically recognised the format of the CSV file. Other general data file formats exist, these involve separating values using different methods (tabs or whitespace), see Table 1.2. To work out how to use these write functions and discover how they write your data to a file, check their respective help pages, and open up the resulting file with a text editor, like you did with the CSVs. Table 1.2: General formats and functions for importing and exporting data. Name Extension Write function Read function CSV .csv write.csv() read.csv() Tab-delimited .tab write.table() read.table() Fixed Width Format .fwf read.fwf() The base R packages do not have a write function for FWF, this is because the other two methods are considered sufficient. 1.2.4 Importing data As you can see, while we have write.*() functions we also have equal and opposite read.*() functions. Try importing the planet.csv file that you produced: dat &lt;- read.csv(file=&quot;planet.csv&quot;) Take a look inside dat, if it looks the same as your planet data frame, then good job! In general, before you import a data set, try to inspect the file using a text editor to confirm the structure. Once you have identified the structure, and if it happens to be something standard, like a CSV, or tab-delimited file, then go ahead and use those respective functions to read it in. But in the event of it being something a bit more special, like colon separated values, you can setup read.table() to import the data correctly. The read.table() function is considered the Swiss Army knife of the read.*() family6. To see all of the possible arguments for read.table() check its help page. 1.2.5 Using a working directory Previously, when importing and exporting data sets, we said we had to specify a complete file path such as: filename &lt;- &quot;H:/My Awesome R Scripts/planet.csv&quot; # again, replace by actual path write.csv(planet, file=filename, row.names=FALSE) However, R has something called a working directory. If set, anything we save, such as data sets and plots, will be saved to this location7. Now, of course, if we specify absolute paths, like the one above then we are effectively overriding this feature. To get the path of your current working directory type into the : getwd() You can change this, by setting it to a different directory using using setwd(), for example: setwd(&quot;H:/My Awesome R Scripts&quot;) If I were to run the following command, the filename is now considered relative, i.e. it does not tell R exactly where to save the data. In this case, R assumes it should use the working directory. filename &lt;- &quot;planet.csv&quot; write.csv(planet, file=filename, row.names=FALSE) So, by setting the working directory at the start of a script, we make our importing-and-exporting lives easier, because we do not have to verbosely state where we want to save something every single time. Also, if future you ever needs to open a script to see where it is saving everything, then this will be obvious from the setwd() near the top of the script. A relative file path can include folders: filename &lt;- &quot;data/planet.csv&quot; write.csv(planet, file=filename, row.names=FALSE) This assumes that there is folder called “data” in my working directory. R will complain if it does not exist. One final thing about why it is good to use relative paths and “working directories” is that it makes your code more portable. By using relative references, you can bundle your code and data together, allowing someone else to run and modify it on their own system. It is worth noting that when writing an Rmarkdown file (.Rmd) RStudio assumes that the working directory is the directory that the .Rmd file is saved in. 1.2.6 Summary Data frames are the standard object for storing data in R. Each column in a data frame is considered to be a variable, such that the type of data it contains (strings vs numbers) can differ from the other columns. Accessing data can be done using both the dollar syntax and the square-bracket syntax: planet$name ## [1] &quot;Jupiter&quot; &quot;Saturn&quot; &quot;Uranus&quot; &quot;Neptune&quot; &quot;Earth&quot; &quot;Venus&quot; &quot;Mars&quot; ## [8] &quot;Mercury&quot; planet$name[1:2] ## [1] &quot;Jupiter&quot; &quot;Saturn&quot; planet[, 1] ## [1] &quot;Jupiter&quot; &quot;Saturn&quot; &quot;Uranus&quot; &quot;Neptune&quot; &quot;Earth&quot; &quot;Venus&quot; &quot;Mars&quot; ## [8] &quot;Mercury&quot; planet[1:2, 1] ## [1] &quot;Jupiter&quot; &quot;Saturn&quot; To manipulate data on a much larger and general scale, it is more efficient to use the functions provided by the dplyr package, namely: Function Action filter() Focus on a subset of the rows of a data frame arrange() Reorders the rows in the data frame select() Allows you to zoom in on a useful number of columns mutate() Add new columns that are functions of existing ones The first argument to each of these functions is always the data frame we wish to work on. With the subsequent arguments involving the names of the variables we wish to operate on or with. To export data use either the write.table() or write.csv() functions. Make sure to set the parameter row.names=FALSE. Check the help pages for each for more details. Both have these have sister functions; read.table() and read.csv(). With read.table() having lots of options that can be modified to read in any structured data file. R has a working directory, use getwd() to find out what it currently is. And use setwd() to change it. Equally, there is a menu in RStudio that allows you to do the same thing. Working directories are good, as they allow you to use relative paths, rather than absolute ones. # relative file names setwd(&quot;H:/Awesome R Scripts/&quot;) write.table(planet, file=&quot;planet.csv&quot;, row.names=FALSE) write.table(letters, file=&quot;letters.csv&quot;, row.names=FALSE) # absolute file names write.table(planet, file=&quot;H:/Awesome R Scripts/planet.csv&quot;, row.names=FALSE) write.table(letters, file=&quot;H:/Awesome R Scripts/letters.csv&quot;, row.names=FALSE) It can be a good idea to set your working directory at the top of your script. 1.2.7 Exercises Now complete the following exercises: Working on your planet data frame: Add the volumes for each planet as new variable using the dollar syntax. Using mutate() add variables with the following definitions: * `mass_earths` The mass of a planet as a proportion of the mass of Earth. * `volume_earths` Express `volume` as a proportion of Earth&#39;s volume. * `gravity_earths` Express `gravity` as a proportion of Earth&#39;s gravity. Create a new data frame which only contains the names of the planets, and each of the measurements expressed as a proportion of Earth.8 Inspect this data frame for information on the other planets relative to Earth. Make sure you are familiar with writing and reading from data files that are Tab-delimitted or CSV. For each file format: Look-up the help page, see the arguments needed to use the particular write.*() function. Store the planet data frame with the appropriate file extension. Can you open it in Excel? Can you read it back into R? 1.3 Data Types To determine the data type of an object use the class() function. Type in the : class(planet$mass) ## [1] &quot;numeric&quot; class(planet$name) ## [1] &quot;character&quot; class(c(&quot;peanut&quot;, &quot;seed&quot;)) ## [1] &quot;character&quot; class(TRUE) ## [1] &quot;logical&quot; Notice how the names of the planets are classed as a factor whereas the vector of strings (peanut and seed) is classed as a character? We will discuss this soon. More generally, class() can be applied to any object: class(planet) ## [1] &quot;data.frame&quot; class(plot) ## [1] &quot;function&quot; When you start getting a lot of errors, class() can be used to sanity-check your objects; ``Is planet actually a data frame?’’ 1.3.1 Numeric Decimal values are stored as numeric data in R. And it is the default computational data type. If we assign a decimal value to a variable x as follows, x will be of type numeric: x &lt;- c(10.5, 19.2, 1) class(x) ## [1] &quot;numeric&quot; Furthermore, even if we assign a vector of integers to a variable k, R will still store it as numeric: k &lt;- c(1, 2, 10, 33) class(k) ## [1] &quot;numeric&quot; 1.3.2 Factors It is common in statistical data to have categorical variables, indicating some subdivision of the data, such as social class, primary diagnosis, tumour stage, gender, species, etc. Such variables are stored in data files often as strings indicating their actual value, though abbreviations may be used; “m” for male, and “f” for female. But, due to the actual number of observations, it may be more sensible to store the categorical data using numerical codes; 1 for male, 2 for female. With their meaningful names then being store in the documentation for the data. Regardless of how they are originally stored, in R they should be converted to factors. A factor is a data structure which stores the categorical data as numerical codes, and the labels which make the codes meaningful. For example, say we have a pain variable that records what level of pain our patients are in using a four-point scale: pain &lt;- c(0, 1, 3, 2, 2, 1, 1, 3) pain_f &lt;- factor(pain, levels=0:3, labels=c(&quot;none&quot;, &quot;mild&quot;, &quot;medium&quot;, &quot;severe&quot;)) pain_f ## [1] none mild severe medium medium mild mild severe ## Levels: none mild medium severe # if you need to work with the actual string labels pain_c &lt;- as.character(pain_f) pain_c ## [1] &quot;none&quot; &quot;mild&quot; &quot;severe&quot; &quot;medium&quot; &quot;medium&quot; &quot;mild&quot; &quot;mild&quot; &quot;severe&quot; This factor data structure is so routinely used by data scientists that R will automatically convert strings to factors when creating a data frame which includes using any of the read.*() functions. Hence why, even though we originally specified the planet names as a vector of strings, it is now being stored as a factor in R. After the factor has been created, we can modify its levels, either all at once or individually: # what are the levels? levels(pain_f) ## [1] &quot;none&quot; &quot;mild&quot; &quot;medium&quot; &quot;severe&quot; # change all levels levels(pain_f) &lt;- c(&quot;none&quot;, &quot;uncomfortable&quot;, &quot;unpleasant&quot;, &quot;agonising&quot;) levels(pain_f) ## [1] &quot;none&quot; &quot;uncomfortable&quot; &quot;unpleasant&quot; &quot;agonising&quot; # change only the first level levels(pain_f)[1] &lt;- &quot;absent&quot; levels(pain_f) ## [1] &quot;absent&quot; &quot;uncomfortable&quot; &quot;unpleasant&quot; &quot;agonising&quot; 1.3.3 Data and time A date or timestamp typically looks like: 2014-10-09 01:45:00 This follows the format of YYYY-MM-DD hh:mm:ss. But there are a number of other ways of laying out the same information; Americans like to switch the month and day around; the time-part may occur before the date part; we may not have seconds; we may use a letter abbreviation for month. The lack of consistency of how people write timestamps can make them a headache to deal with, especially when we are merging data from two different sources. By default, R will make no attempt to parse a date or timestamp as such. Instead R will silently read them as strings, and thus will be turned into a factor. This is not useful, and we will cover how to override this behaviour using the lubridate package in Chapter 4, where we deal with parsing differently formatted timestamps, and the tedious issue of time zones. 1.3.4 Exercises Mercury, Venus, Earth and Mars are called the terrestrial planets, as they are primarily composed of rock and metal. Jupiter and Saturn, are composed mainly of hydrogen and helium, thus are referred to as gas giants. Finally, Uranus and Neptune, are composed largely of substances with relatively high melting points, thus are often referred to separately as ice giants. Create a factor variable, called type, in the planet data frame with these categories. Make sure the each planet has the correct label. Convert the contains_humans variable inside planet to a factor with 0 labelled as \"no\", and 1 as \"yes\". See what happens when you convert a factor back to numeric using the as.numeric() function, and how it differs from as.character(). 1.4 Logical comparisons and Boolean operations We have seen that setting up scientific information in the form of one or more vectors is often convenient for performing additional calculations as well as plotting. R is designed around the idea and use of vectors. Data frames are essentially a collection of vectors, but with a bit of extra useful functionality. Furthermore we have seen that R comes with tools for extracting and manipulating the information in vectors and data frames. In practice, we often have to extract data that satisfy a certain criteria, such as all the data for females, or all those with a particular disease. This is when we start to make statements involving logical comparisons. 1.4.1 Logical comparisons We have previously seen, that we can select parts of a vector using indexing: planet$name[2:5] ## [1] &quot;Saturn&quot; &quot;Uranus&quot; &quot;Neptune&quot; &quot;Earth&quot; But if we want to specifically look at particular type of planet, then without having to calculate the specific numerical index we can use filter: filter(planet, type == &quot;ice giant&quot;) ## name mass radius contains_humans radius_km distance gravity ## 1 Mars 6.42e+23 3390000 0 3390 0 3.727724 ## 2 Mercury 3.30e+23 2440000 0 2440 0 3.698643 ## type ## 1 ice giant ## 2 ice giant Where == means “equal to”, see Table for other operators. In fact, you can practically read that last line of code as a sentence; “Filter the rows in the planet data frame such that the type is equal to ice giant”. Table 1.3: A list of the logical comparison operations available in R. Symbol Comparison &lt; Less than &gt; Greater than &lt;= Less than or equal to &gt;= Greater than or equal to == Equal to != Not equal to Note that when you see a logical comparisons, such as type == \"ice giant\", R is actually returning a vector of TRUEs and FALSEs: planet$type == &quot;ice giant&quot; ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE This is then being used to filter the data frame, returning only the rows that correspond to TRUE. 1.4.2 Boolean operations The real fun begins when you start combining logical comparisons with Boolean operations. If you are adept at searching the Internet i.e. your Google-Fu is strong, then you will likely be familiar with the phrases; “and”, “or”, and “not”. In R, these words are represented by the symbols is shown below. Table 1.4: A list of the Boolean operators available in R. Symbol Phrase &amp; And (ampersand) | Or (vertical bar) ! Not or negation (exclamation) The operators &amp; and | connect two logical comparisons and return TRUE or FALSE depending on the joint truth or falsehood of the two logical comparisons. We also use brackets to help visually separate out the different conditions. filter(planet, (gravity &gt; 5) &amp; (radius &lt; 7*10^6)) ## name mass radius contains_humans radius_km distance gravity type ## 1 Earth 5.97e+24 6371000 1 6371 0 9.814479 gas giant ## 2 Venus 4.87e+24 6052000 0 6052 0 8.872362 gas giant filter(planet, (gravity &gt; 5) | (radius &lt; 7*10^6)) ## name mass radius contains_humans radius_km distance gravity ## 1 Jupiter 1.90e+27 69910000 0 69910 0 25.940784 ## 2 Saturn 5.68e+26 58230000 0 58230 0 11.177968 ## 3 Uranus 8.68e+25 25360000 0 25360 0 9.005946 ## 4 Neptune 1.02e+26 24620000 0 24620 0 11.228770 ## 5 Earth 5.97e+24 6371000 1 6371 0 9.814479 ## 6 Venus 4.87e+24 6052000 0 6052 0 8.872362 ## 7 Mars 6.42e+23 3390000 0 3390 0 3.727724 ## 8 Mercury 3.30e+23 2440000 0 2440 0 3.698643 ## type ## 1 terrestrial ## 2 terrestrial ## 3 terrestrial ## 4 terrestrial ## 5 gas giant ## 6 gas giant ## 7 ice giant ## 8 ice giant The &amp; returns TRUE only when both of the comparisons are true. While | returns TRUE if at least one of the comparisons is true. 1.5 Plots It might seem to you that scientists have an obsession with quantification. Your impressions would be correct. To build reliable, reproducible results, scientists naturally seek to gather and record counts, measurements, and attributes of the phenomenon under observation. Data, the collective name for such counts, measurements, and attributes, are the lifeblood of science. Studying data with quantitative tools allows scientists to detect patterns, make predictions, and assess the reliability of current theory. Being able to visualise data is crucial to this endeavour. Even a small data set is practically incomprehensible all by itself. Plotting data allows us to visualise the relationships we are interested in. Visualisation can be used for two things; (a) for the computer to show the scientist what is happening in the data, and (b) for you to show other people. Graphs that fall into (a) tend to be produced quicker and dirtier than the graphs we produce for (b). Finally, different types of graphs emphasise different aspects of the data and variables under study. Building a good graph therefore takes time and is often an iterative process. 1.5.1 Getting started Before we begin, you should start a new R script, call it something like chp-1-plots.R. One more thing is that within RStudio you need to install a package: install.packages(&quot;ggplot2&quot;) If you are on a computer system where you need to install additional packages in a specific folder you can use the lib=\"filepath\" argument. Once it has installed add library(ggplot2) to the top your new script. This loads the package into your current R session. You can see all the packages you have loaded in your session using: sessionInfo() Note, if you want to remove a package from your session type (do not do this now): detach(package:ggplot2) Also, if when you load in a package and it has a function with the same name as another package that is already loaded in your R session. You will get a warning that one of the function names masks the function from the other package. In such situations, you ensure to use the function from the specific package by prefixing the function name by packagename::, i.e. base::print() ensures you use print() from the base package. The ggplot2 package is a plotting system for R, based on something called the grammar of graphics. By default, ggplot2 takes care of many of the fiddly details that make plotting a hassle when using the plotting functions provided by base R (like drawing legends, and picking good colours). It also provides a powerful framework which makes creating complex graphics easy.9 When you see analyses performed by various data bloggers they will often be using ggplot2 to visualise their data. To see the many functions this package provides view its top level help-file with help(package=ggplot2) 1.5.2 Diamonds data Let us utilise the functionality of ggplot2 by exploring a data set describing the prices and other attributes of 53,940 diamonds. Make sure your working directory is setup correctly, read the data into R. Check it has loaded properly by comparing the dimensions to what is shown below: data(diamonds) dim(diamonds) ## [1] 53940 10 The ten variables are described in Table 1.5. Table 1.5: Variable description of the diamonds data set. Ranges of each variable are stated in brackets. Variable Description price price in US dollars ($326–$18,823) carat weight of the diamond (0.2–5.01) cut quality of the cut (Fair, Good, Very Good, Premium, Ideal) colour diamond colour, from J (worst) to D (best) clarity measurement of how clear the diamond is: I1 (worst), SI1, SI2, VS1, VS2, VVS1, VVS2, IF (best) x length in mm (0–10.74) y width in mm (0–58.9) z depth in mm (0–31.8) depth total depth percentage \\(= 2\\times z/(x + y)\\) (43–79) table width of top of diamond relative to widest point (43–95) How the different measurements relate to a diamond are shown in Figure 1.1. With table then being calculated as follows: \\[\\begin{eqnarray*} \\text{depth} &amp;=&amp; \\text{z / diameter}\\\\ \\text{table} &amp;=&amp; \\text{(width / x) * 100} \\end{eqnarray*}\\] Figure 1.1: Measuring a diamond. To check how the data looks type into the R Console: head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 tail(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.72 Premium D SI1 62.7 59 2757 5.69 5.73 3.58 ## 2 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.5 ## 3 0.72 Good D SI1 63.1 55 2757 5.69 5.75 3.61 ## 4 0.7 Very Good D SI1 62.8 60 2757 5.66 5.68 3.56 ## 5 0.86 Premium H SI2 61 58 2757 6.15 6.12 3.74 ## 6 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 Make sure that the levels for each of the factors are in the correct order, because, by default, R will simply order the levels alphabetically. levels(diamonds$cut) ## [1] &quot;Fair&quot; &quot;Good&quot; &quot;Very Good&quot; &quot;Premium&quot; &quot;Ideal&quot; To fix this, apply the factor() function, but with the levels in the correct order: cut_levels &lt;- c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;) diamonds$cut &lt;- factor(diamonds$cut, levels=cut_levels) Check the other factors, and correct them if necessary. 1.5.3 Histograms A histogram shows the distribution of a single numeric variable, by showing us the frequency with which certain intervals of the data occur. Use qplot() to produce a histogram of the carat distribution: qplot(x=carat, data=diamonds, geom=&quot;histogram&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Because the data is strictly continuous, it needs to be binned. By default ggplot2 splits the data up into 30 equally-sized intervals (bins). To alter this: qplot(x=carat, data=diamonds, geom=&quot;histogram&quot;, binwidth=0.1) Always experiment with the bin width. Try increasing it slowly up to 1. Pay attention to how features in the data become smoothed out? This histogram shows the carat distribution for all of our data. But does this general shape hold if we look at different categories of diamond? For example, does the carat distribution change depending on the cut. The easiest way to do this is to, graphically, reproduce the histogram for each type of cut. This process is called faceting. To create a faceted plot using the cut variable modify the previous code so it now looks like the following and run it: qplot(x=carat, data=diamonds, geom=&quot;histogram&quot;, binwidth=0.25) + facet_wrap(~ cut, ncol=5) So, it looks like the distribution stays fairly similar across each of the cut types. But it is hard to really say with any confidence, because the number of diamonds with a particular cut varies, considerably. It may be more useful to look at boxplots. 1.5.4 Boxplots A boxplot (sometimes called a box-and-whisker plot) is an alternative to producing a histogram, when we want to look at the distribution of a variable but for different categories of another variable. For example, the carat distribution (continuous) with each of the different cut types (categorical). qplot(x=cut, y=carat, data=diamonds, geom=&quot;boxplot&quot;) Each set of box and whiskers relies on five numbers to summarise the carat distribution for each particular cut. The middle three values are the 25%, 50% and 75% quantiles. To calculate these we simply order the data from lowest to highest, and then lookup the values that are 25%, 50% and 75% along the vector. The 50% value is more commonly know as the median, while the 25% and 75% are referred to as lower and upper quartiles (Q1 and Q3 respectively) because they are a quarter into the data range, from either end. # Median by hand x &lt;- sort(diamonds$carat) x[1/2 * length(x)] ## [1] 0.7 # Using in-built function median(diamonds$carat) ## [1] 0.7 Similarly for the quantiles: # Quantiles by hand x[c(0.25, 0.75) * length(x)] ## [1] 0.40 1.04 # Checking with the in-built function quantile(diamonds$carat, c(0.25, 0.75)) ## 25% 75% ## 0.40 1.04 The two quartiles are then used to draw the main box, with the median being the solid line inside the box. How are the whiskers calculated, and what are the other points? The other points are considered outliers, this means that they are thought to deviate too far from the rest of the data to be considered a part of the main distribution. How we identify if something is an outlier is related to the length of the whiskers. The whiskers stem from the main box up and stop at either the range of the data or if they hit the following limits: the lower whisker stops at the Q1 - 1.5\\(\\times\\)IQR the upper whisker stops at the Q3 + 1.5\\(\\times\\)IQR Where IQR represents the Inter-Quartile Range; IQR = Q3 - Q1, also know as the length of the box. Due to the definitions of Q1 and Q3, the box represents 50% of the data. Finally, because boxplots and histograms are effectively trying to summarise the same data, therefore patterns in one correspond to patterns in another, as shown in the margin. 1.5.5 Scatter plots A scatter plot can be used when both variables of interest are numeric (continuous). A scatter plot contains a point for each observation in the data. We can use scatter plots to explore the relationship between the price of the diamond and the other attributes. Start with carat: qplot(x=carat, y=price, data=diamonds, geom=&quot;point&quot;) So, we see a general positive relationship between the weight of the diamond (carat) and it’s price. It might be reasonable to assume that it’s price is also related to it’s cut. qplot(x=carat, y=price, colour=clarity, data=diamonds, geom=&quot;point&quot;) We can see some separation according to colour. This may be improved by faceting the plot by clarity, similar to what we did for the histograms. Try doing this now. 1.5.6 Bar charts When wanting to look at the distribution of a categorical variable we use a bar chart: qplot(x=clarity, data=diamonds, geom=&quot;bar&quot;) We can fill in the bars with colour according to the cut: qplot(x=clarity, fill=cut, data=diamonds, geom=&quot;bar&quot;) What happens if you set the colour argument to be equal to cut, instead of using fill? 1.5.7 How to save plots There is no point in making pretty graphs if you cannot include them in a report or blog post. To save your amazing art work, first you need to store the plot in an object: p_clarity &lt;- qplot(x=clarity, data=diamonds, geom=&quot;bar&quot;) Because we are now storing the plot inside an object, R no longer automatically prints it to the screen. Simply enter the object name in the console to see it. To save the plot to a file, we pass the object to theggsave() function: ggsave(p_clarity, file=&quot;p_carat.png&quot;, width=6, height=4) The width and height are automatically assumed to be specified in inches. If you prefer to use mm or cm then set the units argument according. Check the help page for other arguments. When saving plots to include in a report, the maximum you should be setting your width to is about 6 inches (15cm), so that it does not extend beyond the margins on an A4 page. After saving the clarity bar plot, try importing it into a Word document. If it looks too big, do not resize it in Word, instead alter the values used by ggsave(). This way, after a bit of tweaking, you will have a script that produces the exact graph you want to use. 1.5.8 Going deeper with ggplot We have been making heavy use of the qplot() function, but we need to go deeper. In order to modify other parts of the graph such as axis labels, the number of tick marks, and the colouring scheme that is used, we need to become pro-ggplotters. To become pro we need to stop using qplot() and start using it’s big sister ggplot(). First we will recreate some of the graphs we have already seen, so that you spot the subtle differences in code. To produce the scatter plot of carat vs price: # simple way qplot(x=carat, y=price, colour=clarity, data=diamonds) + facet_wrap(~ clarity) # advanced way ggplot(diamonds) + geom_point(aes(x=carat, y=price, colour=clarity)) + facet_wrap(~ clarity) For boxplots of carat vs cut: # simple way qplot(x=cut, y=carat, data=diamonds, geom=&quot;boxplot&quot;) # advanced way ggplot(diamonds) + geom_boxplot(aes(x=cut, y=carat)) By using ggplot() we are now starting to use the layering approach to making graphics. The only thing we give ggplot() is the data frame. Next we start to add-on our “geom”s, these are the different shapes and summaries that can be plotted. Here are some of the geoms available in ggplot2, a full list is at http://docs.ggplot2.org : geom_abline geom_area geom_bar geom_bin2d geom_boxplot geom_contour geom_density geom_density2d geom_dotplot geom_hex geom_histogram geom_hline geom_jitter geom_line geom_point geom_rug geom_smooth geom_text geom_violin geom_vline In order to use a geom we need to map our variables to the different aesthetics it uses. Aesthetics of a geom include; position on the x-axis and y-axis, colour, size, transparency, and more. We map our variables to the aesthetics through the aes() function within the particular geom_*(). In order to look at cut vs clarity, we previously drew a boxplot. We can take this further by overlaying points on top of the boxplots: ggplot(diamonds) + geom_boxplot(aes(x=cut, y=carat)) + geom_point(aes(x=cut, y=carat)) This does not look pretty. In fact, there is probably a lot of overplotting going on, to improve this try swapping out geom_point for geom_jitter and rerunning the plot. What does it look like now? Well, now we can not see the boxplots. Try making the points semi-transparent by setting their alpha transparency level to be small: ggplot(diamonds) + geom_boxplot(aes(x=cut, y=carat)) + geom_point(aes(x=cut, y=carat), alpha=0.5) “Alpha” in graphics refers to how transparent something is, usually pixels. Alpha values can range between \\(1\\) (solid) to \\(0\\) (invisible). By playing with the alpha value you can begin to get an impression of how stacked the data is. Try lowering it till you can start to see some of the shapes in the data. I do not think we are ever going to be able to see the boxplots below the points, so go ahead and swap the lines of code for each geom around, and rerun the plot. Within the boxplots, we can turn off the fill of the boxes by setting fill=NA, and turn the colour of the lines used for the boxes and whiskers to white: ggplot(diamonds) + geom_jitter(aes(x=cut, y=carat), alpha=0.1) + geom_boxplot(aes(x=cut, y=carat), colour=&quot;white&quot;, fill=NA, outlier.colour=NA) Look-up the help page for labs, and use it to rewrite the axis labels so that they start with capital letters. Also, look up the help page for theme_bw, and see how to add this to your plot. To save this to a file, do what you did before; store it to an object and use ggsave(): p_cut_carat &lt;- ggplot(diamonds) + geom_jitter(aes(x=cut, y=carat), alpha=0.1) + geom_boxplot(aes(x=cut, y=carat), colour=&quot;white&quot;, fill=NA, outlier.colour=NA) + # plus extra code for labels and theme ggsave(p_cut_carat, file=&quot;p_cut_carat.png&quot;, width=5, height=3) 1.5.9 Summary The base environment in R provides plotting functions such as plot(), hist(), and many others. But these plotting functions often have inconsistent arguments, nor do they make it easy to customise the graphs that are produced, nor do they frequently have sensible defaults for things like legends or colours. The ggplot2 package attempts to alleviate these problems by providing a layered system to creating graphics. An example of a scatter plot: ggplot(diamonds) + geom_point(aes(x=carat, y=price), alpha=0.1) An example of a barchart: ggplot(diamonds) + geom_bar(aes(x=cut)) An example of a histogram: ggplot(diamonds) + geom_histogram(aes(x=price), binwidth=1000) An example of a boxplot: ggplot(diamonds) + geom_boxplot(aes(x=cut, y=price)) Aesthetics of a geom generally include: x; y; size; colour; fill; shape; group; alpha. But there are others for specific geoms, check the help pages or online documentation for more information. When producing graphics for a printed report (like on actual physical A4 paper), then append theme_bw(base_size=10) as an extra function to your ggplot command. This will switch the formatting of the plot so it is less ink-intensive and also change the font size to be 10pt. Use the ggsave() function to save the plot as a PNG file, width and height are specified in inches by default: ggsave(p_cut, file=&quot;p_cut.png&quot;, width=3, height=6) Documentation is at http://docs.ggplot2.org and there have been many ggplot2 questions answered on StackOverflow. 1.5.10 Exercises Explore the diamonds data set some more. In particular, look at how the different attributes relate to price. Try to visualise several different attributes at once in relation to price, by mapping different variables to several of the available aesthetics. Hint: check the “Aesthetics” section of the help page for each geom to see what aesthetics can have variables assigned to them. Once you have identified a plot you like the look of make sure that you have a script you can run with source which will save this plot to a file. Download the titanic.csv file from the module page on moodle. This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner Titanic. The data set contains information on whether or not the passengers survived, along with information on economic status (class), sex, and age, see Table 1.6. Table 1.6: Variable description of the Titanic data set. Variable Levels class 1st, 2nd, 3rd, Crew sex m, f age child, adult survived no, yes The plan is to explore this data set using graphs. So, start a new script, and add your packages to the top along with an initial comment. Write your code to read in the data set, and make sure the data is actually read in correctly. Rewrite the levels for sex, so we have male and female' instead ofmandf`, respectively. First, draw a bar chart showing the number of those that did anddid not survive. Save this to a file. Now consider, separately, how class, sex and age relate to whether or not someone survived. Make comments in your script about any observations you make. Tip: You can change the way your bar charts look by altering the position argument within geom_bar(), by default it is set to position=\"stack\", alternatives include \"fill\" and \"dodge\": geom_bar(..., position=\"dodge\") How might you change the bar charts so instead of looking at counts you are looking at percentages? (You will need to research this). For example, rather than looking at how many males vs females died, we would like to see what percentage of males survived, vs what percentage of females. Once you have this working, reconsider class, sex, and age. Look at two attributes at once with survived. How would you produce plots that only consider the relationships between class, sex, age for passengers, and ignore crew? Do this. FYI read.csv() is just a wrapper for the heavy-lifter that is read.table(), but changes the defaults to suit CSV files. R developers think of everything!↩ If you are not familiar with the term directory, we just mean folder. Technically speaking there is a difference between a directory and a folder.↩ The only name for this object I could think of was planet_earths which I think is ugly, see if you can do better.↩ I should say easier than normal.↩ "],
["loops-sim.html", "2 Loops, Conditional Statements, and Simulation 2.1 Before you start 2.2 Loops 2.3 Conditional statements 2.4 Probability and Simulation 2.5 Combining Loops and Simulation", " 2 Loops, Conditional Statements, and Simulation 2.1 Before you start Compare your submission for last week’s coursework with the solution that is on moodle. Look for differences. Make sure to integrate any improvements in style into this week’s code. Revise the Summary sections on Data Manipulation and Basic Plots from last week. Also scan through the section on Logical Comparisons. You will be making use of this knowledge this week. As usual, to save the code you write for this week, create a new R notebook (or script) in your R programming folder. Write a few introductory comments at the top stating that this script covers the basics of looping, writing conditional statwments, and simulating probabilities. Add your favourite libraries: library(ggplot2) library(dplyr) Or indeed, library(tidyverse) As you work through each section in the notes, consider adding a section break in your script. Note that in an R script the new sectioning markup in RStudio works by detecting the name within # ---- such as # Loops ---- as a section title in your script. You can then use RStudio editor controls to jump to different places in your script. Or use markdown section headings in your R markdown notebook, e.g. # Level 1 heading ## Level 2 heading For each of the exercise questions consider starting a new script entirely. The purpose of this chapter is to get you writing and thinking about for loops and conditional statements, also known as if statements. The second half of the chapter focuses on probability simulation. 2.2 Loops Wrapping a block of code inside a for loop, means that the code can be repeatedly executed. Because of this, a for loop is classified as an iteration statement; we repeatedly iterate over the same code, with, typically, only one or two parameters changing each time. For instance, a for loop can be used to make the following sequence of numbers. Can you see the pattern? 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, … The sequence is known as the Fibonacci sequence which is calculated as follows; starting with 1 and 1, each subsequent number is the sum of the previous two. We will use this sequence as an example for learning about loops in R. First, we will need to cover another way to pick out individual elements from a vector. We have seen in previous chapters, that we can use filter() to reduce a data frame. And we have seen that we can pass a logical vector (a vector containing a series of TRUE and FALSE values) to select out elements of a different vector: x &lt;- 0:5 which &lt;- x &lt; 3 which ## [1] TRUE TRUE TRUE FALSE FALSE FALSE x[which] ## [1] 0 1 2 Here, we will be using the index method. In your script, store the first six numbers of the Fibonacci sequence in a vector, and run this. fib &lt;- c(1, 1, 2, 3, 5, 8) Now, in order to access the third element, we use square brackets: fib[3] ## [1] 2 In order to extract a larger subset we specify a vector of indices, or we can use the colon shortcut: fib[c(3,4,5)] ## [1] 2 3 5 fib[3:5] ## [1] 2 3 5 To generalise, if x is a vector, and i is a positive integer, then x[i], will be the \\(i\\)th element of x. And if y is a vector of positive integers, then x[y] gives the corresponding elements of x as a vector (note that some programming languages like python are zero based which means the first element of a vector is referred to as x[0]). 2.2.1 Writing a for loop We can use a for loop to calculate, say, the first 100 numbers of the Fibonacci sequence. In order to do this, we first need to understand two things; that this particular sequence has a recursive property, and loops are one way in which we can implement such properties. What do we mean by recursive? Well, instead of writing out the Fibonacci as just a sequence of numbers, we could instead use algebra: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, … \\(r_1\\), \\(r_2\\), \\(r_3\\), \\(r_4\\), \\(r_5\\), \\(r_6\\), \\(r_7\\), \\(r_8\\), \\(r_9\\), \\(r_{10}\\), \\(r_{11}\\), \\(r_{12}\\), … We already stated that, after the first two numbers, each subsequent number is the sum of the previous two. Written mathematically this looks like: \\[ r_{i} = r_{i-1} + r_{i-2} \\quad \\text{for}\\quad i &gt; 2. \\] So, the idea is to write code that lets i go from 3 up to 100, and as it does so, we calculate the \\(i\\)th value of the Fibonacci sequence, storing the results in a vector: # parameters n &lt;- 100 r &lt;- c(1, 1) # first two values # Fibonacci algorithm for (i in 3:n) { r[i] &lt;- r[i-1] + r[i-2] } Before you run the script, let us examine what is happening line-by-line. The two lines under the heading # parameters set-up the workspace; n contains the how many numbers we want to generate, and r initialises the sequence. The next section defines the Fibonacci sequence using a for loop. There are two parts to specifying a for loop. There is the index which is specified in the round brackets, and then there is the code to be repeated between the curly brackets: When specifying the index we use the phrase in rather than a equals symbol. If everything goes well, when you run the code your r vector should now contain 100 elements, you should check this. Also check some of the elements against the sequence stated above. For instance the 12th number should be 144. 2.2.2 Population growth The Fibonacci sequence might seem a little too abstract of a concept to justify why you should be interested in loops. So, let us take another example; wildlife population growth models. Wild animals typically go through three stages as part of their life-history; juvenile, subadult, and adult. With juveniles being defined as those being born within one time-period (typically a year). Subadults are no longer juveniles, but still remain non-reproductive (1 to 2 years old). And adults being the reproductive members of the population (2 or more years old). The reason why we separate juveniles and subadults, is that juveniles often have a much lower rate of survival than subadults. With adults having an even greater rate of survival year-on-year, than the other two. So, with population growth modelling, the idea is that given a starting size for each of these subgroups, the survival/death rate of each subgroup, and the reproductive rate of the adults, we should then be able to simulate what we expect to happen to the subpopulations over time. To keep track of the size of population subgroups, we let \\(J\\), \\(S\\) and \\(A\\) represent the size of the juveniles, subadults, and adults, respectively. Each of these numbers will change over time, such that \\(J_t\\) will represent the size of the juvenile group at time \\(t\\). To begin specifying models for how the subgroups change over time, let us start with the easiest, the transition from Juvenile to Subadult: \\[ S_{(t+1)} = p_0 J_t. \\] Where \\(p_0\\) is the proportion that survive to the subadult stage. Similarly, we define the proportion of subadults that survive as \\(p_1\\), and adults as \\(p_2\\). The number of adults at time \\(t+1\\) has two sources: \\[ A_{(t+1)} = p_1 S_t + p_2 A_t. \\] One way to read this is in the following way; the number of subadults that survived to adulthood plus the number of adults that have not died. Finally, to model the juveniles, we need to know what proportion of the adults produce offspring, let this be represented by \\(f\\). So, that the juveniles at time \\(t+1\\) are estimated by the equation: \\[ J_{(t+1)} = f A_t. \\] Collectively, these equations take the current sizes of each subgroup; \\(J_t, S_t, A_t\\), and then calculate their size one time-period from now. To implement this in R, we will use a for loop. We will store population sizes at each time in three vectors; J, S, and A, representing juveniles, subadults and adults, respectively. In order to give initial sizes to the populations, as well as numbers for the constants; \\(p_0\\), \\(p_1\\), \\(p_2\\), and \\(f\\), we need to look at a real-life case study. For this we look towards research on the Northern Spotted Owl which is a threatened species that lives and nests primarily in north west America, shown in Figure 2.1. Noon and Biles (1990) extracted estimates for these constants from various field studies; \\(p_0=0.11\\), \\(p_1=0.71\\), \\(p_2=0.94\\), \\(f=0.24\\). These numbers apply to female owls only, which is a common convention in population studies of species which have a constant ratio of males and females. Thus, our \\(J\\), \\(S\\), and \\(A\\) parameters refer only to the number of females at each time period. Figure 2.1: Northern spotted owl, a near threatened species whose wingspan is approximately one metre. To implement the population growth model of female northern spotted owls, we have several tasks in front of us: Start a new script. Add a series of comments at the top outlining what the script will do. Define the number of years we wish to project for. years &lt;- 20 Define the constants from the text above as objects. Create three vectors to store the population sizes of each of the age groups. At the start, the vectors should only contain the following initial population sizes: J &lt;- 1200 S &lt;- 800 A &lt;- 2000 Construct a for loop with an index to represent time: for (t in 1:years) { # population equations go here } Put the three population equations inside the for loop, using the index to pick out appropriate elements of the vectors. And store our new estimates. For example, adding the subadult equation would look like: for (t in 1:(years-1)) { # subadult equation S[t+1] &lt;- p0*J[t] # adult equation goes here # juvenile equation goes here } Complete for the other subgroup equations. Create a data frame called owl_pop with three variables; time, group, and size. Each row should contain the size at a single time point for one of the three subpopulations (juvenile, subadult, and adult), such that it looks like this: nrow(owl_pop) [1] 60 head(owl_pop) time group size 1 1 J 1200.0000 2 2 J 480.0000 3 3 J 587.5200 4 4 J 574.7616 5 5 J 549.2730 6 6 J 527.3291 To do this you will: Need to use the rep() function, check it’s help page for details. Make sure group is a factor with sensible labels. Ask us for help. Create a plot using ggplot() and owl_pop showing the changes in the three subpopulations over time. Things to consider: How do you plot lines instead of points? How do you tell ggplot to plot separate lines for each of the three groups in your data? Are you happy with your axis labels? Your plot should look something like this: 2.2.3 When not to loop We have used for loops to generate data based on a model. Each new value we calculated was based on at least one previously calculated value. This is, pretty much, the only circumstances in which for looping is definitely the answer. Often with data problems it is very tempting to create a solution using a for loop, but what you might find, especially in R, is that your solution takes a long time to run. When this happens there is frequently an alternative solution which involves using vectors instead. For a trivial example, say, we have an incredibly long vector which want to square: # for-loop method square_x_loop &lt;- function() { x &lt;- 1:10^7 # 10 million for (i in 1:length(x)) { x[i] &lt;- x[i]^2 } return(x) } # vector method square_x_vector &lt;- function() { x &lt;- 1:10^7 # 10 million x &lt;- x^2 return(x) } # how much time does each method take to execute? system.time(square_x_loop()) ## user system elapsed ## 1.025 0.048 1.076 system.time(square_x_vector()) ## user system elapsed ## 0.045 0.048 0.094 The for loop took longer than the vector approach. This is because of the way R is optimised for thinking with vectors, rather than thinking about processing individual values. With other more low-level programming languages you might not see any difference in terms of speed between these two methods, in fact it might even be the other way around. However, you should only consider a different approach when your loop seems to be taking a long time, and in these cases there will usually be a better way to code the solution. In all other cases where looping seems to solve your problem in a timely manner this is good enough. 2.2.4 Summary Elements of vectors can be picked out of the vector using one or more indexes: x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) x[3] ## [1] &quot;c&quot; x[2:4] ## [1] &quot;b&quot; &quot;c&quot; &quot;d&quot; x[c(1, 5)] ## [1] &quot;a&quot; &quot;e&quot; A for loop repeatedly executes a block of code. The number of repetitions is based on the size of the sequence passed to the index. The index itself is an object which can be used within the block of code. Example: for (i in c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) { # code to repeatedly execute print(i) } Recursive equations, or problems that involve calculating values based on previous values in the sequence are solved most excellently with for loops. If you write a loop and it executes very slowly, cancel it, and see if there is a better approach. There usually is, and it will most likely involve vectors. 2.3 Conditional statements Like all other programming languages, R can execute a block of code conditional on whether or not a statement is true. This is done by using the if command, here is one in action: for (animal in c(&quot;human&quot;, &quot;cat&quot;, &quot;turtle&quot;, &quot;zebra&quot;)) { # decide on food given to type of animal if (animal == &quot;cat&quot;) { # when TRUE run this food &lt;- &quot;fish&quot; } else { # when FALSE run this food &lt;- &quot;pizza&quot; } # compose message msg &lt;- paste(&quot;Give a&quot;, animal, &quot;some&quot;, food, &quot;for lunch.&quot;) print(msg) } ## [1] &quot;Give a human some pizza for lunch.&quot; ## [1] &quot;Give a cat some fish for lunch.&quot; ## [1] &quot;Give a turtle some pizza for lunch.&quot; ## [1] &quot;Give a zebra some pizza for lunch.&quot; which generates a message for each animal. The structure of an if statement is very similar to that of a for loop. The code they execute (or do not execute) lies between curly brackets. Whether or not an if statement executes a particular block of code depends on the whether or not the conditional statement is true or false. If we wanted to also feed fish to turtles, and perhaps give something more appropriate to zebras such as grass, then modify the above code so it reads: for (animal in c(&quot;human&quot;, &quot;cat&quot;, &quot;turtle&quot;, &quot;zebra&quot;)) { # decide on food given to type of animal if (animal == &quot;cat&quot; | animal == &quot;turtle&quot;) { food &lt;- &quot;fish&quot; } else if (animal == &quot;human&quot;) { food &lt;- &quot;pizza&quot; } else if (animal == &quot;zebra&quot;) { food &lt;- &quot;grass&quot; } else { food &lt;- &quot;soup&quot; } # compose message msg &lt;- paste(&quot;Give a&quot;, animal, &quot;some&quot;, food, &quot;for lunch.&quot;) print(msg) } ## [1] &quot;Give a human some pizza for lunch.&quot; ## [1] &quot;Give a cat some fish for lunch.&quot; ## [1] &quot;Give a turtle some fish for lunch.&quot; ## [1] &quot;Give a zebra some grass for lunch.&quot; See how the printed messages have changed. An example in statistics when we would use an if statement is when we are creating a basic probability event simulator, such as rolling dice, or dealing cards. To simulate rolling a six-sided die we would write a function which would: Draw a random number between 0 and 1. We use runif() for this. Use if statements to workout the dice face based on the random number. Return the dice roll result back to the user. In your script, type and run: roll_d6 &lt;- function() { # draw random number between 0 and 1 p &lt;- runif(1) # allocate dice face if (p &lt; 1/6) { face &lt;- 1 } else if (p &lt; 2/6) { face &lt;- 2 } else if (p &lt; 3/6) { face &lt;- 3 } else if (p &lt; 4/6) { face &lt;- 4 } else if (p &lt; 5/6) { face &lt;- 5 } else { face &lt;- 6 } return(face) } roll_d6() ## [1] 5 The function currently has no arguments, this is because everything about rolling a single dice is fixed. A feature that we could add is to allow multiple dice rolls with one call to roll_d6(), though we should assume by default that the user just wants a single roll. Modify roll_d6() to allow for this by looping over the existing code within the function \\(n\\) times and storing the results: roll_d6 &lt;- function(n=1) { # empty vector of results results &lt;- c() # allocate dice face for each probability for (i in 1:n) { p &lt;- runif(1) if (p &lt; 1/6) { face &lt;- 1 } else if (p &lt; 2/6) { face &lt;- 2 } else if (p &lt; 3/6) { face &lt;- 3 } else if (p &lt; 4/6) { face &lt;- 4 } else if (p &lt; 5/6) { face &lt;- 5 } else { face &lt;- 6 } # store result results[i] &lt;- face } return(results) } Here is an example of simulating 20 dice rolls. Play with this function; simulate a number of dice rolls and plot a graph of the results. Alter the number of dice rolls simulated, and see what it does to the graph. Note that the dice results are perfectly random, and any variation you see between the frequency of certain numbers is purely due to chance. 2.3.1 Summary You can execute a single line of code or block of code based on a certain logical comparison being true. This is done using an if statement which can take the following form:10 if ( condition1 ) { # code block 1 } else if ( condition2 ) { # code block 2 } else { # code block 3 } 2.4 Probability and Simulation This section utilises the probability concepts you have been introduced to in other modules, and assumes that you are familiar with the concept of a random variable; mainly that it is a quantity or measure that is likely to vary if the process being observed was repeated. And that we can use probability distributions to describe variables. Think about how you would answer the following questions if asked: What is a random event? What is a random number? What are some common probability distributions? And what types of data or processes might be described by them? In this section we will cover several topics: Why we use simulation. How to draw random samples from a set. How to draw values from a probability distribution. How simulation can show the law of large numbers (LLN) in action. Firstly, a random event is simply an event for which we do not know, for certain, what the outcome will be, though we may know the probability of each possible outcome. Examples are; flipping a coin, playing rock paper scissors, rolling dice, number of offspring, gender of the offspring. 2.4.1 Random sampling Randomly sampling from a discrete set of values in R is easy! To do this we use the sample() function, which resamples values from a vector, with or without replacement. It has a number of different uses. In the list below n is an integer, x is a vector, p is a vector probabilities with the same length of x: sample(n) creates a vector of integers from 1 to n, in a random order sample(x) randomly permute x sample(x, replace=TRUE) a bootstrap sample sample(x, n) sample n times from x without replacement sample(x, n, replace=TRUE) sample \\(n\\) times from x with replacement sample(x, n, replace=TRUE, prob=p) same as above, but sample each element of x with probability p The last three of these provides us with different ways of sampling from a finite discrete set of values. 2.4.2 Rock, Paper Scissors To begin implementing Rock, Paper, Scissors in R, we need to specify the finite set of choices. Then sample once from it for each player: # set of possible choices x &lt;- c(&quot;Rock&quot;, &quot;Paper&quot;, &quot;Scissors&quot;) # players, make your choice p1 &lt;- sample(x, 1) p2 &lt;- sample(x, 1) Check the values of p1 and p2 to see who won. If you want to do best of 3 do: # players, make your choice (three times) p1 &lt;- sample(x, 3) p2 &lt;- sample(x, 3) # best of 3 rbind(p1, p2) 2.4.3 Simulating the law of large numbers The law of large numbers is a theorem from mathematics which states the following; if a random process could be repeated many, many, many times, the long-run proportion (or fraction) of times that a particular outcome happens stabilises. This means that for a small number of repetitions, we can expect to see a large amount of variability regarding the proportion that hit. Whereas, for a large number of repetitions, the proportion of hits would be more stable (less varied). For example, when looking at science GCSE results from schools, we would expect to see that for the small schools (often in a quaint village), the proportion of the children obtaining a grade C or above would vary wildly from small school to small school. However, looking at the same proportion but for big city schools, we would expect to see less variation in the proportion from big school to big school. Let us put this in to action.11 Start a script or rmarkdown file, with a title-section including information that we are simulating the law of large numbers. With the particular application being that we are looking at how the variability in the proportion of children obtaining a science GCSE grade C or above within a school is dependent on the size of the school. To start, we are going to use the rbinom() function to simulate one schools worth of GCSE results. This function is one of the standard functions that comes with R (no need for packages) and allows us to randomly draw from a binomial distribution. Looking at the help page to understand the arguments of rbinom(), set the size of the trial to be \\(30\\) (this is the size of one year group in our small, quaint school). Set the probability of success to \\(0.59\\), this is the probability of a grade C or above. And set the number of observations to be one, as we’re only interested in looking at one small school right now. When you send this command to the console, it should return a count. To express this as a proportion, update your command to divide by the size of the school. Now, we are interested in the results of \\(100\\) schools of this size, increase the number of observations accordingly. Create a data frame with one column containing these results, and a second column containing the size of the schools. n_obs &lt;- 100 size &lt;- 30 pr &lt;- 0.59 tiny_schools &lt;- data.frame( sci_prop = rbinom(n_obs, size, pr), size = size ) Repeat the above process, but create a new data frame for each of the following sized schools; small (size of \\(100\\)), medium schools (size of \\(200\\)), large (size of \\(500\\)), and mega schools (size of \\(1000\\)). Combine the rows of each of these data frames into one data frame by using rbind() as follows: schools &lt;- rbind( tiny_schools, small_schools, medium_schools, large_schools, mega_schools ) Figure 2.2: Variation in proportion due to size of the year group. Demonstrating the law of large numbers. Create a ggplot comparing school size to the proportion of those of who got grades C and above. Check if it looks similar to Figure 2.2. How do you interpret this plot? Well, for a start, we know from our code that all children across all schools had the same probability of getting a grade C or above, so no school is inherently doing anything different in preparing it’s children for the GCSE exams. In fact, the only thing we allowed to vary was the size of the school. So the variation we are seeing in regards to the proportion of students who obtained a grade C or above within each school is related to the size of the year group at that school. And we see that the big year groups, tend to be closer to the true proportion than the smaller groups. This is the law of large numbers. If you did not know about the law of large numbers, and you were to simply look at the top 25 schools, then you would likely conclude that small schools are better than big schools, as the top ranks are dominated by the smaller-sized schools. We can see the top 25 schools with: schools &lt;- arrange(schools, desc(sci_prop)) schools[1:25, ] 2.4.4 Probability distributions R provides built-in functions that allow us to compute the density, cumulative probability, quantiles and random numbers from many standard probability distributions. Here we will look at the normal distribution and the binomial distribution, but other distributions follow exactly the same pattern. Table 2.1 contains a list of random generator functions for some of the standard probability distributions. The first parameter of these functions is always n which is the number of random values that you want to generate. Table 2.1: Random generator functions for a selection of the standard probability distributions. Probability dist. Function Beta rbeta(n, shape1, shape2) Binomial rbinom(n, size, prob) Chi-squared rchisq(n, df) Exponential rexp(n, rate) Gamma rgamma(n, shape, rate) Normal rnorm(n, mean, sd) Poisson rpois(n, lambda) Student-t rt(n, df) Uniform runif(n, min, max) To begin experimenting with these functions, look at the following two webpages: http://nemeth.shinyapps.io/rnorm http://nemeth.shinyapps.io/rbinom On each of these pages you can adjust the parameters of the random functions and see the results instantly. Note these webpages are Shiny Apps which are simple to create in RStudio. We will meet these again later in the course. To directly simulate random values in R: x &lt;- rnorm(n=10, mean=0, sd=1) print(x) ## [1] 0.01593416 0.17485484 -0.61743859 1.68082666 -0.86537503 0.88374391 ## [7] 1.16457391 0.05594423 -1.17949905 -1.63178444 qplot(x) # quick but not recommended for complex plots - use ggplot() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If you rerun these two commands repeatedly you will notice that the \\(10\\) values you draw each time changes. You may have noticed that all of the random functions start with r and follow with an abbreviation of the distribution name. Replacing r with d, p, q will give you the names of the functions that allow you to calculate density values, cumulative probabilities, and quantiles, respectively. 2.4.5 Repeating your simulation The numbers produced by rnorm(), rbinom(), and the other r... functions are actually pseudo-random numbers. They are random enough rather than genuinely random. The state of the random generator is controlled by the object .Random.seed which contains a list of integers: # next 3 values to be used by the random generator .Random.seed[1:3] ## [1] 10403 619 -470778465 Whenever sample() or an r function is run, the current state of .Random.seed is used. In order to re-run a simulation, we need to control the starting point of .Random.seed this can be done via set.seed() like so: set.seed(4578) rnorm(4) ## [1] -0.3906706 -1.8656330 -0.2399937 -0.8530353 rnorm(4) ## [1] -1.1927956 0.3692942 -1.4857336 -0.3040490 set.seed(4578) rnorm(2) ## [1] -0.3906706 -1.8656330 rnorm(2) ## [1] -0.2399937 -0.8530353 You would typically set your seed when you are performing simulations and wish to make your work reproducible. This allows others to rerun your code on their computers and obtain the same results. Without setting our seeds to be the same, even if we run the same code, the numbers we would generate would be different. 2.4.6 Densities The density function is probably the least used in practice out of the four function types. But if, for instance, you wanted to draw a probability distribution, such as the normal distribution then it could be quickly done like this: For discrete distributions, where variables can only take distinct values, it is more appropriate to draw a bar chart rather than a continuous line. Here is a binomial distribution with \\(size=10\\) and \\(p=0.33\\): 2.4.7 Probabilities The pnorm() and pbinom() functions can be used to calculate probabilities. For example, to calculate the probability of observing \\(X\\) being less than 2 when \\(X\\) is said to follow a standard normal distribution: \\[ \\text{Pr}(X \\leq 2) ~~~\\text{where}~~~ X \\sim \\text{Normal}(0, 1), \\] we would use pnorm() such that pnorm(2, mean=0, sd=1) ## [1] 0.9772499 Similarly, to calculate the probability of flipping a coin ten times and observing at least 3 heads: 1 - pbinom(2, size=10, prob=0.5) ## [1] 0.9453125 The p functions compute the probability for a given number or vector of numbers. These functions also goes by the ominous title of the “Cumulative Distribution Function”. This is because the word cumulative refers to the fact that we always calculate the probability of less than or equal to our number occurring. When you carry out statistical tests such as a t-test or chi-sqaured test, once you have calculated your summary statistic, you can then use pt() or pchisq(), respectively, to calculate your \\(p\\)-value. As performing these tests is a common practice in statistics we obviously have functions that will do all of this for us (namely t.test() and chisq.test(), but inside these functions they are using these p...() functions. Also note that when calculating \\(p\\)-values when you want the upper tail (which is most of the time) it is better practice to code: pbinom(2, size=10, prob=0.5, lower.tail=FALSE) ## [1] 0.9453125 This is because the lower.tail option is better at handling cases at the extremes of the distributions. 2.4.8 Quantiles The quantile functions are the opposite to the probability functions. To use qnorm() we have to specify a probability, and it will return the corresponding value of \\(X\\): qnorm(0.975, mean=0, sd=1) ## [1] 1.959964 To rewrite this mathematically: \\[ \\text{Pr}(X \\leq 1.96) = 0.975 ~~~\\text{where}~~~ X \\sim \\text{Normal}(0, 1). \\] Similarly, if we let \\(Z \\sim \\text{Normal}(1, 9)\\). To find the value \\(q\\) (quantile) such that Pr\\((Z \\leq q)\\) is \\(0.05\\) type: qnorm(0.05, mean=1, sd=sqrt(9)) ## [1] -3.934561 These functions also take a vector of quantiles. For example to find Pr\\((Z \\leq -3.9)\\), Pr\\((Z \\leq 0)\\), and Pr\\((Z \\leq 7)\\) type: q &lt;- c(-3.9, 0, 7) pnorm(q, mean=1, sd=sqrt(9)) ## [1] 0.05119945 0.36944134 0.97724987 2.4.9 Summary Simulation is the process of using a computer to generate random numbers to create data. We can simulate data from discrete and continuous probability distributions. R provides functions for generating random numbers from the common probability distributions. All functions return a vector of n realisations. Table 2.1 contains a list of some these functions. Note that R does not necessarily use the same parameterisation for these probability distributions as you will see in your courses, or in popular stats text books. Therefore always check the help page of the function to confirm its particular parameterisation. Example: # simulate 10 unbiased-coin flips rbinom(10, 1, 0.5) ## [1] 0 0 1 0 0 0 0 0 0 1 # simulate total number of heads for 2 trials # of 100 unbiased-coin flips rbinom(2, 100, 0.5) ## [1] 46 46 # draw from 5 values from a standard normal distribution # Standard normal distribution N(0,1^2) # mean = 0 and standard deviation = 1 rnorm(5) ## [1] -0.1047152 0.9304077 1.4534803 -0.9740542 -0.4645038 To ensure your simulations are reproducible use set.seed() to specify a starting point for the random number generator. The first letter of all the functions in Table 2.1 is r which stands for “random”. For each probability distribution there are also density (d), quantile (q), and cumulative probability functions (p). For example, say, \\(X \\sim \\text{Normal}(6, 5)\\), then to calculate the probability that we observe \\(x\\) being less than or equal to \\(3\\), \\(\\text{Pr}(X \\leq 3)\\): pnorm(3, mean=6, sd=sqrt(5)) ## [1] 0.08985625 Similarly, if we wanted to calculate what quantile, \\(q\\), of \\(X\\) would correspond to the following: \\[ \\text{Pr}(X \\leq q) = 0.975 \\] then we would use the following command: qnorm(0.975, mean=6, sd=sqrt(5)) ## [1] 10.38261 2.4.10 Exercises Generate some random data and calculate quantile summary statistics: Set your seed to be 59810. Simulate 100 values from a random normal distribution with mean 25 and standard deviation 8. Store these in a vector. Use quantile() to calculate the 25% and 75% quantiles for your vectors. d.Compare these against the 0.25 and 0.75 quantiles calculated using qnorm(). Note that differences in quantile values are due to your vector containing randomly simulated values. Quantiles from qnorm() should be 19.60408 and 30.39592. Quantiles from your vector should be 19.20686 and 31.50523. Play with random generator functions for other probability distributions: http://nemeth.shinyapps.io/rbeta http://nemeth.shinyapps.io/rchisq http://nemeth.shinyapps.io/rgamma http://nemeth.shinyapps.io/rexp 2.5 Combining Loops and Simulation The population model discussed in the Section 2.2 is considered to be a deterministic model as there is no random variation, it is simply a set of equations we evaluate at each time point. What we can do is give this model a random component. We do this by changing the way we think about individuals in the population. For example, instead of saying 0.24 of females produce an offspring, we know say each female has a probability of 0.24 of producing an offspring. So, the deterministic model looked like this: # time period years &lt;- 20 # constants p0 &lt;- 0.11 p1 &lt;- 0.71 p2 &lt;- 0.94 f &lt;- 0.24 # initial population sizes J &lt;- 1200 S &lt;- 800 A &lt;- 2000 # population growth model for (t in 1:(years-1)) { S[t+1] &lt;- p0*J[t] A[t+1] &lt;- p1*S[t] + p2*A[t] J[t+1] &lt;- f*A[t] } # store the results owl_pop &lt;- data.frame( time = rep(1:years, 3), group = rep(c(&quot;J&quot;, &quot;S&quot;, &quot;A&quot;), each=years), size = c(J, S, A) ) # plot the results ggplot(owl_pop) + geom_line(aes(x=time, y=size, colour=group, linetype=group)) + theme(legend.position=&quot;top&quot;) Essentially, we want to replace the deterministic equations in the for loop with calls to rbinom(). For example, for the subadult population, at each iteration we want to simulate one binomial sample with a size of J[t] with probability of success equal to p0. Make these changes, and rerun the whole script. If you repeatedly rerun the script you will generate slight variations on the plot, this is due to the random component. Finally, set your seed to 87856, rerun your script, and check if the plot you produce matches Figure 2.3. Figure 2.3: Stochastic population growth model over the next 20 years for the Northern spotted owls. Random seed was set to 87856. The condition statements in if brackets are logical comparisons that return a single TRUE or FALSE. The else if and else portions are optional, and will only be considered if previous conditional statements have all returned FALSE.↩ Without knowing anything about each child, we assume that they all have the same probability of obtaining a grade C or above. In reality, even knowing a child’s gender means that we can start to adjust this probability. With females being more likely to obtain a grade C or above than males. http://gu.com/p/4vqfv.↩ "],
["time.html", "3 Time manipulation 3.1 From last week 3.2 Dates and timestamps in R 3.3 Summarising data over time 3.4 A timestamp split over several columns 3.5 Time zones", " 3 Time manipulation 3.1 From last week Compare your submission for last week’s coursework with the solution that is on Moodle. Look for differences. Make sure to intergrate any improvements in style into this week’s code. Revise the Summary sections in previous chapters. As usual, to save the code you write for this week, create new R or Rmd scripts in your R programming folder. Write a few introductory comments at the top stating that this script covers the basics handling dates and times in R, along with how to aggregate and summarise data. Add your favourite packages to the start of your script: library(ggplot2) library(dplyr) # alternatively library(tidyverse) 3.2 Dates and timestamps in R Figure 3.1: Air traffic control direct aircraft on the ground and through controlled air space. They prevent collisions and organise the flow of air traffic. Figure 3.2: Left: The GPS tracker a runner users when they go out training continuously logs their location over time; Right: A bank statement is an event-log of transactions. Date-time data is typically generated by an automated process or system. See Figures 3.1–3.2 for examples. Data collected in this fashion can be thought of as an event log, with one column containing the date and time of the event, and the remaining columns capturing whatever measures thought necessary. You may not know this yet, but when working with event-log data, the date-time component can be very frustrating to work with. To begin with, a timestamp may take on a variety of forms: 2014/08/12 19:47 2014/12/08 19:47:01 12/08/2014 19:47:01 19:47:01 12/08/2014 7.47PM 12-AUG-14 As you can see timestamps can come in many different formats, which makes recognising and parsing them a challenge. Will R recognise the format that we have? If it does, we still face problems specific to timestamps. How can we easily extract components of the timestamp, such as the year, month, or number of seconds? How can we switch between time zones, or compare times from places that use daylight savings time (DST) with times from places that do not? Handling timestamps becomes even more complicated when we try to do arithmetic with them. Conventions such as leap years and DST make it unclear what we mean by “one day from now” or “exactly two years away”. Even leap seconds can disrupt a seemingly simple calculation. This complexity affects other tasks too, such as constructing sensible tick marks for plotting date-time data. While base R handles some of these problems, the syntax it uses can be confusing and difficult to remember. Moreover, the correct R code often changes depending on the type of date-time object being used. This is exactly why the lubridate package was created; in order to address these problems and makes it easier to work with date-time data in R. It also provides tools for manipulating timestamps in novel but useful ways. Specifically, lubridate helps us to: Identify and parse date-time data; Extract and modify components of a date-time, such as years, months, days, hours, minutes, and seconds; Perform accurate calculations with date-times and time-spans; Handle time zones and daylight savings time. Before we continue install the lubridate package: install.packages(&quot;lubridate&quot;) Now load it. library(lubridate) 3.2.1 Parsing timestamps We can parse dates and timestamps in R using the ymd() series of functions provided by lubridate, these are shown in Table 3.1. These functions parse character strings into date-time objects. The letters y, m, and d in the function names correspond to the year, month, and day elements of a timestamp. To parse a timestamp, choose the function name that matches the order of elements in the timestamp. For example, in the following date the month element comes first, followed by the day and then the year. So we would use the mdy() function: mdy(&quot;12/01/2010&quot;) ## [1] &quot;2010-12-01&quot; The same function can also be used to parse \"Dec 1st, 2010\": mdy(&quot;Dec 1st, 2010&quot;) ## [1] &quot;2010-12-01&quot; The ymd() series of functions can also parse vectors of dates: dmy(c(&quot;31.12.2010&quot;, &quot;01.01.2011&quot;)) ## [1] &quot;2010-12-31&quot; &quot;2011-01-01&quot; These functions automatically recognise the separators commonly used to record dates. These include: -, /, ., and “no separator”. When a ymd() function is applied to a vector, it assumes that all of the elements within the vector have the same order and the same separators. Table 3.1: Parse-function names are based on the order that the year, month, and day appear within the dates to be parsed. Other variations exist. Order of elements in timestamp Parse function year, month, day ymd() year, day, month ydm() month, day, year mdy() day, month, year dmy() hour, minute hm() hour, minute, second hms() year, month, day, hour, minute, second ymd_hms() hour, minute, second, day, month, year hms_dmy() 3.2.2 Manipulating timestamps Most timestamps include a year value, a month value, a day value and so on. Together these elements specify the exact moment that an event occurred or when an observation was made. We can easily extract each element of a timestamp with the accessor functions listed in Table 3.2. For example, if we save the current system time:12 stamp &lt;- now() class(stamp) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; We can then extract each of its elements: year(stamp) # for illustration only; your output might vary ## [1] 2020 minute(stamp) # your output might vary ## [1] 51 For the month and weekday elements (mday and wday), we can also specify whether we want to extract the numerical value of the element, an abbreviation of the name of the month or weekday, or the full name. For example: Table 3.2: Each date-time element can be extracted with its own accessor function. Component Accessor Year year() Month month() Week week() Day of year yday() Day of month mday() Day of week wday() Hour hour() Minute minute() Second second() Time zone tz() month(stamp) # your output might vary ## [1] 10 month(stamp, label=TRUE) # your output might vary ## [1] Oct ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec month(stamp, label=TRUE, abbr=FALSE) # your output might vary ## [1] October ## 12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; ... &lt; December wday(stamp, label=TRUE, abbr=FALSE) # your output might vary ## [1] Thursday ## 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday 3.2.3 Arithmetic with timestamps Arithmetic with timestamps is more complicated than arithmetic with numbers, but it can be done accurately and easily with lubridate. What complicates arithmetic with timestamps? Clock times are periodically re-calibrated to reflect astronomical conditions, such as the hour of daylight or the Earth’s tilt on its axis relative to the sun. We know these re-calibrations as daylight savings time, leap years, and leap seconds. Consider how one of these conventions might complicate a simple addition task. If today were January 1st, 2010 and we wished to know what day it would be one year from now, we could simply add 1 to the years element of our date: mdy(&quot;January 1st, 2010&quot;) + years(1) ## [1] &quot;2011-01-01&quot; Alternatively, we could add 365 to the days element of our date because a year is equivalent to 365 days: mdy(&quot;January 1st, 2010&quot;) + days(365) ## [1] &quot;2011-01-01&quot; However, problems arise if we try the same for January 1st, 2012. 2012 is a leap year, which means it has an extra day. Our two approaches above now give us different answers because the length of a year has changed: mdy(&quot;January 1st, 2012&quot;) + years(1) ## [1] &quot;2013-01-01&quot; mdy(&quot;January 1st, 2012&quot;) + days(365) ## [1] &quot;2012-12-31&quot; At different moments in time, the lengths of months, weeks, days, hours, and even minutes will also vary. We can consider these to be relative units of time; their length is relative to when they occur. In contrast, seconds always have a consistent length. Hence, seconds are exact units of time. Researchers may be interested in exact lengths, relative lengths, or both. For example, the speed of a physical object is most precisely measured in exact lengths. The opening bell of the stock market is more easily modelled with relative lengths. In general, we can change timestamps by adding or subtracting units of time from them. To do this use the helper functions; years(), months(), weeks(), days(), hours(), minutes(), and seconds(). Where the first and only argument is the amount of that unit of time: stamp - hours(48) - minutes(30) ## [1] &quot;2020-10-13 23:21:44 BST&quot; 3.2.4 Intervals and durations Often we do not want to necessarily change a timestamp, but actually calculate the difference between two timestamps. For example, between the start and end of an event to calculate the duration, or to count down to a particular event. We first define an interval using between our two time-points: halloween &lt;- ymd(&quot;2014-10-31&quot;) christmas &lt;- ymd(&quot;2014-12-25&quot;) interval &lt;- interval(halloween, christmas) interval ## [1] 2014-10-31 UTC--2014-12-25 UTC After which we can choose to express this interval as a duration in terms of a specific time-unit (e.g. weeks, days, or seconds): interval / dweeks(1) ## [1] 7.857143 interval / ddays(1) ## [1] 55 interval / dseconds(1) ## [1] 4752000 In order to express an interval as a duration we divide by similar functions to those used in the arithmetic section but they are all prefixed with d. 3.2.5 Unix time To overcome the issues with relative time, some systems store timestamps simply as the number of seconds since 00:00:00, Thursday, 1st January 1970 (UTC). When time is stored like this it is referred to as Unix-time or time-since-Epoch. Figure 3.3: Unix time passed 1,000,000,000 seconds on 2001-09-09 01:46:40 UTC. It was celebrated in Copenhagen, Denmark at a party (03:46 local time). To convert a date-time object to Unix time, simply change the object type to numeric: event &lt;- ymd_hms(&quot;2001-09-09 01:46:40&quot;, tz=&quot;UTC&quot;) as.numeric(event) ## [1] 1e+09 To convert from Unix-time back to a timestamp, take the Unix-time value (which is just a number of seconds) and add it to the origin: origin &lt;- ymd_hms(&quot;1970-01-01 00:00:00&quot;, tz=&quot;UTC&quot;) origin + seconds(10^9) # 1 billion seconds ## [1] &quot;2001-09-09 01:46:40 UTC&quot; 3.2.6 Rounding time Like all measurements, timestamps have a precision; they are often measured to the nearest day, minute, or second. This means that timestamps can be rounded. To perform this rounding we use: round_date(), floor_date(), and ceiling_date(). The first argument of each function is a timestamp or vector of timestamps to be rounded. The second argument is the unit to round to. For example, we could round 11:33, 20th April 2010 to the nearest day: april20 &lt;- ymd_hms(&quot;2010-04-20 11:33:29&quot;) round_date(april20, &quot;day&quot;) ## [1] &quot;2010-04-20 UTC&quot; Note that rounding a timestamp to a particular day sets the hours, minutes and seconds components of the timestamp to \\(00\\). If the timestamp is in the afternoon then it will be rounded up to the next day: april20 &lt;- ymd_hms(&quot;2010-04-20 14:15:02&quot;) round_date(april20, &quot;day&quot;) ## [1] &quot;2010-04-21 UTC&quot; Similarly, rounding to the nearest month, sets the day to \\(01\\) regardless of which month it is rounded to: round_date(april20, &quot;month&quot;) ## [1] &quot;2010-05-01 UTC&quot; We can use ceiling_date() to find the last day of a month. Do this by ceiling a timestamp to the next month and then subtract one day: ceiling_date(april20, &quot;month&quot;) - days(1) ## [1] &quot;2010-04-30 UTC&quot; 3.2.7 A Real Example: Sea Ice Extent We are now going to put some of this lubridate knowledge into practice by exploring the data collected as part of the routine monitoring of the amount of sea ice at the Arctic.13 Certain satelites that pass over the Arctic have equipment that allows them to measure the presence of sea ice and its density. What we are interested in is the extent of the sea ice i.e. the surface area when viewed from above. Figure 3.4 shows the extent of the sea ice in October, 2013. Figure 3.4: Extent of the Arctic sea ice in October, 2013. Outline shows the median ice edge. Download the NH_sea_ice_extent_2014-10-10.csv data set from the Moodle page into your working directory. Within R, read this into an object called sea_ice (or similar). Use head() to look at the first few rows of sea_ice and parse the date column accordingly. The extent column is a measure of the top-down surface area of the sea ice in million square-kilometres. If you use the class() function on sea_ice$date you’ll see that it’s either of type character or factor (depending upon whether you set the stringsAsFactors to FALSE or not respectively). We need to change this so that R recognises that it’s a data. Run the command below and check the class again sea_ice$date &lt;- ymd(sea_ice$date) Plot the extent of the sea ice over time. ggplot(sea_ice) + geom_line(aes(x=date, y=extent)) What can be seen here is that while there is clear seasonal variation, there also appears to be a downward trend over time. To focus on the seasonal variation, we need to create a graphic that shows extent from January to December on the x-axis, with each year then having its own line. To do this we first need to create two extra variables based on date; one which contains the year component, and the other containing the day of the year (1-365): sea_ice$year &lt;- year(sea_ice$date) sea_ice$year_day &lt;- yday(sea_ice$date) head(sea_ice) ## date extent year year_day ## 1 1978-10-26 10.19591 1978 299 ## 2 1978-10-28 10.34363 1978 301 ## 3 1978-10-30 10.46621 1978 303 ## 4 1978-11-01 10.65538 1978 305 ## 5 1978-11-03 10.76997 1978 307 ## 6 1978-11-05 10.96294 1978 309 Using these two new variables we can now create a seasonal plot. Note that in order to tell ggplot() to produce a separate line for each year we specify group=year as part of aes(). ggplot(sea_ice) + geom_line(aes(x=year_day, y=extent, group=year), alpha=0.5) To highlight which lines belong to which year modify the plot so we colour each line according to year: ggplot(sea_ice) + geom_line(aes(x=year_day, y=extent, group=year, colour=year), alpha=0.5) + theme(legend.position=&quot;top&quot;) To change the colours used for the gradient, and the labels shown on the colour bar for year, add scale_colour_gradient() to your ggplot() command: ggplot(sea_ice) + geom_line(aes(x=year_day, y=extent, group=year, colour=year), alpha=0.5) + scale_colour_gradient(low=&quot;red&quot;, high=&quot;blue&quot;) + theme(legend.position=&quot;top&quot;) Try other colours to see if you can find something that looks pretty. An alternative to picking colours yourself is to use the colour brewer:14 library(RColorBrewer) palette &lt;- brewer.pal(3, name=&quot;BrBG&quot;) ggplot(sea_ice) + geom_line(aes(x=year_day, y=extent, group=year, colour=year), alpha=0.5) + scale_colour_gradient(low=palette[1], high=palette[3]) + theme(legend.position=&quot;top&quot;) 3.3 Summarising data over time When wanting to look at year-on-year trends, we often want to look past any variation due to seasonality. There are three ways of doing this: Only look at the same time point at each year e.g. numbers for October every year. Create an average for a fixed time unit e.g. average per year. Calculate an average of a moving window e.g. average of the last 30 days of observations and move this window across the whole time range. We will consider each of these approaches using the R packages we have covered so far in the course. The first of these is the simplest to implement, we begin by keeping only October observations: sea_ice$month &lt;- month(sea_ice$date) sea_ice_oct &lt;- filter(sea_ice, month == 10) head(sea_ice_oct) ## date extent year year_day month ## 1 1978-10-26 10.19591 1978 299 10 ## 2 1978-10-28 10.34363 1978 301 10 ## 3 1978-10-30 10.46621 1978 303 10 ## 4 1979-10-01 7.36108 1979 274 10 ## 5 1979-10-03 7.48100 1979 276 10 ## 6 1979-10-05 7.73403 1979 278 10 However, we have more than one observation per month, and they are not always at the same day within the month. One solution to this is to use only the first observation for each October, but to do this we need to work out which one that is. To do this we use group_by() and summarise() from dplyr. 3.3.1 Group by and summarise In order to break-up our data frame into small subgroups so that we can perform the same calculation on each subgroup, we use group_by() from dplyr, whose first argument is the data frame of interest and all subsequent arguments are the variables to be grouped on. The following will create groups for each unique value in year: sea_ice_oct_grp &lt;- group_by(sea_ice_oct, year) is.grouped_df(sea_ice_oct_grp) ## [1] TRUE sea_ice_oct_grp is a grouped data frame. Use ungroup() to remove a grouping that is no longer needed. To summarise each group we send sea_ice_oct_grp to the summarise() function which will produce a new data frame containing the grouping variable (just year in this case) and any summary variables we decide to calculate: oct_summary &lt;- summarise(sea_ice_oct_grp, date = first(date), extent = first(extent), year_day = first(year_day) ) The code above summarises each group simply by taking the first row in each subgroup and stores the results in a data frame called oct_summary. This is, of course, a very crude summary. But it does give us one observation per October of every year. head(oct_summary) ## # A tibble: 6 x 4 ## year date extent year_day ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1978 1978-10-26 10.2 299 ## 2 1979 1979-10-01 7.36 274 ## 3 1980 1980-10-01 8.17 275 ## 4 1981 1981-10-02 7.94 275 ## 5 1982 1982-10-01 7.71 274 ## 6 1983 1983-10-02 8.09 275 We see that the extent of the sea ice for the first observation in October, 1978 as 10.3 million sq-km. Also, we can see that by asking for the first date, it has now been converted to Unix time. To convert it back to a timestamp see the section on Unix time. Using ggplot() and oct_summary, you should now try to produce a graph showing the extent of the sea ice in October for each year (shown here in Figure 3.5). Figure 3.5: Ice extent (million sq-km) in October each year. Only first observation in October each year was used. 3.3.2 Yearly averages To calculate the average of a variable in a data frame, we pass the variable to the mean() function: mean(sea_ice$extent) ## [1] 11.44243 This calculated the mean across all observations of extent. Repeat the process of grouping and summarising, but on year instead of month. And instead of capturing the first observation, calculate the mean of the extent observations within each group. Thus you will be able to visualise change in yearly-average of ice extent over time. The graph you produce should look like Figure 3.6. Figure 3.6: Average ice extent (million sq-km) for each year. Note the difference in scale on the y-axis between the October graph and this yearly graph. Look back at seasonal variation to understand the cause for the difference. 3.3.3 Moving average So far, to get past the seasonal variation, we have averaged over distinct subsets of data. An alternative to this is to average over a moving window of the data, this type of average is called a moving or rolling average. At each observation, we use it and the previous, say, 12 months of observations to calculate an average. We then move on to the next observation and then calculate its average based on its previous 12 months. This process runs until we reach the last row in our data, but only starts once we have at least 12 months of data: min(sea_ice$date) + months(12) ## [1] &quot;1979-10-26&quot; This would be the first time point at which our average is calculated. To calculate the moving average, we iterate over the rows in sea_ice using a for loop: for (i in 1:nrow(sea_ice)) { # ... moving average code ... } There are several steps we need to take in order to calculate the moving average. For each row we need to: Identify if we can start calculating the average yet. Identify the 12-month subset. Calculate and store the average for this subset. Note that once you execute the code below, it will take a while to finish, as looping is a slow process in R, and we have a lot of rows to loop over: # create our new variable, fill it with missing values sea_ice$extent_mavg &lt;- NA # set our window size (in days) window_size &lt;- 365 # when do we start our averaging? start_at &lt;- min(sea_ice$date) + days(window_size) for (i in 1:nrow(sea_ice)) { current_date &lt;- sea_ice$date[i] # are we at or beyond the start date yet? if (current_date &gt;= start_at) { # identify the previous 12-month subset from &lt;- current_date - days(window_size) sub_set &lt;- filter(sea_ice, (date &gt; from) &amp; (date &lt;= current_date)) # calc and store average sea_ice$extent_mavg[i] &lt;- mean(sub_set$extent) } } In a similar fashion to the previous group-averaging, now visualise the moving average results over time. What happens to the graph when you modify your window size to be 6 months (183 days)? What about a window size of 18 months (548 days)? 3.4 A timestamp split over several columns The original sea ice extent data did not actually contain a timestamp. The timestamp you parsed earlier was created by combining several columns in the original data. The original data is in orig_NH_seaice_extent.csv on Moodle. Download this to your working directory and load it into R: sea_ice_orig &lt;- read.csv(&quot;orig_NH_seaice_extent.csv&quot;) names(sea_ice_orig) ## [1] &quot;Year&quot; &quot;Month&quot; &quot;Day&quot; &quot;Extent&quot; &quot;Missing&quot; ## [6] &quot;Source.Data&quot; # remove junk columns sea_ice_orig &lt;- select(sea_ice_orig, -Missing, -Source.Data) head(sea_ice_orig) ## Year Month Day Extent ## 1 2014 1 1 12.97145 ## 2 2014 1 2 13.06702 ## 3 2014 1 3 13.13399 ## 4 2014 1 4 13.22008 ## 5 2014 1 5 13.12213 ## 6 2014 1 6 13.11912 We have three columns used to capture the time of the observation (Year, Monday, and Day). To turn this into a timestamp we need to join each row of year, month and day together. To do this we use the glue() function from the glue package. Install this package and then load it using library(). Check the help page for glue() and test how it works: library(glue) y &lt;- 2014 m &lt;- 09 d &lt;- 15 glue(&quot;{y}{m}{d}&quot;) ## 2014915 glue(&quot;{y}/{m}/{d}&quot;) ## 2014/9/15 We can also use vectors: animal &lt;- c(&quot;monkey&quot;, &quot;human&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;zebra&quot;) food &lt;- c(&quot;banana&quot;, &quot;pizza&quot;, &quot;fish&quot;, &quot;anything&quot;, &quot;grass&quot;) glue(&quot;{animal} would like {food}&quot;) ## monkey would like banana ## human would like pizza ## cat would like fish ## dog would like anything ## zebra would like grass Use glue() along with mutate() to create a date variable within the sea_ice_orig data frame, which should look something like this: head(sea_ice_orig) ## Year Month Day Extent Date ## 1 2014 1 1 12.97145 2014-01-01 ## 2 2014 1 2 13.06702 2014-01-02 ## 3 2014 1 3 13.13399 2014-01-03 ## 4 2014 1 4 13.22008 2014-01-04 ## 5 2014 1 5 13.12213 2014-01-05 ## 6 2014 1 6 13.11912 2014-01-06 An alternative to glue::glue() is the function stringr::str_c(), which, alongside other useful functions, is included in the stringr package. We will cover the package next time, along with more general ways to summarise a data frame. 3.5 Time zones Time zones give multiple names to the same instance of time. For example, # Australian Christmas lunch aus_christmas &lt;- ymd_hms(&quot;2010-12-25 13:00:00&quot;, tz=&quot;Australia/Melbourne&quot;) # In UK time with_tz(aus_christmas, tz=&quot;GMT&quot;) ## [1] &quot;2010-12-25 02:00:00 GMT&quot; Both of these describe the same instant. The first shows how the instant is labelled in Melbourne time (AEDT). While the second shows the same instant but labelled in Greenwich Mean Time (GMT). Time zones complicate date-time data but are useful for mapping clock time to local daylight conditions. When working with instants, it is standard to give the clock time as it appears in the Coordinated Universal time zone (UTC). This saves calculations but can be annoying if your computer insists on translating times to your current time zone. It may also be inconvenient to discuss clock times that occur in a place unrelated to the data. The lubridate package tries to ease the frustration caused by different time zones in data by two ways. First, we can change the time zone in which an instant is displayed by using the function with_tz(). This changes how the clock time is displayed, but not the specific instant of time that is referred to. For example: uk_christmas &lt;- ymd_hms(&quot;2010-12-25 13:00:00&quot;, tz=&quot;GMT&quot;) with_tz(uk_christmas, &quot;UTC&quot;) ## [1] &quot;2010-12-25 13:00:00 UTC&quot; with_tz(aus_christmas, &quot;UTC&quot;) ## [1] &quot;2010-12-25 02:00:00 UTC&quot; The force_tz() function does the opposite of with_tz(); it changes the actual instant of time saved in the object, while keeping the displayed clock time the same. The new time zone value is the indicator of this change. For example, the code below moves us to a new instant that occurs 11 hours later. force_tz(aus_christmas, &quot;UTC&quot;) ## [1] &quot;2010-12-25 13:00:00 UTC&quot; The with_tz() and force_tz() functions only work with time zones recognised by R. To see a long list of these, use OlsonNames(). head(OlsonNames()) ## [1] &quot;Africa/Abidjan&quot; &quot;Africa/Accra&quot; &quot;Africa/Addis_Ababa&quot; ## [4] &quot;Africa/Algiers&quot; &quot;Africa/Asmara&quot; &quot;Africa/Asmera&quot; Finally, note that the ymd_hms family of functions will, by default, parse all timestamps as being in the UTC timezone. Regardless of whether the timestamp contains a reference to the actual timezone. Here is an example of a timestamp in USA Eastern Standard Time (EST) being overwritten as UTC: ymd_hms(&quot;2010-12-25 13:00:00 EST&quot;) ## [1] &quot;2010-12-25 13:00:00 UTC&quot; To ensure your timestamp is parsed as being in the correct timezone you need to pass the timezone to the tz argument of the ymd_hms() function: ymd_hms(&quot;2010-12-25 13:00:00 EST&quot;, tz=&quot;EST&quot;) ## [1] &quot;2010-12-25 13:00:00 EST&quot; Note that if you the tz argument has to be a valid timezone otherwise with_tz() will not perform the proper conversion when converting it to a different timezone: try(tz_pickle &lt;- ymd_hms(&quot;2010-12-25 13:00:00&quot;, tz=&quot;PICKLE&quot;)) ## Error in C_force_tz(time, tz = tzone, roll) : ## CCTZ: Unrecognized output timezone: &quot;PICKLE&quot; try(with_tz(tz_pickle, &quot;UTC&quot;)) ## Error in is.data.frame(time) : object &#39;tz_pickle&#39; not found The timezone PICKLE is silently replaced to UTC, even though PICKLE is not an actual timezone. A more real example; while EST is a valid timezone in R, USA Central Standard Time (CST) is not, although it is an actual timezone. Instead of CST we have to use CST6CDT which represents both the CST and CDT timezones (both are GMT - 6 hours). any(OlsonNames() == &quot;EST&quot;) ## [1] TRUE any(OlsonNames() == &quot;CST&quot;) ## [1] FALSE any(OlsonNames() == &quot;CST6CDT&quot;) ## [1] TRUE Because of all these issues, a good strategy is: Check your timezones are valid by comparing against OlsonNames(), correcting any that are not. Parse using the appropriate function. Convert all timestamps to UTC. Note that this was the system time when this example was written. now() will return a different timestamp each time it is used.↩ This is collected by the National Snow and Ice Data Center (NSIDC) and is available at: http://nsidc.org/data/G02135.↩ Try setting name in brewer.pal() to any of the following: BrBG, PiYG, PRGn, PuOr, RdBu, RdGy, RdYlBu, RdYlGn, Spectral.↩ "],
["string.html", "4 String processing 4.1 Preamble 4.2 Introducing regular expressions 4.3 Working with strings and regular expressions 4.4 Exercises", " 4 String processing 4.1 Preamble Install the stringr package if it is not already available on your machine. Load your favourite packages at the start of a new script: library(ggplot2) library(dplyr) library(stringr) # Or more simply library(tidyverse) 4.2 Introducing regular expressions Regular expressions are special phrases that we construct to help find and match patterns in a large body of text or sets of strings. They exist in almost all programming languages and are a useful tool when interacting with computers in all sorts of ways: Finding files on a system with a particular pattern in their filename. Parsing a log file which contains thousands of lines, hunting down the reason for a server repeatedly crashing. Generating features and statistics from bodies of text e.g. how many times does the word “potato” appear in each of the works of Shakespeare. Another example; pulling out hashtags from tweets. Parse a web-page, storing all references to other websites. Automating the finding and replacing text. You will be starting with an expression as simple as this: \\d which is one way of saying “any character that matches a digit from 0 to 9”. And moving to something a bit more complicated, such as: (\\(\\d{5}\\)|\\d{5})[ -.]?\\d{5,6} which matches a 10 or 11 digit, UK telephone number, with or without parentheses around the area code, and with or without a hyphen, dot (period) or space to separate the numbers. This lab will give you an idea of the simpler ways to match patterns in text using: Literal text Digits Letters Characters of any kind We will start by using the website RegExr to help you build and understand regular expressions: http://www.regexr.com/ Open fake_phone_numbers.txt, a file in the data sets folder on moodle. Copy and paste its contents into the “Text” area on RegExr. 4.2.1 Matching literal text The most obvious feature of regular expressions is matching strings with one or more literal characters, called literal text or just literals. The way to match literal text is with normal, literal characters. This is similar to the way you might try to find a phrase in a word document or when submitting a keyword to a search engine. When you search for a string of text, character for character, you are searching with literal text. If you want to match the Lancaster area code, 01524, for example, which is a number sequence (string of digits), just type 01524 in the “Expression” box at the top of RegExr, and then all the matching area codes will be highlighted in the Text. If nothing is highlighted, check what you typed. 4.2.2 Matching digits In the Expression box, replace the literal text, 01524, with: \\d This matches all the Arabic digits in the Text. Now in place of \\d use a character class that matches the same thing. Enter the following range of digits as the Expression: [0-9] Though the syntax is different, using \\d does the same thing as [0-9]. When we use square-brackets like this we are specifying a character class, you will learn more about character classes later. The character class [0-9] is a range, meaning that it will match the range of digits 0 through 9. You could also match digits 0 through 9 by listing all the digits: [0123456789] If you want to match only the binary digits 0 and 1, you would use this character class: [01] Try [23] in RegExr and look at the result. With a character class, you can pick the exact digits you want to match. The character shorthand for “all digits” (\\d) is shorter and simpler, but it does not have the power or flexibility of the character class. Use character classes when you need to get very specific about what digits you need to match; otherwise, use \\d because it is a simpler, more convenient syntax. 4.2.3 Matching non-digits if you want to match characters that are not digits, use the shorthand: \\D Try this in RegExr now. An uppercase D, rather than a lowercase, matches non-digit characters. This shorthand is the same as the following character class [^0-9] This is a negated class (a negated class says in essence, “do not match these” or “match all but these”), which is the same as: [^\\d] 4.2.4 Matching words and non-words The fake phone numbers data obviously do not contain any word text. Go to &lt;www.authorama.com&gt;, which contains books that are in the public domain. Examples include, Alice in Wonderland, Flatland, and Frankenstein. Pick one and copy the first page of text into the Text area on RegExr, replacing the fake phone numbers. Now, in the Expression box, swap \\D with: \\w This shorthand will match all word characters. The difference between \\D and \\w is that \\D matches whitespace, punctuation, quotation marks, hyphens, forward slashes, square brackets, and other similar characters, while \\w does not, it only matches letters and numbers. In English, \\w matches essentially the same thing as the character class: [a-zA-Z0-9] To match a non-word character: \\W This shorthand matches whitespace, punctuation, and other kinds of characters that are not used in words. It is equivalent to the following character class: [^a-zA-Z0-9] Character classes allow you more control over what you match, but a lot of the time you do not need to type out all those characters. But sometimes you must explicitly state a character class in order to get precisely what you want. Try both: [^\\w] [^\\W] Can you see the differences in what text they match? Table 4.1 shows a summary of shorthand character classes. Table 4.1: Shorthand character classes Shorthand Description \\d digit \\D non-digit \\w word \\W non-word \\b word boundary \\B non-word boundary \\s space character \\S non-space character \\t tab character \\T non-tab character 4.2.5 Matching a literal word Going back to matching literal text, replace the expression with at, all instances of “at” in your text should now be highlighted. This includes instances where “at” is the actual word, and instances where “at” has occurred within a word, for example, “cat sat on the mat”. To match only instances of the actual word “at” change the expression so it becomes: \\bat\\b Here \\b stands for word-boundary, and it matches the start or the end of a word. The first \\b requires the a to occur at the very start of the word, or after a non-word character. The second \\b requires the t to occur at the very end of the word, or before a non-word character. 4.2.6 Matching whitespace To match whitespace, try the following in RegExr and see what is highlighted: \\s The following character class matches the same characters: [ \\t\\n\\r] which are the characters for spaces, tabs (\\t), new lines (\\n), and carriage returns (\\r). To match a non-whitespace use: \\S which is equivalent to: [^ \\t\\n\\r] 4.2.7 Matching any character To match any character with regular expressions use the dot, also known as a period or a full stop. The dot matches all characters but line ending characters. In RegExr, replace your current expression with: . An equivalent character class to dot would be combining any class and its negated class: [\\w\\W] It can be tempting (and very lazy) to use dot as our character class as we can often be more specific about the text phrases we are looking to match. 4.2.8 Using quantifiers Table 4.2: Quantifiers used after a character class. Quantifier Meaning * 0 or more + 1 or more ? 0 or 1 {3} exactly 3 {3,} 3 or more {3,5} 3, 4, or 5 So far we have been matching individual characters or literal text, hence why when we use expressions such as \\w all word characters are individually highlighted. To highlight all words as a whole: \\w+ or [a-zA-Z0-9]+ The plus symbol here is used to state how many times we match the character class. + specifically means “1 or more” characters from the preceding character class. Other quantifiers are shown in Table 4.2. For example to highlight all three-letter words use the expression: \\w{3} How would you modify this to match four-letter words? Six- to seven-letter words? Also look at the differences in results between the two expressions: \\w{3} and .{3} 4.2.9 Matching alternate patterns We can write regular expressions that state that there is a choice of patterns to match. For example, say you wanted to find how many occurrences of the word, “the”, there are in the text you have selected for analysis. The problem is, the word can occur as THE, The, and the. You can use alternation to deal with this by writing the following expression: \\b(the|The|THE)\\b and you will see all occurrences of the in the text highlighted. Being able to specify alternate patterns like this allows us to create sub-patterns in our expressions. Consider what words the following expression will match and why: [tT]{1}h(eir|ere) 4.2.10 Going deeper If you are interested in learning more about regular expressions, seek out at least one of the following books: Introducing Regular Expressions (2012) by M. Fitzgerald (O’Reilly Media). Mastering Regular Expressions, Third Edition (2006) by J. Friedl (O’Reilly Media). Regular Expressions Cookbook, Second Edition (2012) by J. Goyvaerts and S. Levithan (O’Reilly Media). These cover a lot more than these notes. Each can serve as a good reference book. 4.3 Working with strings and regular expressions Base R provides a solid set of string manipulation functions, but because they have grown organically over time, they can be inconsistent and difficult to learn. Additionally, some string processing tasks that are easy to do in languages like Ruby or Python are rather hard to do in R. The aim of the stringr package is overcome these problems by providing a clean, modern interface to common string operations. Some but not all allow the use of regular expressions. 4.3.1 Basic string operations The following functions allow us to manipulate strings on a manual level rather than using any pattern matching: str_c() is equivalent to paste() which you may have already used, it concatenates vectors but it uses the empty string (\"\") as the default separator rather than a single whitespace (\" \") and silently removes zero length arguments. Consider each of the following str_c() statements and how each differs from the previous: dat &lt;- data.frame( animal = c(&quot;human&quot;, &quot;blue whale&quot;, &quot;cat&quot;, &quot;dog&quot;), food = c(&quot;pizza&quot;, &quot;plankton&quot;, &quot;human&quot;, &quot;anything&quot;), when = c(&quot;this week&quot;, &quot;now and then&quot;, &quot;now&quot;, &quot;always&quot;) ) str_c(dat$animal, &quot; wants to eat &quot;, dat$food) ## [1] &quot;human wants to eat pizza&quot; &quot;blue whale wants to eat plankton&quot; ## [3] &quot;cat wants to eat human&quot; &quot;dog wants to eat anything&quot; str_c(dat$animal, dat$food, sep=&quot; wants to eat &quot;) ## [1] &quot;human wants to eat pizza&quot; &quot;blue whale wants to eat plankton&quot; ## [3] &quot;cat wants to eat human&quot; &quot;dog wants to eat anything&quot; str_c(dat$animal, dat$food, dat$when, sep=&quot; wants to eat &quot;) ## [1] &quot;human wants to eat pizza wants to eat this week&quot; ## [2] &quot;blue whale wants to eat plankton wants to eat now and then&quot; ## [3] &quot;cat wants to eat human wants to eat now&quot; ## [4] &quot;dog wants to eat anything wants to eat always&quot; str_c(dat$animal, &quot; wants to eat &quot;, dat$food, &quot; &quot;, dat$when) ## [1] &quot;human wants to eat pizza this week&quot; ## [2] &quot;blue whale wants to eat plankton now and then&quot; ## [3] &quot;cat wants to eat human now&quot; ## [4] &quot;dog wants to eat anything always&quot; How strings are concatenated and collapsed can be modified by additional arguments to str_c(), check the help page to see how. In general, str_c() behaves in a more useful and consistent manner than paste(). str_length() calculates the length of a string (as the number of characters in the string). If passed a vector it will return a vector of lengths. str_length(dat$animal) ## [1] 5 10 3 3 4.3.2 Consumer Packaged Breakfast Goods The following sections look at how we perform various string processing tasks within R using regular expressions. To give context to when these types of tasks are useful we will be working on a data set which contains nutritional information and ingredient lists for 6,600 breakfast products. Download cpg_breakfast.csv from moodle and open it in Excel to understand the structure, then read it into R (prevent auto-conversion of strings to factors). A variable description is presented in Table 4.3. Only convert manufacturer and brand to factors. bf$manufacturer &lt;- as.factor(bf$manufacturer) bf$brand &lt;- as.factor(bf$brand) We will now be mainly working with the ingredient lists. Table 4.3: Description of relevant variables in the breakfast dataset. Variable Description manufacturer The manufacturer or parent company of the product brand The brand name commonly used by consumers product_name The name of the product. May include variant information such as colour or flavour ingredients List of ingredients as they appear on the products packaging (comma separated). calories Total calories per serving. total_carb Total amount of carbohydrates (grams). total_fat Total amount of fat (grams). size Size information as shown on the packaging. avg_price Average price in dollars. ean13 The EAN-13 barcode used for identifying products worldwide. 4.3.3 Detect if a match can be found within a string The first brick wall we run in to when using regular expressions in R is that instead of simply using one backslash for character classes (e.g. \\w) we have to use two (e.g. \\\\w). Let us try to determine how many breakfast products in our data contain oats. First we create the expression: expr &lt;- &quot;\\\\b[oO]ats\\\\b&quot; which looks similar to the expressions we were creating earlier; we are looking to match either “oats” or “Oats” as whole words only. To then detect if either of these appear in the ingredients we do: bf$has_oats &lt;- str_detect(bf$ingredients, expr) This has added the variable oats to our data frame, and it will contain a series of TRUEs and FALSE depending on whether or not our expression was matched in the ingredients strings. To summarise: table(bf$has_oats) ## ## FALSE TRUE ## 2708 1644 In terms of creating features from text-variables str_detect() is probably the most useful. Practice by looking for other common breakfast ingredients such as: corn and rice. Create a expression that will detect any instance of sugar, honey, or syrup. 4.3.4 Extract the matched text There are many different types of syrup (e.g. agave, maple, corn). Write an expression that retrieves “syrup” and the preceding word: expr &lt;- &quot;\\\\w+ [sS]yrup&quot; bf$syrup_type &lt;- str_extract(bf$ingredients, expr) table(bf$syrup_type) ## ## Agave Syrup Barley Syrup Cane Syrup Caramel Syrup ## 11 7 61 1 ## Chicory Syrup Corn Syrup Date Syrup Ginger Syrup ## 1 807 7 1 ## Glucose Syrup Invert Syrup Juice Syrup Malt Syrup ## 27 2 36 103 ## Maltitol Syrup Maple Syrup Oat Syrup Oligofructose Syrup ## 7 121 28 2 ## Pineapple Syrup Refiners Syrup Refinery Syrup Rice Syrup ## 14 1 1 280 ## Root Syrup s Syrup Sugar Syrup Tapioca Syrup ## 4 3 111 47 ## Wheat Syrup ## 1 We can see that corn syrup is a fairly common ingredient. How many of these matches are actually for “high fructose corn syrup”? In general, we can extend our expression to try to capture all proceeding words (remember the ingredients are comma separated): expr &lt;- &quot;,[ a-zA-Z]*[sS]yrup,&quot; bf$syrup_type &lt;- str_extract(bf$ingredients, expr) head(table(bf$syrup_type)) ## ## , Agave Syrup, , Barley Malt Syrup, , Barley Syrup, ## 3 38 1 ## , Brown Rice Syrup, , Brown Sugar Syrup, , Cane Juice Syrup, ## 163 108 1 But now our syrup_type variable contains commas and whitespace. We can tidy this up by writing an expression that keeps all characters between the first and the last comma. To do this we use parentheses around the part of the expression that we want to keep: expr &lt;- &quot;, (.*),&quot; Because the text we are now working on (the text in bf$syrup_type) is small we can risk being a bit lazy with our expression hence why I use dot-star. When we use parentheses like this it is known as grouping, with the first set of parentheses being referenced as group one. To only keep the text that matches inside the brackets: expr2 &lt;- &quot;\\\\1&quot; bf$syrup_type &lt;- str_replace(bf$syrup_type, expr, expr2) head(table(bf$syrup_type)) ## ## Agave Syrup Barley Malt Syrup Barley Syrup Brown Rice Syrup ## 3 38 1 163 ## Brown Sugar Syrup Cane Juice Syrup ## 108 1 It is possible to have multiple sets of parentheses in our expression, these would then be accessed using \"\\\\2\", \"\\\\3\", etc. Also, here our second expression is very simple, we are free to make this more complicated by including literal text: expr &lt;- &quot;(.*)&quot; expr2 &lt;- &quot;I have found \\\\1!&quot; bf$syrup_type &lt;- str_replace(bf$syrup_type, expr, expr2) head(table(bf$syrup_type)) ## ## I have found Agave Syrup! I have found Barley Malt Syrup! ## 3 38 ## I have found Barley Syrup! I have found Brown Rice Syrup! ## 1 163 ## I have found Brown Sugar Syrup! I have found Cane Juice Syrup! ## 108 1 expr2 works in a similar way to the concatenating we did earlier with str_c(). Try this process of extracting again, but now look for instances of vitamins being added to the product. Commonly added vitamins are A, B12, C, D, and E, ensure your expression can capture these. str_extract() will only retrieve the first match from a string. To retrieve all vitamins that are added to a product use str_extract_all(). Though you will not be able to store the results of that are returned from str_extract_all() simply in a variable in your data frame. Store the results in an object separate from your data frame and inspect it. 4.3.5 Replace matches Say, we know someone who is a vegan, this someone also happens to be fairly good with regular expressions, let’s call this person Rose Myris. If Rose Myris wanted to replace all instances of meat-based ingredients with the phrase “dead animal” then she could do this by using str_replace(). Firstly, let’s find some breakfast foods that do actually contain meat: expr &lt;- &quot;(Duck|Beef|Pork|Bacon|Chicken|Poultry|Turkey)&quot; bf$has_meat &lt;- str_detect(bf$ingredients, expr) bf_meat &lt;- filter(bf, has_meat == TRUE) Inspect bf_meat to see the kinds of products we are dealing with, in particular look at row 161 (which also includes corn syrup). Using str_replace(), we can reuse the same expression in order to swap out each type of meat for a different phrase: bf_meat$ingredients &lt;- str_replace(bf_meat$ingredients, expr, &quot;dead animal&quot;) Now look at what changes have been made to the ingredients in bf_meat. In each string only the first match has been replaced. Obviously, Rose Myris, would prefer to have all matches replaced, the is where str_replace_all() comes in to action: bf_meat$ingredients &lt;- str_replace_all(bf_meat$ingredients, expr, &quot;dead animal&quot;) At last, Rose Myris’ anger can now reach new heights! To really get her blood boiling let’s count how many replaces were made: dead_hits &lt;- str_match_all(bf_meat$ingredients, &quot;dead animal&quot;) dead_count &lt;- c() for (i in 1:length(dead_hits)) { dead_count[i] &lt;- length(dead_hits[[i]]) } sum(dead_count) ## [1] 315 An alternative way to code this which avoids the loop is: dead_count &lt;- lengths(dead_hits) sum(dead_count) ## [1] 315 We can then look at the worst offender. bf_meat[which.max(dead_count), ] ## manufacturer brand product_name size ## 12 Turkey Turkey Bacon 6 oz ## ingredients ## 12 dead animal, Mechanically Separated dead animal, dead animal, dead animal Flavor, Rendered dead animal Fat, Smokey Flavoring, Cooked dead animal, Cured With Water, Salt, Sugar, Smoke Flavorings, Sodium Phosphate, Sodium Erythorbate, Sodium Nitrite, Flavoring, Water, Autolyzed Yeast ## avg_price ean13 serving_size calories fat_calories total_fat sat_fat ## 12 2.29 45300303659 3 slices 70 50 5 2 ## trans_fat cholesterol sodium potassium total_carb dietary_fiber sugars ## 12 0 20 360 0 1 0 1 ## protein polyunsat_fat monounsat_fat soluble_fiber insoluble_fiber calcium ## 12 4 0 0 0 0 2 ## iron has_oats syrup_type has_meat ## 12 2 FALSE &lt;NA&gt; TRUE 4.3.6 Building large expressions If we wanted to find products that contained fruit, we simply have to create one long alternating pattern of literal fruit names (e.g. (apple|orange|pear|raisin), as there is no smart compact expression we can write down that will match only fruit names. The file list_of_fruit.txt is a list of fruit, read it into R and format the contents such that it looks like a list of fruit, with the first few rows looking something like this: Apple Apricot Avocado Banana For example, fruit &lt;- read.table(file=&quot;./data/list_of_fruit.txt&quot;, header=FALSE, stringsAsFactors=FALSE) We are not interested in keeping this data in a data frame, so we can go ahead and overwrite the fruit object with the V1 variable: fruit &lt;- fruit$V1 Now fruit is just a vector of fruit names. Check the length of fruit to see how many we actually have. Next, we can start to build our expression by first collapsing all the different fruit names together, and then adding parenthesis either side: fruit_expr &lt;- str_c(fruit, collapse=&quot;|&quot;) fruit_expr &lt;- str_c(&quot;(&quot;, fruit_expr, &quot;)&quot;) str_length(fruit_expr) ## [1] 769 Our fruit expression has 769 characters which means it’s very long. Let’s put this into action: bf$has_fruit &lt;- str_detect(bf$ingredients, fruit_expr) table(bf$has_fruit) ## ## FALSE TRUE ## 2278 2074 We can use this technique of concatenating a list to build several features, such as whether or not a product: Contains gluten. Is suitable for those who are lactose-intolerant. Is suitable for vegetarians or vegans. For each of these we would need to create specific lists which we would then collapse to form a single, but very long, expression. In some cases, it may be easier to try to match ingredients that they can not or will not eat rather than match those that they can. 4.4 Exercises Create separate boxplots comparing the distribution of total carbohydrates in fruit-based and non-fruit-based breakfast food. item Repeat but for total fat instead of total carbs. item The are a wide variety of products that class as breakfast food: Cereal-bars Cereal (e.g. corn flakes, porridge, granola, puffed rice) Waffles, pancakes Delicious baked goods (e.g. croissants, pan au chocolat) Meat products (e.g. sausage, bacon) Other Construct regular expressions which attempt to classify products into these groups. You will have to decide which variable in the data set is the most appropriate to be working with. You may need to break this task down into several smaller sub-tasks. Once complete, construct similar boxplots to the first two tasks, comparing distributions of total fat and carbs for these groups. "],
["web-app.html", "5 Web scraping, interactive graphics, Shiny Apps, package building, and more 5.1 Packages 5.2 Collecting data 5.3 Alternative graphics 5.4 Shiny Web Applications 5.5 Rmarkdown HTML notebooks and Ipython/Jupyter notebooks 5.6 Exercise: Housing in New York City", " 5 Web scraping, interactive graphics, Shiny Apps, package building, and more 5.1 Packages Install the new packages rvest and ggvis. It will also be worth installing readr and purrr. Load in the new packages and ggplot2: library(tidyverse) library(ggvis) ## ## Attaching package: &#39;ggvis&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## resolution library(rvest) ## Loading required package: xml2 ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## pluck ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding 5.2 Collecting data So far the data we’ve used in the labs has been provided for you. More importantly, it’s been given to you in a nice format (e.g. no missing values). In practice you’ll find that most data are messy and that analysing real data requires you to spend time cleaning and manipulating the data into a useable form before you can do your analysis. One of the best sources of data is the internet. There are now over a billion websites containing data on almost anything you can think of (e.g. income, world poverty, property values, film releases, etc.). 5.2.1 Simple scraping In the first lab we looked at a dataset containing information about planets in our solar system. In that exercise we input that data manually. However, we can obtain these data by scraping the data directly from the web.15 The rvest package from Hadley Wickham will allow us to read the data directly from the website. So let’s try it out. Start by loading the package if you haven’t already. library(rvest) Once we’ve found a good website containing the data we want, we can scrape the data from the website and store it in a data.frame object html &lt;- read_html(&quot;http://nssdc.gsfc.nasa.gov/planetary/factsheet/&quot;) html_data &lt;- html_node(html, &quot;table&quot;) # extract parts of HTML text planet_data &lt;- html_table(html_data, header=TRUE) # display table head(planet_data) ## MERCURY VENUS EARTH MOON MARS JUPITER SATURN ## 1 Mass (1024kg) 0.330 4.87 5.97 0.073 0.642 1898 568 ## 2 Diameter (km) 4879 12,104 12,756 3475 6792 142,984 120,536 ## 3 Density (kg/m3) 5427 5243 5514 3340 3933 1326 687 ## 4 Gravity (m/s2) 3.7 8.9 9.8 1.6 3.7 23.1 9.0 ## 5 Escape Velocity (km/s) 4.3 10.4 11.2 2.4 5.0 59.5 35.5 ## 6 Rotation Period (hours) 1407.6 -5832.5 23.9 655.7 24.6 9.9 10.7 ## URANUS NEPTUNE PLUTO ## 1 86.8 102 0.0146 ## 2 51,118 49,528 2370 ## 3 1271 1638 2095 ## 4 8.7 11.0 0.7 ## 5 21.3 23.5 1.3 ## 6 -17.2 16.1 -153.3 Essentially we’re going to be working with four main functions: read_html() - used to set the webpage address html_node()/html_nodes() - select which parts of the website you want to scrape html_table() - convert an HTML table into a data.frame object html_text() - extract HTML text. Used correctly, these functions will allow you to scrape data from alot of websites. However, as you can imagine, the more complex the design of the website, the harder it will be to identify what you’re looking for. You may find the https://selectorgadget.com tool particularly useful. This tool can be added as an extension to Google Chrome.16 When activated you can simply hover your mouse over the section of the website you want to scrape and the selectorgadget tool will tell you what the xpath/CSS link is. Piping is also helpful when running web scraping code. Piping allows you to direct the output from one function to another and then another and then another, etc. by using the \\%&gt;\\% operator from the dplyr package. As an example, here in an alternative way of implementing the above: planet_data &lt;- read_html(&quot;http://nssdc.gsfc.nasa.gov/planetary/factsheet/&quot;) %&gt;% html_node(&quot;table&quot;) %&gt;% html_table(header=TRUE) You’ll notice by comparing the two implementations, that function(a, b) is equivalent to a %&gt;% function(b). We are plugging our variable into the first argument of the function. If instead we want function(b, a), then we’ll need to use the form a %&gt;% function(b,.). 5.2.2 Exercise The planets dataset contains many unwanted characters (e.g. *, commas, words). Use regular expressions and string processing operations, to remove these to create a clean dataset. 5.2.3 Google Scholar Google scholar is a helpful resource for finding academic works. In addition to finding journal articles, you can also find profiles of researchers and see what they work on, the papers they publish, how influential they are, and who they work with. Let’s consider the Google Scholar page of Peter Diggle, Lancaster University Distiguished University Professor of Statistics. page &lt;- read_html(&quot;https://scholar.google.com/citations?user=GcTKrPIAAAAJ&amp;hl=en&quot;) On the Google Scholar page there are several tables. There is a summary of the researcher’s output (top right). The main table contains the details of each of the researcher’s papers. Let’s scrape these two tables (remember the . indicates the contents from the pipe so far). overview &lt;- page %&gt;% html_nodes(&quot;table&quot;) %&gt;% .[[1]] %&gt;% html_table() citations &lt;- page %&gt;% html_nodes(&quot;table&quot;) %&gt;% .[[2]] %&gt;% html_table() %&gt;% .[-1,2] %&gt;% readr::parse_number() The first operation is straightforward. We’re simply scraping the first list element (i.e. [[1]]), which is a table from the webpage. The second line is a little trickier with two additional operations. Firstly, we’re using [-1,2] to remove the first row of the table and select the second column, just the citations. Figure 5.1: The citations of Prof. Diggle’s papers You’ll often find in practice, especially when working with web scraping, that data isn’t always presented in the most convenient way. The last operation, readr::parse_number(), is simply stripping everything out that isn’t numeric.17 We have obtained some of the information on the webpage. head(overview) ## All Since 2015 ## 1 Citations 49694 16542 ## 2 h-index 86 53 ## 3 i10-index 273 203 head(citations) ## [1] 11817 5290 3396 1393 1379 1251 Researchers almost never publish alone and will work with numerous collaborators, often across various disciplines. Prof. Diggle is no exception, let’s have a look at some of his co-authors. Remember that the selectorgadget Chrome extension can help you find the name of the CSS tag you need to provide to html_nodes(). Another way is to press Ctrl+Shift+I in Chrome and view the HTML source code. Remember also that websites are frequently updated and so the next time you run this code it will fail if Google have changed the Google Scholar HTML code. Coauthors = page %&gt;% html_nodes(css=&quot;.gsc_rsb_a_desc a&quot;) %&gt;% html_text() Coauthors = as.data.frame(Coauthors) names(Coauthors)=&#39;Coauthors&#39; head(Coauthors) ## [1] Coauthors ## &lt;0 rows&gt; (or 0-length row.names) 5.3 Alternative graphics 5.3.1 Introducing ggvis By now we are familiar with using the ggplot2 package instead of the base R graphics engine to produce figures. Now we’re going to look at another package, ggvis, which you’ll notice has the same graph building philosophy, albeit with a slightly different syntax. Install the package and then load it, install.packages(&quot;ggvis&quot;) library(ggvis) We’re going to practice some of the basics of ggvis with a demo dataset that can be found in the R base package. The mtcars dataset contains data on 32 cars from 1973–74. Let’s start by plotting the horsepower against the miles per gallon. # first layer is the points of a scatter plot # second layer is a simple linear model, # SE=TRUE displays confidence bans around the predictions mtcars %&gt;% ggvis(~hp, ~mpg, fill := &quot;red&quot;, stroke := &quot;black&quot;) %&gt;% layer_points() %&gt;% layer_model_predictions(model=&quot;lm&quot;, se=TRUE) This plot can be broken down into smaller components in the same as ggplot graphics are built-up from smaller components. We’re plotting the data and overlaying a straight line, which is the prediction from a linear model that we’ve fitted to the data. We have also added pointwise 95% confidence interval bands to our linear model. Horsepower is related to the number of cylinders in the engine. So let’s group the data into number of cylinders and then, treating each group independently, fit a new linear model. Therefore, we are fitting a piecewise linear model. #piecewise linear mtcars %&gt;% ggvis(~wt, ~mpg, fill = ~factor(cyl)) %&gt;% layer_points() %&gt;% group_by(cyl) %&gt;% layer_model_predictions(model = &quot;lm&quot;, se=TRUE) # the group_by(cyl) tells next layer to group by cylinder factor The plot shows three separate lines, with their corresponding 95% confidence intervals. Figure 5.2: Piecewise linear model plot using ggvis. 5.3.2 Interactive/responsive graphics You might be wondering, why should I use ggvis when I can use ggplot? One of the main advantages of ggvis is that you can make your graphics interactive (they will be viewed with a browser to achieve this). This can be particularly cool if you want to embed your graphics in a responsive web environment, or you wish to manipulate features of the plot to gain a deeper insight into the data you’re plotting. The following code generates a basic interactive plot. # In this plot we have: # a smoothing layer with input selection for span # and a point layer with input selection for size mtcars %&gt;% ggvis(~wt, ~mpg, fill := &quot;purple&quot;, stroke := &quot;black&quot;) %&gt;% layer_smooths(span = input_slider(0.5, 1, value = 1)) %&gt;% layer_points(size := input_slider(100, 1000, value = 100)) Figure 5.3: Interactive plot using ggvis. When viewed in the RStudio Viewer or in a web browser you can move the sliders at the bottom with your mouse. This particular plot has two sliders which you can play with to: adjust the size of the data points; adjust the smoothness of the line. If you decrease the smoothness, then the line will start to become more wobbly and intersect more points, whereas if you increase the smoothness, the line will instead capture the general shape of the data, rather than specific points. Note the message in the RStudio Console “Showing dynamic visualisation. Press Escape/Ctrl + C to stop”. You must press either of these key combinations, or the Stop icon in the RStudio Viewer, to continue submitting R commands in your R session. When modelling a dataset we often make assumptions about how the data is generated. For example, do the residuals follow an underlying Normal distribution, and if so, can we estimate it’s parameters. Alternatively, the data may be distributed as a Poisson, Gamma, or Chi-squared distribution. It’s not always appropriate to assume that the data fits some pre-specified distributional family, especially if we don’t have a lot of data. Instead, we may want to model the data without making such distributional assumptions. In statistics this is known as nonparametrics, and the most common type of nonparametric density estimation is kernel density estimation. Further details can be found here https://en.wikipedia.org/wiki/Kernel_density_estimation. Essentially kernel density estimation replaces each data point with a kernel function (e.g. Gaussian, Epanechnikov), and aims to approximate the underlying population density by smoothing your data (which is assumed to be a sample from the population). Run the code below and see what happens when you change the kernel function and smoothing parameter (more commonly known as the bandwidth). # In this plot we have an input slider to select the bandwidth of smoother mtcars %&gt;% ggvis(~wt) %&gt;% layer_densities( adjust = input_slider(.1, 2, value = 1, step = .1, label = &quot;Bandwidth adjustment&quot;), kernel = input_select( c(&quot;Gaussian&quot; = &quot;gaussian&quot;, &quot;Epanechnikov&quot; = &quot;epanechnikov&quot;, &quot;Rectangular&quot; = &quot;rectangular&quot;, &quot;Triangular&quot; = &quot;triangular&quot;, &quot;Biweight&quot; = &quot;biweight&quot;, &quot;Cosine&quot; = &quot;cosine&quot;, &quot;Optcosine&quot; = &quot;optcosine&quot;), label = &quot;Kernel&quot;) ) Figure 5.4: Another interactive plot using ggvis. The plot allows you to choose which kernel you are using and also to adjust the bandwidth. This is much more convenient than plotting many different plots. 5.4 Shiny Web Applications The team behind RStudio have created an application framework which allows the user to create web pages which R code. One example, of this is we can create interactive plots like those generated by ggvis. To create a new app in RStudio follow this procedure: click the New icon | Shiny Web App… Give yours a name with no spaces such as ``example-shiny-app’’. Choose the single of multiple file approach to coding the app (it doesn’t really matter which you choose) and choose which directory (folder) that you want to make it in. If you chose the single file approach you now get a single file called app.R to edit. However the placeholder code provided by RStudio will actually run. Click the “Run App” button in the top right corner of the Source pane. RStudio will open a new Viewer window running the app. Figure 5.5: Example Shiny App by RStudio. As with the ggvis interactive plots you see that you can move the slider with your mouse and R recalculates and redraws the plot. Experiment with moving the slider. Shiny Apps are very flexible because they need not just produce plots but they provide Web frontends to any type of R code. So you could be providing access to web scraping code or access to a database. Inspect the structure of the code. It is very simple. There are two main functions ui() (which stands for user-interface) and server() which is the part of the code doing the computation/plots. You would edit these functions to perform the task you needed to achieve. Whilst your app is running you will notice that the RStudio console reports “Listening on http://127.0.0.1:4122”. In order to be able to resume your R session you must close the window your app is running in. 5.4.1 Other RStudio features we haven’t covered RStudio has many features we haven’t covered. It has brilliant features when you are building your own package (personally I would never build a package outside of RStudio anymore). When building a package and some other tasks you need the RStudio “Build” pane. To see this I think you must define the top level directory for your package (or bookdown book etc.) as an RStudio project. This creates an .Rproj file in the directory which RStudio detects and then shows the Build pane. RStudio can also run Sweave documents (which contain both LaTeX and R code) and also many types of presentation including LaTeX beamer and RStudio’s .Rpres format. 5.5 Rmarkdown HTML notebooks and Ipython/Jupyter notebooks An advanced type of Rmarkdown document is an “R Notebook”. These are R markdown files with the output in the YAML header defined as output: html_notebook: default When you first save an Rmarkdown file with this output definition its output file with extension .nb.html is created immediately (even if you have not evaluated any of your code blocks). The .nb.html file format is very clever because it actually contains the Rmarkdown code as well as the html output. Therefore, if you open an .nb.html file in RStudio it will show the associated .Rmd code even if you don’t have that separate file. The R output shown in the .nb.html file is the output from the code blocks which have been run, i.e. if you have not evaluated all of the R code blocks in your Rmd file then not all of the output will be present in the .nb.html file. The other main notebook format used in Data Science are Ipython/Jupyter notebooks http://jupyter.org/. These keep the R read-evaluation-print-loop running in the background so you can evaluate one cell of code at a time live in your browser. A screen shot of an example notebook is shown at side of the page. Figure 5.6: Example Jupyter notebook. To try a Jupyter notebook without having to install it on your local machine, you can try it in a browser at https://try.jupyter.org/. However, my personal experience of this website is not good. Either the R kernel dies or you cannot get a space to launch a notebook. Running Jupyter notebooks on my own machine has been reliable. If you want to try such notebooks installation instructions are here: http://jupyter.org/install.html. This is easiest to do on your own computer. If you are on a University Windows network computer these will probably fail. One element of getting Jupyter notebooks running is installing the IPython/Jupyter R kernel (the notebooks can use kernels for other languages such as Python, Julia, and many other languages). Instructions can be found here https://irkernel.github.io/installation/. 5.6 Exercise: Housing in New York City We’ve now covered the basics of the rvest and ggvis packages. We’re now going look at combining these packages, along with others (including dplyr) to perform statistical analysis on dataset we have scraped from the web. If you have installed the tidyverse package hopefully that has given you the purrr and readr packages already. New York city contains some of the most expensive property in the world, particularly Manhattan. The property market is a rich source of data which contains many interesting nuances. For example, just because a property is big, doesn’t mean it’s expensive. Have a look at https://www.zillow.com to get an idea of its layout. Try searching for some property, anywhere in the USA will do, and you’ll notice that the web page of the search results is split into two halves. There’s a map and a series of photo cards. Pulling data from the map, or overlaying data onto a map is possible with the ggvis package, but a little beyond what we’re going to cover today.18 Instead we’re going to focus on the photo cards which contain textual information. The code below will extract the address, price, size, and the number of bedrooms and bathrooms of each property in the search results: # Note that this web address is quite long here so I&#39;ve split it # over several lines using paste0(), but you could just have it on the same line page &lt;- read_html(paste0(&quot;http://www.zillow.com/homes/for_sale/&quot;, &quot;Manhattan-New-York-NY/12530_rid/globalrelevanceex_sort/&quot;, &quot;40.894829,-73.795167,40.664754,-74.151879_rect/11_zm/1_p/&quot;)) # extract html details from the photo cards houses &lt;- page %&gt;% html_nodes(&quot;.photo-cards li article&quot;) address &lt;- houses %&gt;% html_node(&quot;.zsg-photo-card-address&quot;) %&gt;% html_text() %&gt;% gsub(&#39;,&#39;,&#39;&#39;,.) %&gt;% gsub(&#39;\\\\s&#39;,&#39;_&#39;,.) price &lt;- houses %&gt;% html_node(&quot;.zsg-photo-card-price&quot;) %&gt;% html_text() %&gt;% readr::parse_number() params &lt;- houses %&gt;% html_node(&quot;.zsg-photo-card-info&quot;) %&gt;% html_text() %&gt;% strsplit(&quot;\\u00b7&quot;) beds &lt;- params %&gt;% purrr::map_chr(1) %&gt;% gsub(&#39;\\\\D&#39;,&#39;&#39;,.) baths &lt;- params %&gt;% purrr::map_chr(2) %&gt;% readr::parse_number() house_area &lt;- params %&gt;% purrr::map_chr(3) %&gt;% gsub(&#39;\\\\D&#39;,&#39;&#39;,.) ny_housing &lt;- data.frame(address, price, beds, baths, house_area) You may get some warnings when you run this code. Let’s ignore those for the moment. You’ll find scraping data from website is never trivial. Web developers design their sites to be pretty and not scraper friendly. Companies that specialise in web scraping spend a significant portion of their time updating scrapers when websites are updated. Your task is to utilise what you’ve learnt over the past few weeks to perform some statistical analyses on these data. Here are some suggestions for things to explore. It’s always a good idea to take a look at the data. The two most obvious functions to use here would be View() and head(). It’s also a good idea to look at some simple summary statistics, such as mean, median, max, etc. Another useful function for summarising your data is summary(). Note that this function will only work on numeric class data. If there are any non-numeric values in your data fix your data frame so that the appropriate columns are of class numeric. You’ll notice that in the raw data that “Studio” often appears under “number of bedrooms”, really we’d want a number so that we can do summaries on the data. Here I’ve just removed anything that’s not a number, can you do something better? As we saw with the Google Scholar data, when we scrape data from a website we’re only scraping what is on that particular page. This is fine, but if the data we want is over multiple pages then we need to be clever. Try to use a for loop to loop over each webpage and scrape the data you want. Hint: Most websites follow a logical naming and numbering structure. Create an interactive ggvis plot to explore one or more variables. As I’ve already mentioned, location is a very important factor. The longitude and latitude for each property is availble (hint: look at houses[[1]]). Can you extract this data and plot the search results either on a grid or map and highlight the expensive parts of the city? R was not really designed for this sort of task. Other programming languages, such as Python, are generally better for web scraping.↩ Search for this in the Chrome Web Store https://chrome.google.com/webstore/category/extensions↩ You can think of the readr::parse_number() function as a more advanced version of as.numeric().↩ Location is an important factor when determining price so filtering properties based on area would certainly be interesting.↩ "],
["further-reading.html", "Further reading", " Further reading There are many excellent R resources available online. Some are: Hadley Wickham’s Advanced R: https://adv-r.hadley.nz/ Efficient R programming by Colin Gillespie: https://bookdown.org/csgillespie/efficientR/ plotly for R book: https://plotly-book.cpsievert.me/index.html "]
]
